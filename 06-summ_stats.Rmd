# Statistical Fundamentals

```{r ch6_setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE)
```

```{r ch6_load}
library(readr)
library(tidyverse)
```

## Intro
This tutorial details fundamental statistical concepts, illustrated with IBIS data.  The main purpose is two-fold 1) to develop a set of notation, definitions, and ideas that comprise the foundamentals behind standard statistical analyses and 2) to introduce some basic statistical methods and illustrate how they are carried out in R using IBIS data.  The notation and concepts developed here will be referenced again in later tutorials, so those without formal statistical training would be inclined to read this tutorial.  For those with a solid background in statistics, this tutorial should serve as a quick refresher as well as an reference for standard statistical terminology and notation. 

## Statistical Inference
For variable $X$, its distribution can be thought of as a function which states the probability of $X$ equalling a specific value $x$, for every possible $x$.  For example, if $X$ is normal distributed, its distribution can be plotted as shown below.

```{r normal_plot, echo=F}
curve(dnorm, from=-10, to=10, ylab="Distribution")
```

We can see that the probability is highest around $x=0$, and then quickly decreases as you move away from $0$.  When given real data, often a histogram is used to visualize the variable's distribution.  For example, the histogram of AOSI total score 12 months in the AOSI cross sectional data is the following:

```{r histo_ex}
data <- read.csv("Data/Cross-sec_full.csv", stringsAsFactors=FALSE, na.strings = c(".", "", " "))
hist(data$V12.aosi.total_score_1_18, xlab = "AOSI Total Score",
     main="Histogram of AOSI Total Score at 12 months")
```

We can see that it is most likely that AOSI total score will be between 0 and 10, with values higher then 10 unlikely.  This type of distribution is called **skewed**, specifically **right skewed** as it has a long "tail" to the right and most of the probability to the left.  A **left skewed** distribution is the opposite.  We can also consider the distribution of our variable $X$ **conditional** on value **y** of another variable **Y**.  Conditional means "only on the population with Y=y".  This is referred to as the **conditional distribution** of $X$ for $Y=y$.  For example, while we have created the histogram of AOSI total score for the entire sample, we may want to visualize the distribution of AOSI total score separately for each of the following diagnosis groups, High Risk: ASD, High Risk: Negative, and Low Risk: Negative.  For example, we create the histogram for the High Risk: ASD group below.  

```{r histo_ex2}
data_HRASD <- data %>% filter(GROUP=="HR_ASD")
hist(data_HRASD$V12.aosi.total_score_1_18, xlab = "AOSI Total Score",
     main="Conditional Histogram of AOSI Total Score \nat 12 months for High Risk: ASD group") 
# can see variable is considered to be a string by default in the data; need to force it to be numeric to create a histogram
```

Statistical analyses are often designed to **estimate** certain characteristics of a variable's (or many variables') distribution.  These characteristics are usually referred to as **parameters**.  Often times, these parameters are the mean, variance, median/quantiles, or probabilities of specific values (for a discrete variable, i.e., probability of positive ASD diagnosis).  We do not know the true values of these parameters, so we try to obtain an accurate approximation of them based on random samples (your data) from the distribution/population of interest.  These approximations will vary from sample to sample due to the randomness behind the selection of these samples and their finite size.  Thus, we also need to account for the **variance** of these approximations when completing our analyses.  This analysis, composed of the **estimation** of the parameters as well as accounting for the **variance** of this estimation, is referred to **statistical inference**.

### Parameter Estimation: Mean, Median, tutorial, Quantiles
Here, we discuss the estimation of specific parameters that are usually of interest for continuous variables.  For categorical variables, see Chapter 6.  These parameters are the mean, median, variance, and quantiles.  Often, you will see these estimates summarized in "Table 1" in research articles.  To see how to create such summaries, see the Chapter 9.  Here, we discuss the methodology behind their estimation as well as how to compute these estimates in R.

Both the mean and median can intuitively be thought of as measures of the "center" of the distribution.  The variance can be thought of a measure of the "spread" of the distribution.  To illustrate variance, we plot two normal distributions both with mean 0 but one with a variance of 1 and another a variance of 4.  The increase in the likely range of values with a variance of 4 is quite evident, and the probabilities are more spread out.  Note that the **standard deviation** is just the square root of the variance.

```{r variance_ex, echo=F}
plot(function(x){dnorm(x,0,1)},xlim=c(-10,10), 
      ylab="Distribution", main="Normal distribution with variance 1")
plot(function(x){dnorm(x,0,4)},xlim=c(-10,10), 
      ylab="Distribution", main="Normal distribution with variance 4")
```

We estimate the distribution's mean, median, variance, and quantiles using the usual sample mean, median, variance, and quantiles respectively.  These can be calculated in R using the functions **mean(), median(), var()**, and **quantile()** respectively.  Note: when using quantile(), the minimum and maximum values in the sample are also provided.  We illustrate these functions below using AOSI total score at 12 months.  Also, note the inclusion of **na.rm=TRUE** into these functions.  By default, when calculating these estimates, if a missing value is encountered, R outputs NA (missing).  Thus, we need to tell R to remove these missing values before computing the estimates.

```{r sample_mean_median_etc}
mean(data$V12.aosi.total_score_1_18, na.rm=TRUE)
var(data$V12.aosi.total_score_1_18, na.rm=TRUE)
median(data$V12.aosi.total_score_1_18, na.rm=TRUE)
quantile(data$V12.aosi.total_score_1_18, na.rm=TRUE)
```

However, we often want to see these estimates (and others) for many variables in our dataset without having to calculate each one separately.  This can be done using **summary()**.  These estimates are often referred to as **summary statistics** of the data.  We compute summary statistics for a number of the variables in the dataset below.  

```{r summ_stat_ex}
data_small <- data %>%
  select(V12.aosi.total_score_1_18, V06.aosi.total_score_1_18,
         V12.aosi.Candidate_Age)
summary(data_small)
```

There are also a number of packages which expand R's functionality in computing these summary statistics.  One recommended package is the **Hmisc** package.  It includes the function **describe()** which is an improved version of of summary(); describe() is used below with the same variables.

```{r describe_ex}
library(Hmisc)
describe(data_small)
```

### Accounting for estimation variance and hypothesis testing
While these estimates provide approximations to the parameters of interest, they 1) have some degree of error and 2) vary from sample to sample.  To illustrate this, 5 simulated datasets, each of size 100 and with a single variable are generated under a mean of 0.  The sample mean is calculated for each of these 5 samples.  We see that 1) these sample means differ across the samples and 2) none are exactly 0.  Thus, we need to account for this variance when providing the approximations.

```{r sim_ex_mean}
data_sim <- list()
for(i in 1:5){
  data_sim[[i]] <- rnorm(100, 0, 1)
}

lapply(data_sim, mean)
```

### Confidence intervals
This is generally done by including a **confidence interval** with your approximation, more specially a **$x$% confidence interval** where $x$ is some number between 0 and 100.  Intuitively, you can interpret a confidence interval as a "reasonable" range of values for your parameter of interest based on your data.  Increasing the percentage of the confidence interval increases its width, and thus increases the chances that the confidence interval from a sample will contain the true parameter.  However, this increasing width also decreases the precision of the information you receive from the confidence interval, so there is a trade off.  Generally, 95% is used (though this is just convention).  

As an example, we considering computing a confidence interval for the mean.  Consider AOSI total score at 12 months from our dataset.  We have already computed an estimate of the mean; let us add a confidence interval to this estimate.  This can be done using the **t.test()** function (which has other uses which we cover later).

```{r AOSI_ex_CI}
t.test(data$V12.aosi.total_score_1_18)
```

Let us focus on the bottom of the output for now; we cover the top later.  We see that we approximate that the mean AOSI total score at 12 months in the population is 4.98 and a reasonable set of values for this mean in the population is 4.67 to 5.29 using a 95% confidence interval.

### Hypothesis testing
In order to come to a conclusion about a paramater from the sample information, hypothesis testing is generally used.  

The setup is the following: suppose we are interested in inferring if the mean for a variable is some value, say $\mu_0$.  From the data we have, we would like to make an educated guess about the claim that the mean is $\mu_0$.  The main principle in scientific research is that one makes a claim or hypothesis, conducts an experiment, and sees if the results of the experiment contradict the claim.  This is the same reasoning that governs hypothesis testing.

To start, we make a claim which we would "like" to refute (for example, a treatment has no effect).  This claim is referred to as the null hypothesis.  For example, consider the null hypothesis that the mean is $\mu_0$.  We then define the contradiction to this claim, that the mean is NOT $\mu_0$, as the alternative hypothesis.  Finally, we take the information from our data, and see if there is enough evidence to **reject** the null hypothesis in favor of the alternative.  If there is not enough evidence, then we **fail to reject** the null.  That is, we **never** accept the null or prove that the null is true.  Recall that the null is our baseline claim that we are aiming to refute; if we accepted the null, then we would be "proving true" that which we initially claimed to hold.

How do we determine if we have "enough evidence" to reject the null?  The following process is generally used.  First, we reduce our data down to a single value that is related to the hypothesis being tested.  This value is called a **test statistic**.  Then, we see how much the observed test statistic from our data deviates from the range of values we would expect if the null hypothesis were true.  If this deivation is large enough, we decide to reject the null.  Otherwise, we fail to reject.

This is best explained by example.  Consider our null hypothesis of the mean is $\mu_0$.  Intuitively, we would calculate the sample mean and use it as our test statistic, and compare its value to $\mu_0$.  If the sample mean is "close" to $\mu_0$, we would fail to reject, otherwise we would reject.  For example, recall that for AOSI total score at 12 months, the sample mean ws 4.98.  For our null hypothesis was that the population mean was $\mu_0$=5, we probably would not have enough evidence reject the null.  However, we need a formal way of measuring the deviation from the null, preferrably one that is independent of the variable's units.  The **p-value** serves as this measure.  

Formally, the p-value measures the probability of seeing a test statistic value that is as extreme or more extreme from the null hypothesis then the test statistic value you actually observed.  Informally, you can think of a p-value as a unit agnostic measure of how much your data deivates from the null hypothesis.  We calculate the p-value using the observed value of the test statistic as well as the **test statistic's distribution** (also referred to as its **sampling distribution**); this distribution is how we are able to compute the probablity which the p-value reflects. 

To conduct the hypothesis test corresponding to the null hypothesis that the population mean is 0, a **one sample t-test** is often used.  We will cover this in more detail later; here we use it to illustrate the above concepts.  The t.test() function can also be used to conduct this hypothesis test.  We consider testing if the mean AOSI total score at 12 months is 0.  Note that we can choose a values different from 0 to use in our null hypothesis by adding the argument mu=x, where x is the value of interest.

```{r t_test_AOSI}
t.test(data$V12.aosi.total_score_1_18)
```

The top part of the output contains the hypothesis test results.  We see that the test statistic (denoted by t) is 31.156 and the corresponding p-value is essentially 0.  Note that **there is a one to one correspondence between the test statistic value and the p-value**.  Thus, reporting the p-value only is sufficient.  We see that the p-value is very small; a small p-value implies that the data deviates from the null.  However, to make a reject or fail to reject decision, we have to set a threshold for this p-value.  Generally, people select 0.05 as a threshold; below 0.05 implies the evidence is strong enough to reject and above 0.05 implies the evidence is not strong enough.  However this is **just a rule of thumb** and one could justify a different threshold.  

Furthermore, it may not be wise to make an all or nothing decision in terms of judging the scientific value of the results.  Using a p-value threshold implies that a p-value of 0.051 has the same interpretation as a p-vale of 0.51 and that a p-value of 0.049 is "significant" evidence while a p-value of 0.051 is not.  This dramatically limits the scientific information that the results are providing.  Instead, it is better to interpret p-values more broadly and in terms of their strength of evidence.  For example, a p-value of 0.06 or 0.025 may indicate "strong evidence" in favor of the alternative hypothesis and 0.10 or 0.12 may indicate "moderate evidence".  Simply reducing the interpretation to "significant" or "not significant" is not optimal and should be avoided.

### Hypothesis Testing with Means
Here we cover methods to conduct hypothesis testing with population means.  You will likely recognize many of these methods; we cover **t-tests** and **ANOVA**.  We illustrate these using AOSI total score at 12 months and diagnosis group (High Risk: ASD, High Risk: Negative, Low Risk: Negative)

#### T-tests: One Sample and Two Sample
First, consider testing the null hypothesis that the mean AOSI total score at 12 months is 0 in the population.  As discussed before, this comparison of a single mean to a value is generally done with a **one sample t-test**.  Recall that anytime we do a hypothesis test, we require the following:
1) Test statistic
2) Distribution of the test statistic
3) P-value from this distribution and the test statistic's observed value

In this case, the test statistic is denoted by $T$ since it has a **$T$ distribution**.   This test statistic $T$ is equal to the sample mean minus the null value (often 0) and then divided by the spread of the sample mean (called **standard error**).  $T$ distributions are differentiated by the parameter called **degrees of freedom** (similar to how normal distributions are differentiated by their mean and variance).  Using the value of $T$ with the value of the degrees of freedom and the $T$ distribution, components 1) and 2), we can calculate a p-value.  All three of these components are provided in the output from t.test(), along with the alternative hypothesis of the test (that the mean is not 0).

```{r t_test_AOSI_again}
t.test(data$V12.aosi.total_score_1_18)
```

We see that under a p-value of about 0, we have strong evidence in favor of the mean AOSI total score at 12 months not equalling 0.  Note that you can also see that 0 is quite outside the 95% confidence interval, indicating that 0 is not a reasonable value for the population mean based on this dataset (which points to the alternative hypothesis).  It turns out that p-values and confidence intervals are generally linked together and will agree in this fashion **by their design**.

Note that for this process to be valid, some assumptions are made.  They are a) AOSI total score at 12 months is normally distributed and b) all observations are independent.  If one or more of these is violated, the results from your one-sample t-test will be invalid.

Now suppose we wanted to compare mean AOSI total score at 12 months between two diagnosis groups, such as High Risk: ASD and High Risk: Negative.  Specifically, the null hypothesis will be that the means are the same between the two groups.  The common corresponding test is called a **two sample t-test**.  Again, we use the function t.test().  Here, we use y~x notation where x is the grouping variable to do the two sample test.  You will again obtain a test statistic t, degrees of freedom, and a p-value.  As in the one sample case, this test statistic also has a $T$ distribution.

```{r t_test_AOSI_2samp}
data_HR <- data %>%
  filter(GROUP=="HR_ASD"|GROUP=="HR_neg")

t.test(V12.aosi.total_score_1_18~GROUP,data=data_HR)
```

You can see R provides the following, starting from the top.
1) Test statistic t, degrees of freedom, and p-value
2) alternative hypothesis
3) 95% confidence interval for the **difference in the means**
4) sample means for each group

We see that, as expected, based on a p-value about 0, we have strong evidence that the mean AOSI total scores at 12 months are different between High Risk: ASD and High Risk: Negative.  This can also be seen in 0 being far away from the confidence interval and the sample means of 7.34 and 4.84 being quite different from one another.

#### ANOVA
Suppose we want to compare more then two groups' means.  For example, suppose we want to compare the mean AOSI total score at 12 months for the High Risk: ASD, High Risk: Negative, and Low Risk: Negative groups.  This is done using an **ANOVA F test**.  The null hypothesis is that the mean AOSI total score at 12 months is same in all three groups.  The alternative is that at least one group's mean differs from the rest.  We can conduct this test in R using **aov()**.  Again, it uses formula notation (y~x).  We conduct this test below.  Note that many components are calculated by R for this test.  To obtain the main values of interest, we must save the output as an object (which stores all of the components) and then use the function **summary()** on this object.

```{r f_test_AOSI}
aov_object <- aov(V12.aosi.total_score_1_18~GROUP, data=data)
summary(aov_object)
```

We see degrees of freedom are provided (3 and 508 in this example), the value of the test statistic (F value=16.68) and the corresponding p-value (2.39e-10 or essentially 0).  For an ANOVA F test, the test statistic $F$ has an $F$ distribution.  An $F$ distribution is defined by two degrees of freedom parameters.  Using this F statistic value and F distribution, the p-value can be calculated.

While an ANOVA F test will infer if the groups differ overall, we would like to see **which** groups differ.  This generally done by doing all of the pairwise comparisons.  For our example, that would entail hypothesis tests for 1) HR: ASD vs HR: Negative, 2) HR: ASD vs LR: Negative, 3) HR: Negative vs LR: Negative, etc. for 6 total tests.  Recall when comparing two groups' means, we can use a two sample t-test.  For each pairwise test, we use this two sample t-test.  Then, we interpret the test results for each pair separately.  This is frequently referred to as **post-hoc** analysis.  It turns out that because we are conducting multiple hypothesis tests at once, we need to "correct" each p-value from these pairwise comparisons to account for this **multiple comparison**.  Why this is the case is beyond the scope of these tutorials.  Generally, the correction done is called **Tukey's Method**, though other corrections such as **Bonferroni** or **Holm's Method** can also be used.  These **corrected p-values** can then be "validly" interpreted like usual p-values.  To compute this post-hoc analysis in R using Tukey's Method, use the function **TukeyASD()** with the object from aov().

```{r AOSI_posthoc}
TukeyHSD(aov_object)
```

The output contains the following:
1) Difference in sample means between the groups (diff)
2) 95% confidence intervals for each mean difference (lwr and upr)
3) Corrected p-values for each pairwise comparison (p adj)

This entire ANOVA analysis provides valid results under the following assumptions
1) All groups' values are normally distributed **with the same variance**
2) All observations are independent (both within the groups and between the groups)

The first assumption can be checked visually using a histogram and by estimating the variances with confidence intervals, or conducting hypothesis tests for equal variances (not covered here).  The second assumption is verified based on the study from which the data originated.
