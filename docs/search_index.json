[
["index.html", "Data Analysis and Processing with R based on IBIS data 1 Index 1.1 Preface", " Data Analysis and Processing with R based on IBIS data Kevin Donovan October 21, 2019 1 Index 1.1 Preface Over the course of my time working with the Carolina Insitute for Developmental Disabilities (CIDD) and the Infant Brain Imaging Study (IBIS) network, I have seen a great interest in learning how to do basic statistical analyses and data processing among the trainees. Specially, there is an interest in learning how to use R, due to its popularity across the sciences and its zero financial cost. As a statistican in training, I feel it is a great benefit for scientists to learn R. It is vital for scientists to understand the fundamentals of statistics to foster communication with the statisticans they are working with and learning some statistical computing facilitates this understanding. Furthermore, R has great tools for data processing, which is an essential first step in any data analysis. The objectives of this set of R tutorials are four-fold. Learn the interface of RStudio and understand the fundamentals of how R works (i.e., learn the “language” of R) Learn how to use the data processing tools in R Learn how to do basic data analysis methods in R (plotting, 1-way and 2-way tables, regression modeling, contingency table analyses, comparing means) Learn to present these results in a report directly through R (called R Markdown) Intermediate and advanced statistical analyses, such as machine learning techniques, are not covered in these tutorials. While exploratory and standard regression analyses are useful for non-statisticans to understand and learn how to do, these other types of analyses are beyond the goal of these tutorials. Much of the and content and structure of these tutorials will be based off of Hadley Wickham’s excellent book R for Data Science. For those who want more detail and some exercises for the techniques detailed here, I recommmend going through Wickham’s book. All examples and exercises detailed in these tutorials will be based on IBIS data. I hope these tutorials are useful and make R more inviting to use and learn, as after the inital learning curve I think you will find that R is an intuitive software for data analysis and processing. I would like to thank Shoba Sreenath Meera who is a post-doctoral Fullbright Scholar at the CIDD for the suggestion to create these tutorials, as well as for processing the datasets that were used in these tutorials. The examples using these datasets are essential for the usefulness of these tutorials. Without her work creating these datasets, these tutorials could not have been created. Thank you, Kevin Donovan PhD student in Biostatistics University of North Carolina-Chapel Hill "],
["introducing-r-and-rstudio.html", "2 Introducing R and Rstudio 2.1 Intro 2.2 Why use R for data analysis? 2.3 R and RStudio: What is the difference? 2.4 Installing R and RStudio 2.5 The interface of RStudio 2.6 File Directories in R", " 2 Introducing R and Rstudio 2.1 Intro The goal of this tutorial is to complete Objective 1; learn the interface of RStudio and understand the fundamentals of how R works. Those who understand the following can skip this tutorial: The “difference” between R and RStudio How to install and open R and Rstudio The interface of RStudio and why after installing both, one only needs to open and work in RStudio What packages are, what a working directory is, and how RStudio handles file locations/location references If you don’t fully understand these points, I strongly recommend going through this tutorial or just reading the specific sections which are necessary. 2.2 Why use R for data analysis? While R is necessary for many statisticians and data scientists due to it’s computation power and great flexibility, it may not be clear how R is useful for scientists looking to do data processing and simple analyses. However, R has many advantages over other popular data analysis software suites such as SAS or SPSS, even for investigators who do not require R’s advanced capabilities. R is free to download and use SAS and SPSS are very costly to use and/or require access through an employer license R is easy to download and install on Windows, Mac, and Linux R takes up less space to install then other such software R is open-source Users can expand the functionality of R through add-ons called packages Capabilities of R are continually growing as doesn’t require large-scale releases to expand functionality Data processing in R is very easy Can import datasets from most other programs, including Excel, SAS, and SPSS Creating subsets of your data, creating new variables, selecting specific observations and variables is very easy Great flexiblity in these tools Data visualization tools in R are very extensive Very flexible tools for creating custom graphs and tables Advanced functionality often used in practice by scienitists is available in R Very robust mixed modeling, principal component, factor analysis, structural equation modeling, etc. Will improve one’s understanding of statistics Key part of understanding statistics is through real data analysis Due to the breadth of its data analysis tools and the way R is operated, understanding R will foster an improved understanding of statistics It is very easy to share your output from R Can easily select to display only the output you are interested in Easy to save and load figures, datasets, etc. created in R Due to R using programming through scripts (to be discussed later), your analyses are completely and easily reproducibile and shareable Can save your results and code in a report-style form with notes using R Markdown (to be discussd later) R provides reproducibility for your analyses Use of scripts means every step of your analysis is documented and can be easily shared 2.3 R and RStudio: What is the difference? First time users of R are often confused as to the difference between R and RStudio. RStudio is actually an add-on to R: it takes the R software and adds to it a very user-friendly graphical interface. Thus, when one uses RStudio, they are still using the full version of R while also getting the benefit of greater functionality and usability due to an improved user interface. As a result, when using R, one should always use RStudio; working with R itself is very cumbersome. In these tutorials, from Chapter 3 onwards R will be used to reference R and RStudio with the assumption that the reader is operating in RStudio. Since RStudio is an add-on to R, you must first download and install R as well as RStudio, two steps which are done separately. On your computer, you will see R and RStudio as separate installed programs. When using R for data analysis, you will always open and work in RStudio; you must leave R installed on the computer for RStudio to work, even though you will likely never open R itself. 2.4 Installing R and RStudio As discussed above, you must download and install both R and RStudio. First, the installer for R can found by opening the following link: https://cran.r-project.org/mirrors.html and then selecting the mirror link closest your location. After opening this mirror link, you will see “Download and Install R” with links for Windows, Mac, and Linux installers. Always select the newest version posted. Then run the installer and follow the instructions. Second, for RStudio, the installer can found at the following link: https://www.rstudio.com/products/rstudio/download/ for Windows, Mac, and Linux (Ubuntu). Scroll due to “Installers for Supported Platforms”, open the choosen platform’s link, run the installer after downloading, and follow the instructions. The instructions for the installer will eventually ask you where R itself has when been installed. Generally it defaults to the correct path on your system for R though you may have to find where you have installed R and type the path into the RStudio installer manually. 2.5 The interface of RStudio When you open R Studio for the first time, you should see the following interface: 2.5.1 Console The large window on the left side is the Console. You can think of this as the “calculator” for R Studio. This is were all of the input, calculations and output are contained. In fact, if you were to run R and not R Studio, this Console is the only window you would see; R Studio adds all of the other interface components that you see. One simple command is to add two numbers. Let’s calculate 2+2 in R. The &gt; symbol indicates the current line in the Console, with the pointer at this line incidated by a blinking vertical bar. To input a command (2+2 for example) into R, you simply type this command in the current line and press enter. R will print the output from the command below the input. R will also print out messages corresponding to the input in the Console. In R, each command is considered a single line; you cannot have multiple commands on the same line unless you separate the commands in the single line with a ;. Try typing 2+2;3+3 in the Console and view the output. R will automatically seaprate the commands using ; and print each command’s output in a separate line. Now, try 2+2 3+3 in the Console. You see R returns an error indicating that something what wrong when R tried to execute the command. Note that R is not space sensitivty, meaning you can places as many spaces as you want between the components of your commands. For example, try the commands 2 + 2 in your console. You will see R completes the commands as expected without any errors. This holds for all commands in R and can be used to neatly format your code (especially when creating scripts which is discussed below). 2.5.2 Scripts While the Console forms the workhorse of R, operating solely in the Console is very cumbersome. Instead of typing your commands in the Console each time you run R, we will instead create a script. A script is a list of R commands that is saved as a text file to then be submitted into R line by line. Scripts are what makes R so useful as they allow easy reproducibility of your analysis since you have a typed list of what you did, as well as making your analysis easy to share with others since the typed list is a text file. Scripts are saved as a .R extension which can be read by most text editors (e.g., Notepad in Windows). To create a new script you can select File &gt; New File &gt; R Script at the top of R Studio. You should see the Console window go to the bottom and a new, empty window appear above the Console. This new window is where your scripts that are open will appear; each script that you have loaded is marked by a tab at the top of the window (notice there is now a single tab for your new script with the default name “Untitled1”). Anytime you change a script, the text becomes red and an * appears to indicate that it has been edited since it was last saved. To save the script, select the icon below the tabs that looks like a blue floppy disk. Scripts are saved as .R files, which are essentially text files (meaning they can also be edited and opened in a text editor such as Notepad on Windows). Each “line” in your script corresponds to a line to be inputted into the R console. You can extend a command across multiple lines in your script; R Studio is intellegent enough to interpret these lines as a single command/“line” for the Console. After you write these commands in your script, you then have R read them one by one into the Console. This is done by highlighting the lines of interest and selecting the Run button in the upper right; you also can use the shortcut CTRL+Enter in Windows. To quickly run all of the script, you can quickly highlight the whole script’s text using CTRL+A in Windows and then select Run (or CTRL+Enter). When running the script, you will see each line be fed into the Console below with the corresponding output and messages returned. 2.5.3 RMD Files Included with these tutorials are a set of scripts which were used to create all of the content contained in them. Usually, scripts are saved as .R files. However, you will notice that all of these are saved as .RMD files. These are called R Markdown files, which are explained in detail at a later chapter of these tutorials. However, it will be very useful when going through these tutorials to run the included scripts yourself to clearly see the code as well as obtain hands-on practice in running the code inside R. When you open the .RMD file, you will notice both the pain text used in that file’s tutorial as well as some R code inside of grey boxes called chunks. You can ignore the lines which begin and end with – for now. Each chunk is a set of R code, that is a “piece of a script”, with the entire “script” composed of the combination of these pieces. Each time you run the code in a chunk, the output from that code will be printed beneath the chunk. This makes it very easy to learn each piece of the code in an interaction fashion, as you can edit each chunk’s code just as you would edit a piece of your script. To run the code in a chunk, simply press the green triangle icon in the upper right corner of the chunk. To run all of the code in chunks above the chunk of interest, press the grey triangle icon directly to the left. This is needed when the chunk of interest depends on the results from chunks above it in the .RMD file. In this way, these .RMD files are like “advanced” versions of scripts, i.e. scripts with additional features (output prints below the chunk instead of in the console and you can easily accompany your R code with text detailing the code by using the white space outside the chunks). Thus, the best way to use these tutorials is the following. As you are reading through the HTML files, you should have the corresponding .RMD file open as well. Run each piece of code in the .RMD file as you come across it in the HTML file. In this way, you can combine your reading with hands-on practice by replicating what you have read at the same time. These R Markdown files are especially useful for creating detailed reports to document your statistical analyses. Please see the R Markdown chapter in these tutorials for more information. 2.5.4 Environment The upper right-hand window in the R Studio interface contains the Environment. See the Chapter 3 for more information on this window and the Environment in general. 2.5.5 Plots, Packages, and Help Finally, the lower right-hand window of the R Studio interface contains various components, each separated by a tab. The “Files” tab is a graphical way of opening files in R. Generally, it is easier to use the commands in R through your script so we move on to the next tab. The “Plots” tab is where all plots that you create in R will show up; see Chapter 5 for more details. The “Packages” tab is where you can install new packages and view the ones that you have already installed. Packages are add-ons for R that expand it’s capabilities. To install a new package, in the Packages tab select the Install button and type in the names of the package you are interested in. This works like a search engine where as you type, packages which have matching characters in their name will appear for you to select. Whenever you load R, you must always “turn on” any packages that are required for your analysis. This is most easily done by using the library() function. We discuss functions in detail later one; for now, to turn on package named ex, use the command library(ex). Include this command in your script at the beginning so that you load up the packages you need each time your run your script. Lastly, we have the “Help” tab. Here is where you can access the R documentation to help with any issues you encounter in R or to use as a source of information about R. 2.6 File Directories in R When working in R you often will have to reference various external files, for example datasets that you want to load. To reference the external file, you will use its file path. However, the concept of working directories makes this process much easier. It would be a pain to have to type the full file path when refering to an external files. The working directory is the default directory that R looks for when you specify a file name. That is, you just provide the file name, and R will look in the working directory for this file. To see what your working directory is, use the command get.wd(); R will print the working directory’s path in the Console. By default, when you load a script, R will set the working directory to the directory where the script is. To set a new working directory, use set.wd() and provide the directory’s name in quotes inside the parentheses. "],
["objects-and-functions.html", "3 Objects and Functions 3.1 Intro 3.2 Functions 3.3 Objects 3.4 Object classes and types 3.5 Subsetting 3.6 Base R 3.7 Environment 3.8 Putting It All Together: read.csv", " 3 Objects and Functions 3.1 Intro Data processing and analysis in R essentially boils due to creating output and saving that output, either temporarily to use later in your analysis or permanently onto your computer’s hard drive for later reference or to share with others. In R, output is created using commands called and when saving output temporarily to be used later in your R session, the output is saved as an . For example, suppose you have a previously saved Excel dataset you wish to analyze by performing linear regression. First, you must load this Excel dataset into R, which is done using a function. However, you want to give this read-in dataset a name so that you easily reference it in your R session. Thus, you save the read-in dataset as an R object called excel_data. Then, you use another function on excel_data to run the linear regression, whose results you store as another object called fit_results to be easily accessible in your R session. Note that objects excel_data and fit_results are not stored permanently on your computer but are instead stored on the computer’s RAM. Thus, when your computer is restarted, these R objects will be deleted (though the Excel file itself is a permanent file on your computer: the form of the Excel dataset created when read-in by R is not permanent). A similar example is detailed in the following code using an IBIS dataset, where a linear regression model is fit where the outcome is AOSI Total Score at Month 12 and the independent variable is Age at Month 12. # Read-in Excel dataset using function called read.csv read.csv(&quot;Data/cross-sec_aosi.csv&quot;) # look at the console: running command reads-in data, however data is not saved; cannot reference dataset using name since we have only read it in, not given it a name/saved it in R excel_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings = c(&quot;.&quot;, &quot;&quot;, &quot; &quot;)) # are telling R to take output from function read.csv (the read-in data) and save it as object excel_data # Can see dataset no longer pops up in console: when storing output as object, R does not print the output for you excel_data # running just the object is a command: tells R to print out the output stored in the object. Now can easily reference dataset set using excel_data # Run linear regression using ASOSI Total Score as outcome, Diagnosis as predictor variable. Running regression models is discussed in detailed in Chapter 8 lm(V12.aosi.total_score_1_18~V12.aosi.Candidate_Age, data=excel_data) # using AOSI at month 12 ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ V12.aosi.Candidate_Age, ## data = excel_data) ## ## Coefficients: ## (Intercept) V12.aosi.Candidate_Age ## 3.98890 0.07879 You can see that the end result is the estimated intercept and slope from your linear regression model. By the end of all of these tutorials, you will understand every step of the above code. By the end of this tutorial, you should be able to understand the above code in general as a series of functions and objects. 3.2 Functions Every time R is given a command, R produces the output specified from the command. As a simple example, suppose you wish to add 2 and 2. 2+2 ## [1] 4 After running the above line of code, you will see R output the result of the addition in the console. The command + is called a function (specifically an operation, a class of commands/functions). For usability, mathematical operators such as +,-,/, and * have this form, although most functions have the following form: fun(x,y,z) where fun is the name of the function, and x,y, and z are options the user places in the function which modify how it operates. These options are called arguments. This example function has 3 arguments (x,y,z), however functions can have many more arguments for the user to manipulate. Note that all functions have default values for most of their arguments which will automatically be used without the user having to specify them. As a more interesting example, suppose you want to read-in some data into R which you will analyze. The dataset is of a sample of children from IBIS, and includes site, geneder, and AOSI information at month 6 and month 12 visit. The dataset is found in the file cross-sec_aosi.csv. To do read this file in, we must choose the right function so R knows what we want it to do. A function that will do this command is read.csv(). This function has default values for all of its arguments, except for the first one where you much specify the file path of CSV dataset you want to read-in. First, let’s leave the default values as is and read-in the data. read.csv(&quot;Data/cross-sec_aosi.csv&quot;) From the previous tutorial, since the file is in our working directory, we only need to specify the file name and not the whole path. Notice that the file path needs to be in quotes (use of quotes will be discussed later in this chapter). After you run the code, you will notice the dataset is printed in the R console. For reading in other files types, different functions are needed. Many of these are found in the package haven which includes functions such as read_sas() for SAS files and read_spss() for SPSS files. When learning and using functions it is useful to submit the command ?fun where fun is the name of the chosen function. For example, try running ?read.csv in your script. # ?read.csv You will see in the lower right-hand window, the Help tab will open up along with a description of the function read.csv(), including all of its arguments, some notes about the function and its usage, and some examples to refer to. However, this is not a visually pleasing way to view the data. Furthermore, this dataset has not be given a name in R to reference, which will make using this data difficult to use in your R script All R has done is read-in the CSV data, converted it into a format it understands, and then printed this output. Actually storing this output in R and giving it a name so that it can be easily referenced later involves creating objects. 3.3 Objects Output generated in R can be temporarily saved in your R session with a name to be referenced later. This saved output is called an object. The most intuitive example is reading in data to be analyzed. Let’s read-in the previous Excel sheet and save it as an object named excel_data. To save output as an object with a name, the command is name &lt;- fun(…) excel_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;) Now the dataset can be easily accessed by using excel_data in your code. For example, one way of intuitively viewing data in your R session is using the function View() (note: case matters in R). 3.4 Object classes and types Now that objects have been introduced, we will go into further detail about them which will be vital when using functions and understanding errors. While you can use R without understanding this section, learning this information will help you understand the structure of R which is essential when using R on real data. Every object in R is classified into a class and a type. The object’s class represents the general structure of the object as a whole, while the type is a more specific description of the actual contents of the object. These concepts are best explained with examples. The classes that you will see likely see in practice are a vector, matrix, list, and data frame. The types you will see in pratice are numeric, character (letters/words), factor (group levels), and logical (TRUE/T or FALSE/F). Numeric means the values are interpreted as numbers. Character type refers to values which are “strings” (combinations) of letters, words, symbols, numbers, etc. surrounded by quotation marks. Factor types are explained later in this chapter. The values TRUE and FALSE (caps required, no quotations) have a specific meaning in R, and objects with these values are referred to as logical types (these are not character types, values not surrounded by quotes). For all types, the value NA (caps required, no quotes) is interpreted by R as a missing value; NA is the only value in R which refers to a missing value. 3.4.1 Vectors A vector is a single-dimension collection of values, all of which must have the same type. For example, (0,1,2,3) is a numeric vector of length 4, and (“Yes”, “No”, “Maybe”) is a character vector of length 3. When working with a dataset, each row can be considered as a vector as can each column. As with all objects, these vectors can be created and saved with a name in R. This is done using c(…) where the elements of the vector are place inside () separated by commas. Some examples are found below. You can think of a vector as either a variable (column) in your dataset or an observation (row) in your dataset. To index vectors, use [] where the indicies of interest are placed inside the []; see below for an example. One function of use with vectors is length(), which will output the number of values in the vector. c(0,1,2,3) # outputs as vector ## [1] 0 1 2 3 c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Maybe&quot; num_vect &lt;- c(0,1,2,3) # save vector as named object char_vect &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) num_vect # outputs saved vector in console ## [1] 0 1 2 3 char_vect ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Maybe&quot; num_vect[1:3] # a:b creates sequence of integers from a to b ## [1] 0 1 2 char_vect[c(1,2)] # selecting the 1st and 2nd entries ## [1] &quot;Yes&quot; &quot;No&quot; char_vect[1,2] # Creates an error; need to have single entity inside [] ## Error in char_vect[1, 2]: incorrect number of dimensions length(num_vect) # length=3, vector has 3 elements ## [1] 4 3.4.2 Matricies A matrix is a two-dimensional collection of values, all of which need to be the same type. When viewing a matrix in R, it largely looks like an Excel spreadsheet. In standard data analysis, matricies are rarely used so they are not covered here in much detail. An example of a matrix is shown below; they can created using the matrix() function, though you will rarely if ever create matricies in standard data analysis. It is more important to know how to manipulate them, which is discussed in a later section of this chapter. Notice that although numbers 1,2,3 were specified as entries, these show up as “1”,“2”, and “3” (characters) respectively in the outputted matrix. This called coercion, where R forces values into a certain type to meet the requires of the class of the object. It is important to recognize when coercion happens and why, since some commands only work on specific types of values (for example when taking the mean of a variable in your dataset, the values need to be numeric). matrix(c(1,2,3,&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;), nrow=3, ncol=2) # numbers forced into characters to match type with the non-numeric entries of the matrix ## [,1] [,2] ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; You can also combine multiple vectors into a matrix, either row-wise or column-wise. This is done using the rbind() and cbind() functions respectively. nums &lt;- c(1,2,3) # create numeric vector words &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) # notice since the vectors were saved as objects before binding, the names show up as names for the rows for rbind and columns for cbind respectively. In a matrix, the rows and/or cols can be given names as shown here rbind(nums, words) ## [,1] [,2] [,3] ## nums &quot;1&quot; &quot;2&quot; &quot;3&quot; ## words &quot;Yes&quot; &quot;No&quot; &quot;Maybe&quot; cbind(nums, words) ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; 3.4.3 Lists Recall the two above classes required all elements be of the same type; for example, you cannot have both number and character values in the object. Lists allow values to be of different types. Essentially, a list of a collection of other objects. Generally, lists will be a collection of different vectors and/or matricies. For example, your list could be composed of 2 vectors, one of which with numeric values and the other with character values. Any combination of vectors and/or matricies is allowed, along with many other possibilities (including a list composed of multiple lists). Lists are created using the list() function, as shown in the following example. nums &lt;- c(1,2,3) # create numeric vector words &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) # create character # store the 2 vectors together as a list list(nums, words) ## [[1]] ## [1] 1 2 3 ## ## [[2]] ## [1] &quot;Yes&quot; &quot;No&quot; &quot;Maybe&quot; Each element of the list is indexed by [[]]; in the above example, you can see the 2 vectors used to create the list. Notice the numeric vector does not have quotes surrounding it; within the list, the vector is still considered numeric as desired. Lists are useful since they allow you to hold objects that are of different classes together in a single entity, making them easier to keep track of and reference later. You will see lists come up very frequently when using R for data analysis and it is important to become comfortable with them. 3.4.4 Data Frames Generally, when you read-in data as an object, it will be stored in R as a data frame. Essentially, a data frame is a list of the individual columns/variables which compose the dataset, where the columns are considered vectors. That is, a data frame is a list of vectors, but when outputted is shown like a matrix with rows and columns instead of the usual look of a list. While data frames can be viewed in the console like all objects by just entering their name, it is most useful to see them using the View() function. Another useful function for data frames (and matricies) is dim(), which will output the number of observations (rows) and columns (variables) in your dataset. The function names() will output the column (variable) names of your dataset. This function can also be used in a similar way with vectors, matricies, and lists. excel_data &lt;- read.csv(&quot;Data/AOSI_small.csv&quot;) # read-in data as data frame, save as named object excel_data # view dataset in console; hard to read especially for datasets of usual size ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 1 HR_ASD PHI Male 7.1 ## 2 2 HR_ASD PHI Male 6.1 ## 3 3 HR_ASD PHI Male 6.6 ## 4 4 HR_ASD PHI Male . ## 5 5 HR_neg PHI Male 6.8 ## 6 6 HR_neg PHI Male . ## 7 7 HR_ASD PHI Male 6.2 ## 8 8 HR_ASD PHI Male 6.2 ## 9 9 HR_neg PHI Male . ## 10 10 HR_ASD PHI Male . ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 8 12.5 ## 2 18 12.4 ## 3 4 12.9 ## 4 . 12.7 ## 5 6 . ## 6 . 12.1 ## 7 16 12.3 ## 8 10 12.2 ## 9 . 12.4 ## 10 . 12.2 ## V12.aosi.total_score_1_18 ## 1 3 ## 2 10 ## 3 2 ## 4 3 ## 5 . ## 6 4 ## 7 4 ## 8 14 ## 9 1 ## 10 7 # View(excel_data) # can see dataset pops up in tab in upper right window, easy to see/scroll through names(excel_data) ## [1] &quot;Identifiers&quot; &quot;GROUP&quot; ## [3] &quot;Study_Site&quot; &quot;Gender&quot; ## [5] &quot;V06.aosi.Candidate_Age&quot; &quot;V06.aosi.total_score_1_18&quot; ## [7] &quot;V12.aosi.Candidate_Age&quot; &quot;V12.aosi.total_score_1_18&quot; dim(excel_data) ## [1] 10 8 3.4.5 Types Object types are a more intuitive idea and have been mentioned above; types describe the actual values inside the object. Recall the common types are character (letters/words), numeric, factor (group levels), and logical (TRUE/T or FALSE/F). Recall that for vectors and matricies, all entries must be the same type. Thus, a vector/matrix with all numeric values is of type numeric, with all character values is of type character, with all logical vaues is of type logical, etc. Since lists and data frames of made of up a collection of vectors, matricies, and/or lists, they do not have an explicit “type”. To see what type an object is, you can use the function typeof(). Note that when a list or data frame is inputted into the function, “list” or “df” respectively is returned. num_vect &lt;- c(1,2,3,4) char_vect &lt;- c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;) log_vect &lt;- c(T, F, TRUE, FALSE) typeof(num_vect) ## [1] &quot;double&quot; typeof(char_vect) ## [1] &quot;character&quot; typeof(log_vect) ## [1] &quot;logical&quot; matrix_ex &lt;- cbind(num_vect, char_vect, log_vect) typeof(matrix_ex) ## [1] &quot;character&quot; list_ex &lt;- list(num_vect, char_vect, log_vect, matrix_ex) list_ex ## [[1]] ## [1] 1 2 3 4 ## ## [[2]] ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ## ## [[3]] ## [1] TRUE FALSE TRUE FALSE ## ## [[4]] ## num_vect char_vect log_vect ## [1,] &quot;1&quot; &quot;a&quot; &quot;TRUE&quot; ## [2,] &quot;2&quot; &quot;b&quot; &quot;FALSE&quot; ## [3,] &quot;3&quot; &quot;c&quot; &quot;TRUE&quot; ## [4,] &quot;4&quot; &quot;d&quot; &quot;FALSE&quot; typeof(list_ex) # returns &quot;list&quot;; lists have no single type such as numeric, character, etc. ## [1] &quot;list&quot; The factor type requires some explanation. Most character variables you will work with indicate group membership; for example a gender variable recorded as “Male”, “Female”, or “Other” or a treatment variable recorded as “Treatment” or “Control”. We want to treat these as groups, not separate words/strings. This can be done using the factor type. Note for many functions were you want the variable to be treated as a factor, for example in linear regression, R will convert the character variable into an appropriate factor variable. If the variable is actually numeric but you want it to reflect group membership (for example a 0, 1 variable indicating treatment), you likely will have to convert it yourself. A factor variable, i.e. a vector of type factor, has two parts: the vector of values as well as a vector indicating what the factor “levels” (essentially labels for the groups) are. For example, consider the following character variable indicating the genders in a set of observations. # M is male, F is female, O is other genders &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;O&quot;, &quot;O&quot;) genders # print character vector ## [1] &quot;M&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;F&quot; &quot;O&quot; &quot;O&quot; Let’s convert this to a factor variable; by default, R will automatically set the factor levels to the unique values in the variable (M, F, and O in this case). This is done using the factor() function. To see the levels assigned to a factor variable, use the function levels(). # M is male, F is female, O is other genders_fact &lt;- factor(genders) genders_fact ## [1] M F M M F F F O O ## Levels: F M O levels(genders_fact) ## [1] &quot;F&quot; &quot;M&quot; &quot;O&quot; To alter the levels, you overwrite the levels() output as seen below. Since the levels of a factor variable are actually a R object, you edit them by overwriting this object which is accessed using the levels() function. We overwrite the levels object by specifying a new character vector to define the levels; can see from the levels() call that they are saved as a character vector. After the levels are changed, you will see these changes also occur in the variable values. Note that you can also convert back to a character/numeric variable by using the as.character()/as.numeric() functions respectively. # Original variable, levels genders_fact ## [1] M F M M F F F O O ## Levels: F M O levels(genders_fact) ## [1] &quot;F&quot; &quot;M&quot; &quot;O&quot; # Overwrite levels levels(genders_fact) &lt;- c(&quot;Female&quot;, &quot;Male&quot;, &quot;Other&quot;) # pay attention to order of levels, need to follow order as F-&gt;Female, M-&gt;Male, O-&gt;Other. # New variable, levels genders_fact ## [1] Male Female Male Male Female Female Female Other Other ## Levels: Female Male Other levels(genders_fact) ## [1] &quot;Female&quot; &quot;Male&quot; &quot;Other&quot; # convert back to character as.character(genders_fact) ## [1] &quot;Male&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Other&quot; ## [9] &quot;Other&quot; # Can also do numeric levels levels(genders_fact) &lt;- c(1, 0, 2) genders_fact ## [1] 0 1 0 0 1 1 1 2 2 ## Levels: 1 0 2 # convert to numeric as.numeric(genders_fact) ## [1] 2 1 2 2 1 1 1 3 3 Most of the time in standard data analysis, you won’t have to manually convert variables to factor variables. When reading-in a dataset, you can have R automatically convert character variables to factors (will be covered in the next Chapter). When doing regression, R will automatically convert character variables to factors as well. However, for completeness and to understand certain errors which may occur when using R, it is useful to understand what factors are. Understanding types is very important for using R. Certain functions only work on specific types; for example, you cannot compute the mean of a variable of character values. This concept will help you understand some errors that may come up. 3.5 Subsetting Often times you will need to select specific parts of an object to view or using in your analysis; for example, computing the mean of a specific variable in your dataset or viewing only specific observations in your dataset based on some chosen critieria. In these tutorials, this is referred to as subsetting. There are 2 main ways to subset objects in R: 1) using Base R functions and 2) using functions from the dplyr package. Base R refers to the functions built into R by default; i.e., functions that do not require a package to be installed in order to access. You will likely use the dplyr method most of the time. However, for completeness and since the Base R method is useful to understand how R operates, it is important that you understand both. 3.6 Base R The tools for subsetting in Base R differ depending on what class your object is. These tutorials cover the classes discussed previously: vectors, matricies, lists, and data frames. In general, for object x of any class, subsetting is done using x[…], where the values placed inside [] will determine how the object is subset. Note that when subsetting, you are not changing the original object x, but are creating a new object which is a “subset”\" of x. First, let’s discuss this operation for vectors. Recall vectors are single-dimension collections of values of the same type, for example x=(1,2,3,4). Vectors are subset by stating the indicies you either want to select or omit inside the []. Suppose we wanted to keep 1 from the vector (1,2,3,4). Since this is the 1st value in the vector by order from left to right, we would use x[1]. If we want to keep all values except 1, i.e. (2,3,4), we would use x[-1]; adding a negative to an index removes its value. If you want to select multiple indicies to include or remove, for example (2,3), you must use x[c(2,3)]. That is, you have provide these indices as a vector inside the []; R does not understand what 2,3 is unless represented as a vector. See below for some examples of this subsetting in action. num &lt;- c(2,4,6,8,10) char &lt;- c(&#39;a&#39;,&#39;e&#39;,&#39;i&#39;,&#39;o&#39;,&#39;u&#39;) num[1] ## [1] 2 num[-4] ## [1] 2 4 6 10 num[c(1,3)] # correct ## [1] 2 6 num[1:3] # 1:3 is interpretated in R as c(1,2,3), this notation is useful as a shortcut when selecting many values ## [1] 2 4 6 char[1] ## [1] &quot;a&quot; char[-4] ## [1] &quot;a&quot; &quot;e&quot; &quot;i&quot; &quot;u&quot; char[c(1,3)] # correct ## [1] &quot;a&quot; &quot;i&quot; char[1:3] # 1:3 is interpretated in R as c(1,2,3), this notation is useful as a shortcut when selecting many values ## [1] &quot;a&quot; &quot;e&quot; &quot;i&quot; # num[c(1,-2)] # cannot mix positive and negative indicies, running line will give error # char[c(1,-2)] # cannot mix positive and negative indicies, running line will give error Second, let’s discuss matricies. Working with matricies is similar to working with vectors, the main difference being you can select indicies for two dimensions (rows and columns) instead of just one. Suppose x is a matrix. Then we can subset x using x[,] where row indicies go to the left of the comma and column indicies go to the right. To select all rows/columns leave the right/left side empty respectively. The rules for selecting indicies are the same as before: use a vector when selecting multiple indicies, use negative indicies to omit a row/column, and do not mix positive and negative indicies. If your matrix’s columns are named, you can also refer to these names inside [] as strings instead of referring to the index of the column. See below for some examples. nums &lt;- c(1,2,3) # create numeric vector words &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) logs &lt;- c(TRUE, FALSE, TRUE) matrix &lt;- cbind(nums, words, logs) matrix[,c(&quot;nums&quot;,&quot;words&quot;)] # select words and nums columns using strings, as this matrix has column names ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; matrix[,c(1,2)] # select words and nums columns by indicies ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; matrix[1,c(&quot;nums&quot;,&quot;words&quot;)] # select words and nums columns, select only 1st row ## nums words ## &quot;1&quot; &quot;Yes&quot; Third, let’s discuss lists. Recall lists of collections of other objects, such as vectors, matricies, or other lists. Lists introduce [[]] as subset operator, used to select the specific objects in the collection. You can think of [[]] as selecting “outer” elements, and then using [] to subset these outer elements. You will seldom be subsetting lists, and generally will using be using [[]] when you do subset them. Another way to subset lists instead of [[]] is using $. Suppose you have a list x which contains two objects with the names x_1 and x_2 respectively. Then to select the first object, you either using x[[1]] from before or using x$x_1, or using x[[“x_1”]]. This will be referred to again next when we discuss data frames. Some examples are provided below. nums &lt;- c(1,2,3) # create numeric vector words &lt;- c(&quot;Yes&quot;, &quot;No&quot;, &quot;Maybe&quot;) logs &lt;- c(TRUE, FALSE, TRUE) matrix &lt;- cbind(nums, words) example_lst &lt;- list(matrix, logs) # create a list containing a matrix and vector, without names example_lst # notice the [[]] indexing each part of the list ## [[1]] ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; ## ## [[2]] ## [1] TRUE FALSE TRUE example_lst &lt;- list(x_1=matrix, x_2=logs) # create a list containing a matrix and vector, with names x_1 and x_2 respectively example_lst # notice the [[]] indicies have been replaced with their specified names ## $x_1 ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; ## ## $x_2 ## [1] TRUE FALSE TRUE example_lst[[1]] # selects the matrix from the list, its first element ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; example_lst$x_1 # also selects the matrix ## nums words ## [1,] &quot;1&quot; &quot;Yes&quot; ## [2,] &quot;2&quot; &quot;No&quot; ## [3,] &quot;3&quot; &quot;Maybe&quot; example_lst[[2]] # selects the vector ## [1] TRUE FALSE TRUE example_lst[[2]][c(1,2)] # after selecting the vector, we can subset the vector using [] as discussed earlier ## [1] TRUE FALSE Lastly, let’s discuss data frames. First, recall that a data frame is the type that is generally used when reading in or working with a dataset. Thus, it looks like a matrix; it has rows for the observations and columns for the variables. As a result, you can subset it using the same techniques as were used with matricies, with the column names being the variable names and the row indicies representing individual observations. However, also recall that a data frame is atcually a list of vectors representing the columns/variables of the dataset, but visualized in a row by column form. Thus, you can also subset a data frame using the $ technique for lists, where the variable name follows $ (since each variable is a element of the list). Some examples are provided below. excel_data &lt;- read.csv(&quot;Data/AOSI_small.csv&quot;) # read-in data as data frame, save as named object excel_data$GROUP # select diagnosis group ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_neg HR_neg HR_ASD HR_ASD HR_neg HR_ASD ## Levels: HR_ASD HR_neg excel_data[,&quot;GROUP&quot;] # select diagnosis group using matrix-style subsetting ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_neg HR_neg HR_ASD HR_ASD HR_neg HR_ASD ## Levels: HR_ASD HR_neg excel_data[1:5,&quot;GROUP&quot;] # select diagnosis group and 1st 5 observations, using matrix-style subsetting ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_neg ## Levels: HR_ASD HR_neg When working with real datasets, you will likely manipulate them using the dplyr package and not Base R as discussed here. The next set of tutorials discusses using dplyr. 3.7 Environment Every time you create an object in R, you may notice the upper right window of R Studio start to populate with rows. This window displays the current R Environment (more specifically the global environment, though this distinction will not be important in most cases). For most analyses, this window will serve as a helpful reference to see what you have saved in your R session and what each object name refers to. It can also be used to easily open and view one of your R objects, which can be very helpful when you have loaded in datasets. First, note that there are two graphical styles that you can choose for this window using a button on the top, far right of the window. This button will state either List or Grid, depending on what style you currently are using. Just click the button and choose the option you want from the drop-down menu to change this style. Grid is generally the most user-friendly style. One nice feature of this style is the easy deletion of objects from your Environment (and thus, your computer’s memory). Simply select the blank, white squares next to the objects you wish to delete and press the Broom icon to delete them. To select all objects, select the blank, white square at the top of the window and press the Broom icon. Grid style with saved object x 3.8 Putting It All Together: read.csv Now that we understand R functions and objects, let us see how these concepts relate by again reading in a dataset using read.csv(). Again, we read in the AOSI data and save it in R as an object called excel_data. excel_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;) When working with a dataset, it is important to make sure all of the variables are of the correct object type (numeric, character, etc.) and that missing values are correctly labeled as NA when the dataset is read into R. This because, from before, functions operate differently based on the type of data that is being used (recall the example where we tried to take the mean of a character variable). We can view the type of each column using sapply(excel_data, class). The sapply() function is more advanced so we do not cover it here; those that are interested can view Chapter 10. sapply(excel_data, class) ## Identifiers GROUP ## &quot;integer&quot; &quot;factor&quot; ## Study_Site Gender ## &quot;factor&quot; &quot;factor&quot; ## V06.aosi.Candidate_Age V06.aosi.total_score_1_18 ## &quot;factor&quot; &quot;factor&quot; ## V12.aosi.Candidate_Age V12.aosi.total_score_1_18 ## &quot;factor&quot; &quot;factor&quot; We see that variables which we would think of as numeric such as AOSI total score and Age are factor variables. Let’s look at month 12 AOSI total score more closely. excel_data$V06.aosi.total_score_1_18 ## [1] 8 18 4 . 6 . 16 10 . . 8 10 . 9 6 11 . . 3 18 14 . 12 ## [24] 14 18 . . 5 5 5 12 11 10 7 8 12 10 8 9 11 16 9 10 18 12 11 ## [47] 13 16 13 12 5 7 14 8 13 9 12 15 10 8 10 5 9 12 10 12 10 12 ## [70] 12 12 11 9 12 9 9 . 15 10 10 3 . 2 9 19 10 10 11 11 8 4 ## [93] 18 6 8 6 . 15 9 12 16 9 10 12 15 11 8 10 20 8 4 14 11 15 22 ## [116] 16 . 9 14 12 13 22 20 10 12 12 6 14 20 12 11 7 10 . 5 . ## [139] 10 12 11 . 12 . . 14 . 9 19 12 . 6 7 9 11 . 15 10 13 10 19 ## [162] 7 8 8 15 5 17 5 8 6 8 12 8 17 8 6 8 15 11 6 5 5 6 13 ## [185] 9 7 5 15 4 6 9 6 4 3 14 1 4 9 9 24 12 6 5 5 ## [208] . 5 15 9 10 7 10 11 8 10 13 5 12 . 6 3 3 7 12 6 17 10 ## [231] . 6 11 8 . 12 5 11 16 11 7 17 16 8 12 8 10 10 11 11 12 7 5 ## [254] 8 14 . 17 8 7 3 5 14 12 . 10 12 4 7 6 . 7 . 12 10 12 . ## [277] 10 15 12 12 12 2 7 7 . . . 2 11 11 . 9 15 . . . 7 9 17 ## [300] 4 6 6 . 11 12 . . . 11 14 15 11 14 17 6 . 10 4 5 9 7 7 ## [323] . . 18 9 . . . 17 9 . 13 11 16 7 12 17 . 11 11 12 8 3 ## [346] 8 9 8 8 14 11 . 6 17 . . 28 16 11 11 7 15 . . 4 21 6 8 ## [369] 10 5 9 14 13 12 7 10 . 15 7 . . 8 8 7 12 11 9 8 11 6 3 ## [392] 9 8 9 4 7 8 14 6 9 15 7 9 6 11 6 6 8 9 8 7 19 15 18 ## [415] 10 12 5 5 . 7 9 . . . 14 11 4 6 10 10 7 5 . 17 . ## [438] . 17 . 9 . . 13 . 11 9 . . 2 13 4 5 7 8 4 9 6 ## [461] 5 . 7 . 6 11 15 8 7 5 . 4 9 6 6 6 3 6 7 7 2 5 11 ## [484] 4 9 5 10 5 16 . 7 4 8 9 . 17 7 8 7 . 8 . 4 9 . 24 ## [507] 13 5 7 8 6 12 5 . 3 8 9 6 6 9 7 7 6 10 11 7 14 4 . ## [530] 7 8 5 7 7 5 5 12 10 15 9 12 10 9 13 6 . 7 . . 7 . 11 ## [553] 3 7 7 6 3 7 13 7 8 4 13 15 . . 8 . 3 5 8 2 6 8 ## [576] 16 5 . 12 . 9 7 7 5 6 10 ## 26 Levels: . 1 10 11 12 13 14 15 16 17 18 19 2 20 21 22 24 28 3 4 5 ... 9 We see that the Excel sheet data for this variable has values in the form “.” and \" “, which are meant to denote missing values. R considers these entries to be character values. As a result, since each column is a vector and thus each column needs to be a single type, R reads this column as a character variable. However, didn’t R tell us that this was a factor variable? This is due to the argument stringsAsFactors in read.csv(). This argument is set to TRUE by default; hence, R converts the character variable to a factor variable. This is generally what we to do with variables that are supposed to be character variables as you will see later (for example, gender). Setting it to FALSE keeps the variables as character variables. However, we do not want to consider variables like AOSI total score as character or factor variables. To do this, we need to tell R to consider the values”.\" and \" \" as missing values or NA. If this is done, then R will read the column as a collection of numbers and NA values, resulting in numeric being chosen as the variable’s type. To do this, we use the na.strings argument. By default, this argument is “NA”, implying that any values marked “NA” in the Excel sheet will be marked as missing (NA) when read in R. However, we often use different values to denote missing values (as seen here with “.” and \" \"). To select the values to be denoted as missing, we use na.strings=c(“.”, \" \") in this example. That is, we provide a vector which contains the strings used to mark missing values as the value for na.strings. excel_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) excel_data$V06.aosi.total_score_1_18 ## [1] 8 18 4 NA 6 NA 16 10 NA NA 8 10 NA 9 6 11 NA NA 3 18 14 NA 12 ## [24] 14 18 NA NA 5 5 5 12 11 10 7 8 12 10 8 9 11 16 9 10 18 12 11 ## [47] 13 16 13 12 5 NA 7 14 8 13 9 12 15 10 8 10 5 9 12 10 12 10 12 ## [70] 12 12 11 9 12 NA 9 9 NA 15 10 10 3 NA 2 9 19 10 10 11 11 8 4 ## [93] 18 6 8 6 NA 15 9 12 16 9 10 12 15 11 8 10 20 8 4 14 11 15 22 ## [116] 16 NA NA 9 14 12 13 22 20 10 12 12 NA 6 14 20 12 11 7 10 NA 5 NA ## [139] 10 12 11 NA 12 NA NA 14 NA 9 19 12 NA 6 7 9 11 NA 15 10 13 10 19 ## [162] 7 8 8 15 5 17 5 8 6 8 12 8 17 8 6 8 15 11 6 5 5 6 13 ## [185] 9 7 5 15 4 6 9 6 4 3 14 1 4 9 9 24 NA 12 NA 6 NA 5 5 ## [208] NA 5 15 9 10 7 10 11 8 10 13 5 12 NA 6 3 3 7 NA 12 6 17 10 ## [231] NA 6 11 8 NA 12 5 11 16 11 7 17 16 8 12 8 10 10 11 11 12 7 5 ## [254] 8 14 NA 17 8 7 3 5 14 12 NA 10 12 4 7 6 NA 7 NA 12 10 12 NA ## [277] 10 15 12 12 12 2 7 7 NA NA NA 2 11 11 NA 9 15 NA NA NA 7 9 17 ## [300] 4 6 6 NA 11 12 NA NA NA 11 14 15 11 14 17 6 NA 10 4 5 9 7 7 ## [323] NA NA 18 9 NA NA NA NA 17 9 NA 13 11 16 7 12 17 NA 11 11 12 8 3 ## [346] 8 9 8 8 14 11 NA 6 17 NA NA 28 16 11 11 7 15 NA NA 4 21 6 8 ## [369] 10 5 9 14 13 12 7 10 NA 15 7 NA NA 8 8 7 12 11 9 8 11 6 3 ## [392] 9 8 9 4 7 8 14 6 9 15 7 9 6 11 6 6 8 9 8 7 19 15 18 ## [415] 10 12 5 NA 5 NA 7 9 NA NA NA 14 11 4 6 10 10 7 NA 5 NA 17 NA ## [438] NA 17 NA 9 NA NA NA 13 NA 11 9 NA NA NA 2 13 4 5 7 8 4 9 6 ## [461] 5 NA 7 NA 6 11 15 8 7 5 NA 4 9 6 6 6 3 6 7 7 2 5 11 ## [484] 4 9 5 10 5 16 NA 7 4 8 9 NA 17 7 8 7 NA 8 NA 4 9 NA 24 ## [507] 13 5 7 8 6 12 5 NA 3 8 9 6 6 9 7 7 6 10 11 7 14 4 NA ## [530] 7 8 5 7 7 5 5 12 10 15 9 12 10 9 13 6 NA 7 NA NA 7 NA 11 ## [553] 3 7 7 6 3 7 13 7 8 4 13 15 NA NA 8 NA NA 3 5 8 2 6 8 ## [576] 16 5 NA 12 NA 9 7 7 5 NA 6 10 sapply(excel_data, class) ## Identifiers GROUP ## &quot;integer&quot; &quot;factor&quot; ## Study_Site Gender ## &quot;factor&quot; &quot;factor&quot; ## V06.aosi.Candidate_Age V06.aosi.total_score_1_18 ## &quot;numeric&quot; &quot;integer&quot; ## V12.aosi.Candidate_Age V12.aosi.total_score_1_18 ## &quot;numeric&quot; &quot;integer&quot; Now, we see that missing values are marked by NA and numeric variables are correctly read-in as numeric (or integer which is a type of numeric variable) by R. "],
["using-dplyr.html", "4 Using dplyr 4.1 Intro 4.2 dplyr Functions", " 4 Using dplyr 4.1 Intro The most intutive way to manipulate your data is using the functions inside the dplyr package, which is downloaded and installed into R (as all packages are, see Chapter 2 for more details). “Manipulating” your data refers to choosing a subset of the variables and/or observations in your dataset, as well as filtering (selecting observations based on their variable values), creating new variables, and more. Now that you understand the interface of R and R Studio and how to work with objects and functions, the rest of these tutorials will introduce techniques in R designed to address specific needs in standard data analyses. We start with data manipulation. 4.2 dplyr Functions The package dplyr has many functions, each designed to do a specific task in data manipulation. The main ones are: select(), filter(), arrange(),mutate(), and summarize(). We will cover all of these, as well as other functions such as group_by(), spread(), gather(), separate(), and unite(). As with all packages, we first must load the package after it has been installed. # install.packages(&quot;dplyr&quot;) library(dplyr) # loads package 4.2.1 Select, filter, and arrange The functions select(), filter(), and arrange() handle the usual subsetting operations discussed in the previous chapter. We will go through each one in order. The select() function is used to include or omit chosen variables (columns) from the dataset, as well as rearrange the order of the variables. The syntax is the following select(dataset, variable_1, variable_2, …) where the argument “dataset” is where you state the name of the object that represents the dataset of interest, and “variable_1”, “variable_2”,… are the arguments where you state the name of the variables/columns you want to include or remove (to remove the variable, begin the name with a -) in the order you would like them to apear in the dataset. There is no limit (I believe) to the number of variables that can be referenced. You can reference an ordered collection of variables (ordered meaning from left to right in the dataset) using variable_i:variable_j as an argument in the function, where “variable_i” is the name of the first variable and “variable_j” is the name of the last. This idea works with - as well. Lastly, unlike with subsetting in Base R, you can reference variables to include and remove (with a -) in a single select() function call. The filter() function is used to reduce the dataset to a subset of observations based on a set of criteria for their variable values. The syntax is the following filter(dataset, criteria_1, criteria_2, …) where the arguments “criteria_1”, “criteria_2”,… are where you state the conditions needed for a observation to be included. Note that these are AND conditions, meaning an observation has to meet all provided criteria to be included and OR conditions where an observation has to meet at least one of the provided criteria. Let’s go into more detail on a AND condition and an OR condition, as well as logical statements in general. In R, a logical statement is an operator that returns the value TRUE, FALSE, or NA (missing). They are conditions where if the condition is met, TRUE is returned, if not FALSE is returned, and if the variable in the condition is missing NA is returned. Conditional operators include &gt; (greater then), &lt; (less then), equal (==), &gt;= (greater then or equal to), &lt;= (less then or equal to), and ! (not). Consider the following simple examples. x &lt;- 5 x==5 ## [1] TRUE !(x==5) # not equal ## [1] FALSE x!=5 # also not equal ## [1] FALSE x&gt;3 ## [1] TRUE x&lt;3 ## [1] FALSE !(x&lt;3) ## [1] TRUE x&lt;=5 ## [1] TRUE Logical statements can be combined and evaluated as a single statement using AND and OR statements. An AND statement means all individual statements must be TRUE to be evaluated as TRUE, otherwise is FALSE (or NA) while an OR statement means at least one statement must be TRUE to be evaluated as TRUE, and is FALSE only if all statements are FALSE. AND statements are created by separating individual logical statements with &amp; while OR statements are created by separating individual logical statements with |. Parentheses may also be needed so that R evaluates the individual statements in the correct order. Consider the following examples. x &lt;- 5 y &lt;- 3 x==5|y==4 ## [1] TRUE x==5 &amp; y==4 ## [1] FALSE x==5 &amp; y==3 ## [1] TRUE x&gt;3 &amp; x&lt;7 ## [1] TRUE !(x&gt;3 &amp; x&lt;7) # negates TRUE expression =&gt; FALSE returned ## [1] FALSE Now, let’s apply select() and filter() to some IBIS data. We will use the small AOSI dataset from before, and do the following: Select specific variables to keep Filter observations by AOSI total score and diagnosis group Remember that all changes are not saved nor do they overwrite the original dataset object in R, unless done by the user by either creating a new object with these changes or saving these new changes as an object of the same R object name as the dataset. Even if one changes the original R object, the Excel sheet is not changed (we will see later how to have the changes done in R be reflected in either the original Excel sheet or in a new Excel sheet). excel_data &lt;- read.csv(&quot;Data/AOSI_small.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) # read-in data as data frame, save as named object # View whole dataset excel_data ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 1 HR_ASD PHI Male 7.1 ## 2 2 HR_ASD PHI Male 6.1 ## 3 3 HR_ASD PHI Male 6.6 ## 4 4 HR_ASD PHI Male NA ## 5 5 HR_neg PHI Male 6.8 ## 6 6 HR_neg PHI Male NA ## 7 7 HR_ASD PHI Male 6.2 ## 8 8 HR_ASD PHI Male 6.2 ## 9 9 HR_neg PHI Male NA ## 10 10 HR_ASD PHI Male NA ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 8 12.5 ## 2 18 12.4 ## 3 4 12.9 ## 4 NA 12.7 ## 5 6 NA ## 6 NA 12.1 ## 7 16 12.3 ## 8 10 12.2 ## 9 NA 12.4 ## 10 NA 12.2 ## V12.aosi.total_score_1_18 ## 1 3 ## 2 10 ## 3 2 ## 4 3 ## 5 NA ## 6 4 ## 7 4 ## 8 14 ## 9 1 ## 10 7 # Select only ID and Group and view data select(excel_data, Identifiers, GROUP) ## Identifiers GROUP ## 1 1 HR_ASD ## 2 2 HR_ASD ## 3 3 HR_ASD ## 4 4 HR_ASD ## 5 5 HR_neg ## 6 6 HR_neg ## 7 7 HR_ASD ## 8 8 HR_ASD ## 9 9 HR_neg ## 10 10 HR_ASD # Remove gender and site from data select(excel_data, -Gender, -Study_Site) ## Identifiers GROUP V06.aosi.Candidate_Age V06.aosi.total_score_1_18 ## 1 1 HR_ASD 7.1 8 ## 2 2 HR_ASD 6.1 18 ## 3 3 HR_ASD 6.6 4 ## 4 4 HR_ASD NA NA ## 5 5 HR_neg 6.8 6 ## 6 6 HR_neg NA NA ## 7 7 HR_ASD 6.2 16 ## 8 8 HR_ASD 6.2 10 ## 9 9 HR_neg NA NA ## 10 10 HR_ASD NA NA ## V12.aosi.Candidate_Age V12.aosi.total_score_1_18 ## 1 12.5 3 ## 2 12.4 10 ## 3 12.9 2 ## 4 12.7 3 ## 5 NA NA ## 6 12.1 4 ## 7 12.3 4 ## 8 12.2 14 ## 9 12.4 1 ## 10 12.2 7 # Only include observations with ASD diagnosis filter(excel_data, GROUP==&quot;HR_ASD&quot;) ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 1 HR_ASD PHI Male 7.1 ## 2 2 HR_ASD PHI Male 6.1 ## 3 3 HR_ASD PHI Male 6.6 ## 4 4 HR_ASD PHI Male NA ## 5 7 HR_ASD PHI Male 6.2 ## 6 8 HR_ASD PHI Male 6.2 ## 7 10 HR_ASD PHI Male NA ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 8 12.5 ## 2 18 12.4 ## 3 4 12.9 ## 4 NA 12.7 ## 5 16 12.3 ## 6 10 12.2 ## 7 NA 12.2 ## V12.aosi.total_score_1_18 ## 1 3 ## 2 10 ## 3 2 ## 4 3 ## 5 4 ## 6 14 ## 7 7 # only include observations with ASD diagnosis and month 6 visit AOSI total score of at least 10 filter(excel_data, GROUP==&quot;HR_ASD&quot; &amp; V06.aosi.total_score_1_18&gt;=10) ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 2 HR_ASD PHI Male 6.1 ## 2 7 HR_ASD PHI Male 6.2 ## 3 8 HR_ASD PHI Male 6.2 ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 18 12.4 ## 2 16 12.3 ## 3 10 12.2 ## V12.aosi.total_score_1_18 ## 1 10 ## 2 4 ## 3 14 # only include Female observations filter(excel_data, Gender==&quot;Female&quot;) # Blank dataset returned, only have male subjects ## [1] Identifiers GROUP ## [3] Study_Site Gender ## [5] V06.aosi.Candidate_Age V06.aosi.total_score_1_18 ## [7] V12.aosi.Candidate_Age V12.aosi.total_score_1_18 ## &lt;0 rows&gt; (or 0-length row.names) Finally, the arrange() function is used to reorder the observations in the dataset based on specific variables. The syntax for the function is arrange(dataset, variable1, variable2, …) where variable1, variable2, … are the variables you wish to used for the reordering. Simply stating the variable name will order the dataset by increasing values of the variable. To use a decreasing order, use desc(variable) instead. arrange(excel_data, GROUP, V12.aosi.Candidate_Age) # increasing age ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 8 HR_ASD PHI Male 6.2 ## 2 10 HR_ASD PHI Male NA ## 3 7 HR_ASD PHI Male 6.2 ## 4 2 HR_ASD PHI Male 6.1 ## 5 1 HR_ASD PHI Male 7.1 ## 6 4 HR_ASD PHI Male NA ## 7 3 HR_ASD PHI Male 6.6 ## 8 6 HR_neg PHI Male NA ## 9 9 HR_neg PHI Male NA ## 10 5 HR_neg PHI Male 6.8 ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 10 12.2 ## 2 NA 12.2 ## 3 16 12.3 ## 4 18 12.4 ## 5 8 12.5 ## 6 NA 12.7 ## 7 4 12.9 ## 8 NA 12.1 ## 9 NA 12.4 ## 10 6 NA ## V12.aosi.total_score_1_18 ## 1 14 ## 2 7 ## 3 4 ## 4 10 ## 5 3 ## 6 3 ## 7 2 ## 8 4 ## 9 1 ## 10 NA arrange(excel_data, GROUP, desc(V12.aosi.Candidate_Age)) # decreasing age ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 3 HR_ASD PHI Male 6.6 ## 2 4 HR_ASD PHI Male NA ## 3 1 HR_ASD PHI Male 7.1 ## 4 2 HR_ASD PHI Male 6.1 ## 5 7 HR_ASD PHI Male 6.2 ## 6 8 HR_ASD PHI Male 6.2 ## 7 10 HR_ASD PHI Male NA ## 8 9 HR_neg PHI Male NA ## 9 6 HR_neg PHI Male NA ## 10 5 HR_neg PHI Male 6.8 ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 4 12.9 ## 2 NA 12.7 ## 3 8 12.5 ## 4 18 12.4 ## 5 16 12.3 ## 6 10 12.2 ## 7 NA 12.2 ## 8 NA 12.4 ## 9 NA 12.1 ## 10 6 NA ## V12.aosi.total_score_1_18 ## 1 2 ## 2 3 ## 3 3 ## 4 10 ## 5 4 ## 6 14 ## 7 7 ## 8 1 ## 9 4 ## 10 NA arrange(excel_data, desc(GROUP), V12.aosi.Candidate_Age) # &#39;decreasing GROUP&#39;, notice R uses alphabetical order when character variables are used ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 6 HR_neg PHI Male NA ## 2 9 HR_neg PHI Male NA ## 3 5 HR_neg PHI Male 6.8 ## 4 8 HR_ASD PHI Male 6.2 ## 5 10 HR_ASD PHI Male NA ## 6 7 HR_ASD PHI Male 6.2 ## 7 2 HR_ASD PHI Male 6.1 ## 8 1 HR_ASD PHI Male 7.1 ## 9 4 HR_ASD PHI Male NA ## 10 3 HR_ASD PHI Male 6.6 ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 NA 12.1 ## 2 NA 12.4 ## 3 6 NA ## 4 10 12.2 ## 5 NA 12.2 ## 6 16 12.3 ## 7 18 12.4 ## 8 8 12.5 ## 9 NA 12.7 ## 10 4 12.9 ## V12.aosi.total_score_1_18 ## 1 4 ## 2 1 ## 3 NA ## 4 14 ## 5 7 ## 6 4 ## 7 10 ## 8 3 ## 9 3 ## 10 2 4.2.2 Mutate and summarize The functions mutate() and summarize() are used to create new variables to add to the dataset. First, we cover mutate(). The syntax for this function is when used with the pipe is mutate(dataset, variable1, variable2, …) where variable1, variable2, … are the definitions of the variables you want to add. They will be calculated for each observation in the dataset. We consider an IBIS dataset of high risk children (HR) with Vineland, Mullen, and AOSI information. Suppose we wanted create the following variables 1) variable containing each child’s mean Mullen composite standard score across the 4 visits in the dataset and 2) variable indicating if the site of the visit is on the East Coast, Midwest, or West Coast of the United States. We can easily do this using mutate(), as shown below. # Create data set of only HR children HR_data &lt;- read.csv(&quot;Data/Cross-sec_full.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) HR_data &lt;- filter(HR_data, V24.demographics.Risk==&quot;HR&quot;) # Create variables using mutate() HR_data &lt;- mutate(HR_data, Mullen_Mean=(`V06.mullen.composite_standard_score`+`V12.mullen.composite_standard_score`+ `V24.mullen.composite_standard_score`+`V36.mullen.composite_standard_score`)/4, Site_Location=factor(ifelse(Study_Site==&quot;PHI&quot;|Study_Site==&quot;UNC&quot;,&quot;East_Coast&quot;, ifelse(Study_Site==&quot;STL&quot;,&quot;Midwest&quot;,&quot;West_Coast&quot;)))) ftable(HR_data$Study_Site, HR_data$Site_Location) ## East_Coast Midwest West_Coast ## ## PHI 108 0 0 ## SEA 0 0 101 ## STL 0 110 0 ## UNC 90 0 0 From the above example, you will notice the use of the function ifelse(). Using this function, you can create a series of if…else commands so that R execute pieces of code based on specified criteria. The syntax for ifelse() is ifelse(condition, action, else) where condition is some logical statement to test (i.e., this condition needs to return TRUE or FALSE), action is the command R will execute if the condition is TRUE, and else is the command R will execute if the condition if FALSE. As you can see in the above example, you ca create a series of if…else commands by chaining ifelse() calls by setting the else argument to another call to ifelse(). The ifelse chain used in the above example is interpreted as follows. If site is UNC or site is PHI, set variable Site_Location’s value to East_Coast Else, if site is STL, set variable Site_Location’s value to Midwest Else, if 1) and 2) are not true, set variable Site_Location’s value to West_Coast (since site then must be SEA) Make sure to keep track of missing values; in our example, there are no missing values for site so this was not a concern. If there were missing values for site, the variable Site_Location would be set to West_Coast and not NA as desired. The function summarize() is used to reduce a dataset to a set of chosen summary statistics such as the mean, sample size, standard deviation, etc. The syntax is summarize(dataset, variable1, variable2, …) where variable1, variable2, … are the definitions of the summary statistics you wish to calculate. For example, suppose in the AOSI dataset, we want to create a dataset containing the mean values of total score at month 6 and at month 12. This can be done using summarise(). aosi_data &lt;-read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) summarise(aosi_data, sample_size=n(), mean_aosi_ts_V6=mean(V06.aosi.total_score_1_18), mean_aosi_ts_V12=mean(V12.aosi.total_score_1_18)) ## sample_size mean_aosi_ts_V6 mean_aosi_ts_V12 ## 1 587 NA NA This function becomes much more useful when combined with the function group_by(). The calling group_by() on the dataset before using summarise() will have R incorporate the created groups in calculations; after calling group_by(), while the data may look to the same to us, R has marked observations as being grouped together in the background. Let’s repeat the above example, but first group by gender and diagnosis. aosi_data_grouped &lt;- group_by(aosi_data, Gender, GROUP) # Notice in the environment window, this object is marked as type grouped_... summarise(aosi_data_grouped, sample_size=n(), mean_aosi_ts_V6=mean(V06.aosi.total_score_1_18), mean_aosi_ts_V12=mean(V12.aosi.total_score_1_18)) ## # A tibble: 8 x 5 ## # Groups: Gender [2] ## Gender GROUP sample_size mean_aosi_ts_V6 mean_aosi_ts_V12 ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female HR_ASD 20 NA NA ## 2 Female HR_neg 145 NA NA ## 3 Female LR_ASD 1 6 5 ## 4 Female LR_neg 69 NA NA ## 5 Male HR_ASD 75 NA NA ## 6 Male HR_neg 173 NA NA ## 7 Male LR_ASD 2 8 3 ## 8 Male LR_neg 102 NA NA We can see the summary statistics are calculated for each specified group. There are many summary statistics that can be calculated using the summarise() function; note that due to missing values, some of the means are NA (this can be changed by adding na.omit=TRUE as arguments to the calls to mean(). For a list of some of these statistics and how to implement them, use ?summarise. 4.2.3 Spread, Gather, Separate and Unite The dplyr package also has functions used to transform the dimensions of your data, for example going from wide to long. Recall the full dataset has data on children for multiple time points; thus, it will be essential for certain analyses to have the data in long form or in wide form. The functions spread(), gather(), separate(), and unite() can be used to do these transformations. We show all four functions through an example. For more explanation of these functions, please see and from Hadley Wickham’s excellent book R for Data Science. Consider the AOSI_small.csv data set. Recall, we previously loaded this dataset into R and saved it as excel_data. This dataset has the child’s age and AOSI total score at 6 months and 12 months and is “wide” form (1 row per child). Let us transform this to “long” form (multiple rows per subject). excel_data ## Identifiers GROUP Study_Site Gender V06.aosi.Candidate_Age ## 1 1 HR_ASD PHI Male 7.1 ## 2 2 HR_ASD PHI Male 6.1 ## 3 3 HR_ASD PHI Male 6.6 ## 4 4 HR_ASD PHI Male NA ## 5 5 HR_neg PHI Male 6.8 ## 6 6 HR_neg PHI Male NA ## 7 7 HR_ASD PHI Male 6.2 ## 8 8 HR_ASD PHI Male 6.2 ## 9 9 HR_neg PHI Male NA ## 10 10 HR_ASD PHI Male NA ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 8 12.5 ## 2 18 12.4 ## 3 4 12.9 ## 4 NA 12.7 ## 5 6 NA ## 6 NA 12.1 ## 7 16 12.3 ## 8 10 12.2 ## 9 NA 12.4 ## 10 NA 12.2 ## V12.aosi.total_score_1_18 ## 1 3 ## 2 10 ## 3 2 ## 4 3 ## 5 NA ## 6 4 ## 7 4 ## 8 14 ## 9 1 ## 10 7 First, we extract the variable names which represent the repeated measures (age and AOSI total score) for the children. vars_to_convert &lt;- names(excel_data)[5:8] vars_to_convert ## [1] &quot;V06.aosi.Candidate_Age&quot; &quot;V06.aosi.total_score_1_18&quot; ## [3] &quot;V12.aosi.Candidate_Age&quot; &quot;V12.aosi.total_score_1_18&quot; Then, we force each of these variables to be it’s own row, creating two new variables; 1) “variable” which contains the variable’s name and 2) “var_value” with the value of the variable. This is done using gather(). With gather(), first you the generate new variable name which holds the old variable, second you specify the new variable name which holds of the value of the old variable, and thirdly you specify the names of the old variables. Notice the use of %&gt;% in the following code. This symbol is called the pipe and is used to connect various lines together, instead of writing and saving each line as a separate object to then be overwritten. We discuss the pipe in detail in a later part of this chapter; for now, all you need to understand is that it connects each of the proceeding lines together into a single command. library(tidyr) excel_data %&gt;% gather(variable, var_value, vars_to_convert) ## Identifiers GROUP Study_Site Gender variable ## 1 1 HR_ASD PHI Male V06.aosi.Candidate_Age ## 2 2 HR_ASD PHI Male V06.aosi.Candidate_Age ## 3 3 HR_ASD PHI Male V06.aosi.Candidate_Age ## 4 4 HR_ASD PHI Male V06.aosi.Candidate_Age ## 5 5 HR_neg PHI Male V06.aosi.Candidate_Age ## 6 6 HR_neg PHI Male V06.aosi.Candidate_Age ## 7 7 HR_ASD PHI Male V06.aosi.Candidate_Age ## 8 8 HR_ASD PHI Male V06.aosi.Candidate_Age ## 9 9 HR_neg PHI Male V06.aosi.Candidate_Age ## 10 10 HR_ASD PHI Male V06.aosi.Candidate_Age ## 11 1 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 12 2 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 13 3 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 14 4 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 15 5 HR_neg PHI Male V06.aosi.total_score_1_18 ## 16 6 HR_neg PHI Male V06.aosi.total_score_1_18 ## 17 7 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 18 8 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 19 9 HR_neg PHI Male V06.aosi.total_score_1_18 ## 20 10 HR_ASD PHI Male V06.aosi.total_score_1_18 ## 21 1 HR_ASD PHI Male V12.aosi.Candidate_Age ## 22 2 HR_ASD PHI Male V12.aosi.Candidate_Age ## 23 3 HR_ASD PHI Male V12.aosi.Candidate_Age ## 24 4 HR_ASD PHI Male V12.aosi.Candidate_Age ## 25 5 HR_neg PHI Male V12.aosi.Candidate_Age ## 26 6 HR_neg PHI Male V12.aosi.Candidate_Age ## 27 7 HR_ASD PHI Male V12.aosi.Candidate_Age ## 28 8 HR_ASD PHI Male V12.aosi.Candidate_Age ## 29 9 HR_neg PHI Male V12.aosi.Candidate_Age ## 30 10 HR_ASD PHI Male V12.aosi.Candidate_Age ## 31 1 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 32 2 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 33 3 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 34 4 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 35 5 HR_neg PHI Male V12.aosi.total_score_1_18 ## 36 6 HR_neg PHI Male V12.aosi.total_score_1_18 ## 37 7 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 38 8 HR_ASD PHI Male V12.aosi.total_score_1_18 ## 39 9 HR_neg PHI Male V12.aosi.total_score_1_18 ## 40 10 HR_ASD PHI Male V12.aosi.total_score_1_18 ## var_value ## 1 7.1 ## 2 6.1 ## 3 6.6 ## 4 NA ## 5 6.8 ## 6 NA ## 7 6.2 ## 8 6.2 ## 9 NA ## 10 NA ## 11 8.0 ## 12 18.0 ## 13 4.0 ## 14 NA ## 15 6.0 ## 16 NA ## 17 16.0 ## 18 10.0 ## 19 NA ## 20 NA ## 21 12.5 ## 22 12.4 ## 23 12.9 ## 24 12.7 ## 25 NA ## 26 12.1 ## 27 12.3 ## 28 12.2 ## 29 12.4 ## 30 12.2 ## 31 3.0 ## 32 10.0 ## 33 2.0 ## 34 3.0 ## 35 NA ## 36 4.0 ## 37 4.0 ## 38 14.0 ## 39 1.0 ## 40 7.0 Next, we split the “variable” into two new variables; 1) contains the visit and 2) contains the variable at that visit. This is done using separate(). With separate(), first you specify the variable which you are separating, second you specify the names of the new variables created from the separating, and third you specify where in the old variable you want to separate (here we spit after the 3rd character, hence sep=3). library(tidyr) excel_data %&gt;% gather(variable, var_value, vars_to_convert) %&gt;% separate(variable,c(&quot;Visit&quot;,&quot;Variable&quot;),sep=3) ## Identifiers GROUP Study_Site Gender Visit Variable ## 1 1 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 2 2 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 3 3 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 4 4 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 5 5 HR_neg PHI Male V06 .aosi.Candidate_Age ## 6 6 HR_neg PHI Male V06 .aosi.Candidate_Age ## 7 7 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 8 8 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 9 9 HR_neg PHI Male V06 .aosi.Candidate_Age ## 10 10 HR_ASD PHI Male V06 .aosi.Candidate_Age ## 11 1 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 12 2 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 13 3 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 14 4 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 15 5 HR_neg PHI Male V06 .aosi.total_score_1_18 ## 16 6 HR_neg PHI Male V06 .aosi.total_score_1_18 ## 17 7 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 18 8 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 19 9 HR_neg PHI Male V06 .aosi.total_score_1_18 ## 20 10 HR_ASD PHI Male V06 .aosi.total_score_1_18 ## 21 1 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 22 2 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 23 3 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 24 4 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 25 5 HR_neg PHI Male V12 .aosi.Candidate_Age ## 26 6 HR_neg PHI Male V12 .aosi.Candidate_Age ## 27 7 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 28 8 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 29 9 HR_neg PHI Male V12 .aosi.Candidate_Age ## 30 10 HR_ASD PHI Male V12 .aosi.Candidate_Age ## 31 1 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 32 2 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 33 3 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 34 4 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 35 5 HR_neg PHI Male V12 .aosi.total_score_1_18 ## 36 6 HR_neg PHI Male V12 .aosi.total_score_1_18 ## 37 7 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 38 8 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## 39 9 HR_neg PHI Male V12 .aosi.total_score_1_18 ## 40 10 HR_ASD PHI Male V12 .aosi.total_score_1_18 ## var_value ## 1 7.1 ## 2 6.1 ## 3 6.6 ## 4 NA ## 5 6.8 ## 6 NA ## 7 6.2 ## 8 6.2 ## 9 NA ## 10 NA ## 11 8.0 ## 12 18.0 ## 13 4.0 ## 14 NA ## 15 6.0 ## 16 NA ## 17 16.0 ## 18 10.0 ## 19 NA ## 20 NA ## 21 12.5 ## 22 12.4 ## 23 12.9 ## 24 12.7 ## 25 NA ## 26 12.1 ## 27 12.3 ## 28 12.2 ## 29 12.4 ## 30 12.2 ## 31 3.0 ## 32 10.0 ## 33 2.0 ## 34 3.0 ## 35 NA ## 36 4.0 ## 37 4.0 ## 38 14.0 ## 39 1.0 ## 40 7.0 Finally, we force the non-visit index variable to be separate columns instead of a single column of variable names. This is done using spread(). With spread(), first we specify the list of variables (key=) and second we specify the list of values of these variables (value=) library(tidyr) excel_data %&gt;% gather(variable, var_value, vars_to_convert) %&gt;% separate(variable,c(&quot;Visit&quot;,&quot;Variable&quot;),sep=3) %&gt;% spread(key=Variable, value=var_value) ## Identifiers GROUP Study_Site Gender Visit .aosi.Candidate_Age ## 1 1 HR_ASD PHI Male V06 7.1 ## 2 1 HR_ASD PHI Male V12 12.5 ## 3 2 HR_ASD PHI Male V06 6.1 ## 4 2 HR_ASD PHI Male V12 12.4 ## 5 3 HR_ASD PHI Male V06 6.6 ## 6 3 HR_ASD PHI Male V12 12.9 ## 7 4 HR_ASD PHI Male V06 NA ## 8 4 HR_ASD PHI Male V12 12.7 ## 9 5 HR_neg PHI Male V06 6.8 ## 10 5 HR_neg PHI Male V12 NA ## 11 6 HR_neg PHI Male V06 NA ## 12 6 HR_neg PHI Male V12 12.1 ## 13 7 HR_ASD PHI Male V06 6.2 ## 14 7 HR_ASD PHI Male V12 12.3 ## 15 8 HR_ASD PHI Male V06 6.2 ## 16 8 HR_ASD PHI Male V12 12.2 ## 17 9 HR_neg PHI Male V06 NA ## 18 9 HR_neg PHI Male V12 12.4 ## 19 10 HR_ASD PHI Male V06 NA ## 20 10 HR_ASD PHI Male V12 12.2 ## .aosi.total_score_1_18 ## 1 8 ## 2 3 ## 3 18 ## 4 10 ## 5 4 ## 6 2 ## 7 NA ## 8 3 ## 9 6 ## 10 NA ## 11 NA ## 12 4 ## 13 16 ## 14 4 ## 15 10 ## 16 14 ## 17 NA ## 18 1 ## 19 NA ## 20 7 Finally, we clean up the formatting of the variables using the functions discussed previously (rename(), mutate(), etc.). We save this new transformed dataset as long_data. library(tidyr) long_data &lt;- excel_data %&gt;% gather(variable, var_value, vars_to_convert) %&gt;% separate(variable,c(&quot;Visit&quot;,&quot;Variable&quot;),sep=3) %&gt;% spread(key=Variable, value=var_value) %&gt;% plyr::rename(c(&quot;.aosi.Candidate_Age&quot;=&quot;AOSI_Age&quot;, &quot;.aosi.total_score_1_18&quot;=&quot;AOSI_Total_Score&quot;)) %&gt;% mutate(ASD_Diag = factor(ifelse(grepl(&quot;ASD&quot;, GROUP), &quot;ASD_Neg&quot;, &quot;ASD_Pos&quot;)), Visit=factor(Visit)) %&gt;% arrange(Identifiers, Visit) long_data ## Identifiers GROUP Study_Site Gender Visit AOSI_Age AOSI_Total_Score ## 1 1 HR_ASD PHI Male V06 7.1 8 ## 2 1 HR_ASD PHI Male V12 12.5 3 ## 3 2 HR_ASD PHI Male V06 6.1 18 ## 4 2 HR_ASD PHI Male V12 12.4 10 ## 5 3 HR_ASD PHI Male V06 6.6 4 ## 6 3 HR_ASD PHI Male V12 12.9 2 ## 7 4 HR_ASD PHI Male V06 NA NA ## 8 4 HR_ASD PHI Male V12 12.7 3 ## 9 5 HR_neg PHI Male V06 6.8 6 ## 10 5 HR_neg PHI Male V12 NA NA ## 11 6 HR_neg PHI Male V06 NA NA ## 12 6 HR_neg PHI Male V12 12.1 4 ## 13 7 HR_ASD PHI Male V06 6.2 16 ## 14 7 HR_ASD PHI Male V12 12.3 4 ## 15 8 HR_ASD PHI Male V06 6.2 10 ## 16 8 HR_ASD PHI Male V12 12.2 14 ## 17 9 HR_neg PHI Male V06 NA NA ## 18 9 HR_neg PHI Male V12 12.4 1 ## 19 10 HR_ASD PHI Male V06 NA NA ## 20 10 HR_ASD PHI Male V12 12.2 7 ## ASD_Diag ## 1 ASD_Neg ## 2 ASD_Neg ## 3 ASD_Neg ## 4 ASD_Neg ## 5 ASD_Neg ## 6 ASD_Neg ## 7 ASD_Neg ## 8 ASD_Neg ## 9 ASD_Pos ## 10 ASD_Pos ## 11 ASD_Pos ## 12 ASD_Pos ## 13 ASD_Neg ## 14 ASD_Neg ## 15 ASD_Neg ## 16 ASD_Neg ## 17 ASD_Pos ## 18 ASD_Pos ## 19 ASD_Neg ## 20 ASD_Neg 4.2.4 Renaming variables The final function in dplyr we discuss is rename(), used for renaming variables. The syntax for this function is rename(data, variable1_new_name=variable1_old_name, variable2_new_name=variable2_old_name, …). See the example below where the ID variable and diagnosis group variable are renamed. rename_ex &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) %&gt;% rename(ID=Identifiers, Diagnosis_group=GROUP) rename_ex[1:10,] ## ID Diagnosis_group Study_Site Gender V06.aosi.Candidate_Age ## 1 1 HR_ASD PHI Male 7.1 ## 2 2 HR_ASD PHI Male 6.1 ## 3 3 HR_ASD PHI Male 6.6 ## 4 4 HR_ASD PHI Male NA ## 5 5 HR_neg PHI Male 6.8 ## 6 6 HR_neg PHI Male NA ## 7 7 HR_ASD PHI Male 6.2 ## 8 8 HR_ASD PHI Male 6.2 ## 9 9 HR_neg PHI Male NA ## 10 10 HR_ASD PHI Male NA ## V06.aosi.total_score_1_18 V12.aosi.Candidate_Age ## 1 8 12.5 ## 2 18 12.4 ## 3 4 12.9 ## 4 NA 12.7 ## 5 6 NA ## 6 NA 12.1 ## 7 16 12.3 ## 8 10 12.2 ## 9 NA 12.4 ## 10 NA 12.2 ## V12.aosi.total_score_1_18 ## 1 3 ## 2 10 ## 3 2 ## 4 3 ## 5 NA ## 6 4 ## 7 4 ## 8 14 ## 9 1 ## 10 7 4.2.5 Using the pipe All of these functions in dplyr can be used in combination efficiently by using the pipe operator. The idea of the pipe is very simple; instead of writing each new data transformation as a new line and saving each previous transformation as an object to then overwrite, the pipe allows you to connect all of the steps together in a single line. The pipe operator is denoted by %&gt;%. Many functions can be connected using the pipe, including all of the dplyr package functions described above along with most of the other functions you will use in R when reading-in and manipulating data. For example, let’s revisit calculating the mean AOSI scores by gender and site. Previously, this required multiple lines of R code, including creating a separate form of the dataset as an R object in which the dataset was grouped by gender and site. Let’s do this example using the pipe operator, seen below. You can see it is essentially the same code as before, with the first arguments of each function removed and each line connected by the operator %&gt;%. The first argument is removed because by connecting the function calls with %&gt;%, the result of the previous function is carried forward into the first argument of the next function by %&gt;%. Thus, any function in which the first argument is an R dataset can bse used with the pipe operator. Notice also that no intermediate objects need to be saved. aosi_data %&gt;% group_by(Gender, GROUP) %&gt;% summarise(sample_size=n(), mean_aosi_ts_V6=mean(V06.aosi.total_score_1_18), mean_aosi_ts_V12=mean(V12.aosi.total_score_1_18)) ## # A tibble: 8 x 5 ## # Groups: Gender [2] ## Gender GROUP sample_size mean_aosi_ts_V6 mean_aosi_ts_V12 ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female HR_ASD 20 NA NA ## 2 Female HR_neg 145 NA NA ## 3 Female LR_ASD 1 6 5 ## 4 Female LR_neg 69 NA NA ## 5 Male HR_ASD 75 NA NA ## 6 Male HR_neg 173 NA NA ## 7 Male LR_ASD 2 8 3 ## 8 Male LR_neg 102 NA NA As a result, using the pipe operator makes your code more efficient (memory-wise in your computer and in the amount of code used) and readable. In fact, you can even avoid saving the dataset aosi_data all together and writing your code in the folowing fashion. read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) %&gt;% group_by(Gender, GROUP) %&gt;% summarise(sample_size=n(), mean_aosi_ts_V6=mean(V06.aosi.total_score_1_18), mean_aosi_ts_V12=mean(V12.aosi.total_score_1_18)) ## # A tibble: 8 x 5 ## # Groups: Gender [2] ## Gender GROUP sample_size mean_aosi_ts_V6 mean_aosi_ts_V12 ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female HR_ASD 20 NA NA ## 2 Female HR_neg 145 NA NA ## 3 Female LR_ASD 1 6 5 ## 4 Female LR_neg 69 NA NA ## 5 Male HR_ASD 75 NA NA ## 6 Male HR_neg 173 NA NA ## 7 Male LR_ASD 2 8 3 ## 8 Male LR_neg 102 NA NA As a result, no objects are saved in R, making this code very efficient if you do not plan on using the aosi_data dataset for eany other analyses. If you want to save the results of this code as a new dataset, just use new_data &lt;- … as usual to save the output as a data object in R named new_data. This underscores how R is a language and has fundamental rules which allow your instructions to be understood by the software. If you can understand these rules and properties, you can apply them in general and understand how R is operating under the surface. This knowledge allows you to go beyond memorizing specific commands and examples, providing the ability to continually learn new techniques in R. new_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, na.strings=c(&quot;.&quot;, &quot; &quot;)) %&gt;% group_by(Gender, GROUP) %&gt;% summarise(sample_size=n(), mean_aosi_ts_V6=mean(V06.aosi.total_score_1_18), mean_aosi_ts_V12=mean(V12.aosi.total_score_1_18)) new_data ## # A tibble: 8 x 5 ## # Groups: Gender [2] ## Gender GROUP sample_size mean_aosi_ts_V6 mean_aosi_ts_V12 ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Female HR_ASD 20 NA NA ## 2 Female HR_neg 145 NA NA ## 3 Female LR_ASD 1 6 5 ## 4 Female LR_neg 69 NA NA ## 5 Male HR_ASD 75 NA NA ## 6 Male HR_neg 173 NA NA ## 7 Male LR_ASD 2 8 3 ## 8 Male LR_neg 102 NA NA 4.2.6 Editing factor variables: recode() and relevel() Recall categorical variables are referred to as factor variables in R. Every factor variable also includes a set of levels which is an vector containing the group labels, in a specific order. Often times you will want to either 1) change these group levels or 2) change the order of these group levels. Changing the levels can be done easily with the function fct_recode() and changing the order can be done easily with the function fct_relevel(). These functions are actually in the forcats package. To use fct_recode(), the first argument is the factor variable vector, and then you specify the old and desired new value for the group levels you wish to change using “new”=“old” where old is the old level and new is the new level. You then separate each level change with commas. We take the AOSI data and change “HR_neg” to “HR_NoASD” and “LR_neg” to “LR_NoASD” below. Note that we only print the first 10 observations. library(forcats) fct_recode(aosi_data$GROUP, &quot;HR_NoASD&quot;=&quot;HR_neg&quot;, &quot;LR_NoASD&quot;=&quot;LR_neg&quot;)[1:10] ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_NoASD HR_NoASD HR_ASD ## [8] HR_ASD HR_NoASD HR_ASD ## Levels: HR_ASD HR_NoASD LR_ASD LR_NoASD To permanently change the GROUP variable in the dataset, we can use the pipe. Since we are changing the variable, we actually have to use mutate() to have R overwrite the old GROUP variable. aosi_data_2 &lt;- aosi_data %&gt;% mutate(GROUP=fct_recode(GROUP, &quot;HR_NoASD&quot;=&quot;HR_neg&quot;, &quot;LR_NoASD&quot;=&quot;LR_neg&quot;)) aosi_data_2$GROUP[1:10] ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_NoASD HR_NoASD HR_ASD ## [8] HR_ASD HR_NoASD HR_ASD ## Levels: HR_ASD HR_NoASD LR_ASD LR_NoASD # Same result Now, we rearrange these levels using fct_relevel. Note that this does NOT change the actual values of the variable in data, simply what order R considers when using these group levels. This order is important when sorting the dataset, plotting, running linear regression and ANOVA analyses, and in other contexts. To use fct_relevel(), the first argument is the factor variable vector, then list the levels in increasing order using strings. For example, using the new group levels from fct_recode, we change the diagnosis group label order to start at Low Risk: Negative and end at High Risk: ASD. fct_relevel(aosi_data_2$GROUP, &quot;LR_NoASD&quot;, &quot;LR_ASD&quot;, &quot;HR_NoASD&quot;, &quot;HR_ASD&quot;)[1:10] ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_NoASD HR_NoASD HR_ASD ## [8] HR_ASD HR_NoASD HR_ASD ## Levels: LR_NoASD LR_ASD HR_NoASD HR_ASD # Notice the Levels: row To change the variable permanently in the dataset, we again need to use mutate() and can use the pipe for cleaner looking code. aosi_data_3 &lt;- aosi_data_2 %&gt;% mutate(GROUP=fct_relevel(GROUP, &quot;LR_NoASD&quot;, &quot;LR_ASD&quot;, &quot;HR_NoASD&quot;, &quot;HR_ASD&quot;)) aosi_data_3$GROUP[1:10] ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_NoASD HR_NoASD HR_ASD ## [8] HR_ASD HR_NoASD HR_ASD ## Levels: LR_NoASD LR_ASD HR_NoASD HR_ASD # Same result "],
["creating-graphs-with-ggplot2.html", "5 Creating Graphs With ggplot2 5.1 Base R vs. ggplot2 5.2 ggplot2 5.3 ggplot Aesthetics 5.4 Additional customization", " 5 Creating Graphs With ggplot2 library(ggplot2) 5.1 Base R vs. ggplot2 By default, R includes systems for constructing various types of plots. Plotting with these built-in functions is referred to as using Base R in these tutorials. While Base R can create many types of graphs that are of interest when doing data analysis, they are often not visually refined. Futhermore, customizing your plot using Base R can be a convoluted process. A package called ggplot2 creates a separate system for plotting which generally results in visually pleasing graphs with extensive and easy-to-use customization features. Thus, in these tutorials plotting with ggplot is covered and the Base R functions for plotting are discussed sparingly. 5.2 ggplot2 5.2.1 Introduction ggplot2 is a package for R and needs to be downloaded and installed once, and then loaded everytime you use R. Like dplyr discussed in the previous chapter, ggplot2 is a set of new functions which expand R’s capabilities along with an operator that allows you to connect these function together to create very concise code. In dplyr, this operator was the pipe %&gt;% and in ggplot this operator is +. The guiding principle behind ggplot2 is that you build your plot from its foundational components (what dataset you are using, a template, etc.) to its more specific components (title, legend, etc.) and connect these components together using +. You can think this as different layers placed on the same space which when placed on top of one another compose your plot. The general structure of your ggplot code is the following: ggplot(…) + aesthetic(…) + … where the function ggplot2() is where you can specify the dataset, variables (x-axis and y-axis), groupings, colors, etc.to be used for all relevant layers in the plot. The argument aesthetic(…) is replaced with the name of the function corresponding to the general way you would like your data to be shown. Some examples include a scatterplot, smoothed line(s) of best fit, box and whisker, etc. where each of these types has their own function you would call in place of aesthetic(…). Within this function’s arguments, you would specify the parameters specific to this aesthetic/layer (dataset, x and y variables, groupings, etc.). Thus, the arguments of ggplot() can be also left blank. Let’s go through some simple examples to illustrate these two concepts. Using the AOSI data, let’s first create a scatterplot of AOSI total score at the 12 month visit by AOSI total score at the 6 month visit. The function which displays the data as a scatterplot is called geom_point(). aosi_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, stringsAsFactors=FALSE, na.strings = &quot;.&quot;) ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_point() ## Warning: Removed 161 rows containing missing values (geom_point). ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18)) ## Warning: Removed 161 rows containing missing values (geom_point). # same result produced; can specify plot parameters in general for all layers (1st example) or for the specific layer of interest. Since only have one layer, both ways create the same plot. Notice the use of aes(x=…, y=…) and not just x=…, y=…. When specifying dataset variables as values for your plotting paramaters, you must surround these arguments in aes() so that ggplot2 knows these names refer to dataset variable names. Note also how specifying the plotting parameter values inside ggplot() and inside geom_point() resulted in the same plot. You can specify plot parameters in general for all layers (inside ggplot()) or for the specific layer of interest (in this case, inside geom_point()). Since you only have one layer, both ways create the same plot. 5.3 ggplot Aesthetics A scatterplot with geom_point() is just one example of the aesthetics available with ggplot. Let’s go through a few other ones that you will likely be using when analyzing IBIS data. You will see that many of the discussed customization options are available for all of these aesthetics, however they have effects which differ across the aesthetics. 5.3.1 Scatterplot We have already introduced the scatterplot, so let’s dive into customization. Suppose I wish to color the points with something other then black; let’s use blue. ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18), color=&quot;blue&quot;)+ geom_point() ## Warning: Removed 161 rows containing missing values (geom_point). # does not work, still black. Why? ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18), color = &quot;blue&quot;) ## Warning: Removed 161 rows containing missing values (geom_point). First, notice that since “blue” does not refer to a variable in the AOSI dataset, this plotting parameter is placed outside of aes(). Also, notice that placing color=“blue” in ggplot() did not work while placing it inside of the aesthetic geom_point() did. This is because the ggplot function only accepts arguments of the form data= and aes(…). Parameter values which do not refer to variables in the dataset can not be placed in ggplot() and must be placed in the specific layer(s) of interest. Suppose instead that we want to color the points based on the value of another value in the dataset such as ADS diagnosis. To do this, we follow the structure discussed above with the following code. ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, color=GROUP))+ geom_point() ## Warning: Removed 161 rows containing missing values (geom_point). ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, color = GROUP)) ## Warning: Removed 161 rows containing missing values (geom_point). Notice how both commands create the same plot. Why? (refer to explanation before with “blue”) Notice also that a legend is automaticaly created. This legend can be heavily customized, which is discussed later in this chapter. You can specify more plotting parameters in this simple example, such as size, shape and many more. We will cover size and shape, but for all aesthetics, you should take a look at the documentation for that aesthetic’s function (ex., geom_point()) and experiment. Size and shape work similarly to color; you can specify fixed values for all points or have the values vary depending on another variable. You also specify any combination of these plotting parameters in your code. # size only ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, size=GROUP))+ geom_point() ## Warning: Using size for a discrete variable is not advised. ## Warning: Removed 161 rows containing missing values (geom_point). # shape only ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, shape = GROUP)) ## Warning: Removed 161 rows containing missing values (geom_point). # Size and color ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, color=GROUP, size=Gender)) ## Warning: Using size for a discrete variable is not advised. ## Warning: Removed 161 rows containing missing values (geom_point). # Shape and color ggplot()+ geom_point(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, color=GROUP, shape=Gender)) ## Warning: Removed 161 rows containing missing values (geom_point). The above examples all have size and shape vary with a variable, but you can also use fixed values like what was done for color. You use numbers to refer to different sizes and shapes; for a comprehensive view on all of the different plotting parameters you can set for each aesthetic and which values corresponding to which visual, see the following ggplot2 documentation page. 5.3.2 Colors in R Following the past example, let’s discuss how to refer to colors in R. Namely, R needs to understand that when you specify “blue” in the color parameter, you are referring to a color and not just a string of letters. Colors in R are actually referenced by special strings of a few letters and numbers which R translates to represent a specific color in its library of colors. By default, R has a set of colors it can display with corresponding codes. For example, the string “#0000FF” is recognized by R as dark blue. This library of colors can be expanded (see package ColorBrewer), but geenrally the default set is enough. In many functions, such as ggplot(), you can specify a color using its actual English name as a string (as was done in the example above with “blue”) since it is assumed that you would only be specifying a specific color and not simply the word “blue”. You may never need to use the actual color codes, but if you notice an error occuring when specifying a color, try to replace its name in the function code with the correspnding color code. The names and codes for specifc colors can be easily found online through a Google search. 5.3.3 Line of best fit It is often of interest to visualize your data using a line of best fit or some other “smoother” in order to better illustrate patterns between specific variables. This can be done using geom_smooth(). It’s arguments are structured just like with geom_point(), including the use of aes(). Note that this structure is the same for all aesthetics, although certain parameters are exclusive to specific aesthetics due to their inherent differences in visualizing the data of interest. Note that different smoothing methods can be specified. The two most common are a linear smoother (line of best fit/regression line) and using LOESS (a non-linear smoother) but there are other then can be specified. Let’s use the AOSI total score example shown before, but plot a smooth line constructed from the data instead of a scatterplot. ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_smooth(method=&quot;lm&quot;) ## Warning: Removed 161 rows containing non-finite values (stat_smooth). ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_smooth(method=&quot;loess&quot;) # loess is default and will automatically be used if method is not specified ## Warning: Removed 161 rows containing non-finite values (stat_smooth). Notice that error bars for the smoother are also provided by default, reflected by the gray shading around the line. These can be turned off as well (add argument se=FALSE to geom_smooth()). While this visual is nice, we often want a scatterplot and line on best fit on the same plot. This can done with ggplot by recalling its layered structure when creating the plot. Imagine the scatterplot as one layer and the smoothed line plot as another layer; we wish to paste them together with the smoothed line on top. This done using the + operator, as shown below. Note that the gray error bars in the smoothed line have a measure of transparency (which can also be adjusted), so the scatterplot points will still be visible. ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_point()+ geom_smooth(method=&quot;lm&quot;) ## Warning: Removed 161 rows containing non-finite values (stat_smooth). ## Warning: Removed 161 rows containing missing values (geom_point). ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_point()+ geom_smooth(method=&quot;loess&quot;) ## Warning: Removed 161 rows containing non-finite values (stat_smooth). ## Warning: Removed 161 rows containing missing values (geom_point). What happens when you switch the order of the layers in the code? This idea of stacking layers and building the plot component-by-component forms the skeleton of gglot’s functionality. We will see this more when we discuss titles, subtitles, etc. The plotting parameters for geom_smooth include color and size from before, as well as method as shown in the above example. A few other handy parameters are linetype and group; linetype allows you to change the aesthetics of the smoothed trend line and group allows you to create a separate trend line for each value of a chosen variable. See below for an example of using group. ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_point(aes(color=Gender))+ geom_smooth(method=&quot;lm&quot;, aes(group=Gender)) ## Warning: Removed 161 rows containing non-finite values (stat_smooth). ## Warning: Removed 161 rows containing missing values (geom_point). # Notice the color of the line of best fit did not change with the groups, why? ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18))+ geom_point(aes(color=Gender))+ geom_smooth(method=&quot;lm&quot;, aes(group=Gender, color=Gender)) ## Warning: Removed 161 rows containing non-finite values (stat_smooth). ## Warning: Removed 161 rows containing missing values (geom_point). # Much more clear! 5.3.4 Box and whisker While the examples above are aesthetics that simply change how the raw data is displayed, now we consider aesthetics that represent the data in a different way by applying some statistical transformation. One example is a box and whisker, or just boxplot, which calculates an estimate of the distribution of the data (median, percentiles, etc.) and then visualizes this distribution. A boxplot can be created using the function geom_boxplot(). Consider the following boxplot example visualizing the distribution of AOSI total score at 12 months for each ASD diagnosis group. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(y=GROUP, x=V12.aosi.total_score_1_18))+ geom_boxplot() ## Warning: Removed 75 rows containing missing values (stat_boxplot). # Does not create the plot of interest; x needs to be grouping variable and y needs to be the # continuous variable of interest. We will a see way later to switch the two axes appropriately. You should experiment with all of the different parameters available for each aesthetic and see how the resulting plot changes. Here, we also color the boxes by ASD diagnosis group to further highlight them in the plot. Notice that color only changes the border and lines of the boxplot while fill changes the white space of the boxplot. What happens if you specify both of these in the same plot? We can also use the color/fill parameter here to have an additional variable used to group the data. Do not use the group plotting parameters with boxplot (see example below). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, color=GROUP))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, color=Gender))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, group=Gender))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=Gender, y=V12.aosi.total_score_1_18, group=Gender))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). # Notice that these last 2 plots are the same; using group=Gender with x=GROUP in the 5th example caused Gender to override x=GROUP when calculating the bxoplots, but due to x=GROUP the x-axis was labeled by GROUP and not gender 5.3.5 Barchart Suppose you want to create a bar chart with frequency counts for a cateogrical variable. Again, the data have to be transformed, this time to a dataset of the frequency counts and/or percentages and then visualized. The aesthetic geom_bar() creates a bar chart. See an example below with ASD diagnosis. ggplot(data=aosi_data, aes(x=GROUP))+ geom_bar() While you may want to visualize frequency counts, you may also want to create a similar plot but using a different statistic. This highlights a useful feature with ggplot; every aesthetic also has a stat argument, along with a default value for this argument. Thus, you can specify the statistical transformation done to your data to be plotted using this stat argument. For geom_point(), the default stat value is “identity” and for geom_bar() the default value is “count”. Thus, you often will not have to specify this argument. These different statistical transformations are omitted in these tutorials, however those interested should read R for Data Science by Hadley Wickham for more information and some examples. The one example that is covered here is plotting frequency percentages instead of counts. This is done by specifying y=..prop.. and a value for the argument group in the geom_bar() function. See the ASD diagnoses by AOSI total score example below. ggplot(data=aosi_data, aes(x=GROUP))+ geom_bar(aes(y=..prop.., group=1)) ggplot(data=aosi_data, aes(x=GROUP))+ geom_bar(aes(y=..prop.., group=GROUP)) When plotting the frequency percentages (or proportions), we are essentially changing the calculated variable used on the y axis from count to percentage. This is done by specifying y=..prop..; R understands ..prop.. to mean “calculate frequency proportion” even though this variable is not in the dataset. We also need to tell R how to calculate the proportion in each x value; i.e., specify the “denominator”. To compare to the whole sample, use group=1. As a silly example for illustration purpose, notice what happens when group=GROUP (recall GROUP is the ASD diagnosis variable) is specified. Why did this occur? Since we have seen many examples of using the various plotting parameters, try the ones discussed as well as the ones detailed in the ggplot2 documentation. How do these behave with barcharts compared to the other aesthetics discussed above? 5.3.6 Other aesthetics There are many other ggplot2 aesthetics that are available for you to choose from. You can find detail on these additional aesthetics at ggplot2’s website, however the examples above are likely what you will be using when doing data analysis. You also now have enough knowledge to go out and learn how to use these other aesthetics yourself. 5.4 Additional customization So far all of our ggplot2 code has been fairly simple, generally of the form ggplot2(…)+ aethestic_1(…)+ aesthetic_2(…) Of course we could keep piling on aesthetics, but we want to add elements to our code that further refine our plot to have as much control as is needed to create the optimal visual representation of the data. To do this, we can add layers to our ggplot code which will adjust all of the elements of the plot we want to change, and leave the others as default by not referencing them. In general your ggplot code will have the form ggplot2(…)+ aethestic_1(…)+ aesthetic_2(…)+ element_1(…)+ element_2(…)+ … where you likely will have one or two aesthetics and many elements. Let’s go through some common elements you will use, motivated by the following basic plot created before. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). 5.4.1 Titles and Labels First, we need a title for this plot. The titles (main title, subtitle, caption, etc.) for your plot can specified using labs(). Since labs() is just an element of your plot, just add on +labs(…) to your code; this format is how all elements are added. Then within labs(), specify the values for the arguments corresponding to the types of titles you would like to modify; main title (title=…), subtitle (subtitle=…), caption (caption=…), legend title (fill=…), etc. See the example below. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender&quot;, fill=&quot;Child&#39;s Gender&quot;, caption=&quot;Created by KMD&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Looking much better, but let’s also clean up these convoluted axis labels. By default, R takes the variable name to be the corresponding axis label; sometimes this works, but often it does not look very clean. The axis labels are actually just additional arguments to the labs() function, namely x= and y=. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). 5.4.2 Colors We can also change the colors used for the fill color, specifically which colors map to which value of gender. Again, R has default values, generally red for a variable of one level, red and blue for a variable of two levels (as seen here), red, blue, and green for a variable of three levels, etc. While the default often works fine, you may want to manually set these colors, specifically if red and green are used together (people who are colorblind will have trouble reading your plot). Note that here we are discussing the case where the variable is categorical, however you can also apply colorings to a continuous variable (see the example below). For categorical colorings, use the element scale_color_manual() (and for a continuous color scale use scale_color_gradient() and the associated variations). # continuous color scale plot ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, color=V12.aosi.total_score_1_18))+ geom_point()+ labs(title=&quot;AOSI Total Score at Month 12, colored by AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing missing values (geom_point). # default color is hard to see # change color scale, see documentation about scale_color_gradient() to see how it works ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, color=V12.aosi.total_score_1_18))+ geom_point()+ labs(title=&quot;AOSI Total Score at Month 12, colored by AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;)+ scale_color_gradient(low=&quot;blue&quot;,high=&quot;red&quot;) ## Warning: Removed 75 rows containing missing values (geom_point). 5.4.3 Sizes and shapes For scatterplots, you can also change the sizes of the points as well as their shapes using the size and shape arguments respectively. These function just like color; please see below for an example with shape to distinguish the ASD diagnoses and increasing the size of the data points to better illustrate the shapes. # continuous color scale plot ggplot(data=aosi_data, aes(x=V06.aosi.total_score_1_18, y=V12.aosi.total_score_1_18, shape=GROUP))+ geom_point(size=3)+ labs(title=&quot;AOSI Total Score at Month 12 and AOSI Total Score at Month 6&quot;, subtitle=&quot;By ASD&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 161 rows containing missing values (geom_point). 5.4.4 Themes Most of the other components of your plot can customized using the theme() function in ggplot2. Themes serve two purposes 1) Change the general look of the plot (can be thought of like changing the visual template of your plot) 2) Fine tune all of the specific visual elements of the plot (including titles, labels, legends, etc.) Some elements that we have already discussed, such as titles and axis labels, can also be customized using theme(). Each argument of the function specifies a different piece of the plot to customize; if the argument is not specified, the default value is used automatically. All plots created using ggplot2 have a default theme that is used. You can see the gray background and vertical and horizontal grid lines included in all of the plots created thus far. This default theme can be changed by calling the function related to the theme of interest. Then, you can customize this chosen theme ever further by using the theme() function. For some examples of this, let’s work with the AOSI Total Score at month 12 boxplot example from before. Recall, without changing the theme, the plot was the following. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). First, let’s change the theme to theme_bw (“bw” stands for “black and white”). We specify this theme by including its associated function call, which is theme_bw(). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_bw()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Notice that we connect the theme_bw() function call to the rest of the code using +. This is because a theme can be thought of as an element of the plot, so we use + to connect it to the rest of the components specified in the plot’s code. Now, we can see the gray background has been replaced with a white background, te grid lines are now light gray, and the plot axes are marked by a black line. There are other themes you can use, as shown below. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_minimal()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_classic()+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Classic Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). For a more complete list of available themes, see https://ggplot2.tidyverse.org/reference/ggtheme.html, though there are many more available though various R packages. After choosing the theme (or leaving the default gray background theme), you can fine tune specific elements of the plot using theme() and specifying its element using a different argument in the function. For a complete list of components you can customize, please see https://ggplot2.tidyverse.org/reference/theme.html. We will customize the legend as an example of using this function. You can see from the above plot that by default, the legend is located to the right of the plot and the legend title’s font is basic text. Let’s alter these components using theme(). We will move the legend to the bottom of the plot and bold and enlarge the legend’s title. This is done using the arguments legend.position and legend.title respectively. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_bw()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(face=&quot;bold&quot;, size=20))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). We can also bold and enlarge the text defining the color labels in the legend using legend.text() and change the sisze of the symbols in the legend using legend.key.size(). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_bw()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(face=&quot;bold&quot;, size=20), legend.text = element_text(face=&quot;bold&quot;, size=20), legend.key.size=unit(15,&quot;mm&quot;))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). To alter the plot title’s text (size, font, bold, itallic, etc.) you would use the argument plot.title in the same fashion. Note that we had to use the element_text() function to specify these font characteristics; this must be done when using the theme() function to adjust proporties of any text in the plot. Notice also that the function units() had to be used when changing the size of the legend symbols. Use ?units to see what units are available to specify; milimeters (mm) was used here. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_bw()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(face=&quot;bold&quot;, size=15), plot.title = element_text(face=&quot;bold&quot;, size=20))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). 5.4.5 Reversing the axes You can also flip the x and y axes of your plot using coord_flip(). Note that even after you use coord_flip(), all the other arguments for your plot wich reference the specific axes (x=, y=, etc.) will still refer to original orientation of the plot (i.e., before the flip). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=Gender))+ geom_boxplot()+ theme_bw()+ theme(legend.position = &quot;bottom&quot;, legend.title = element_text(face=&quot;bold&quot;, size=15), plot.title = element_text(face=&quot;bold&quot;, size=20))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD and gender: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;)+ coord_flip() ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). 5.4.6 Facets The last piece of customization that is covered is faceting. This allows you to group your plot into sub-plots based on one or more grouping variables. This is best explained by example. Consider the above boxplot. Let’s visualize the same information, but using faceting. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_wrap(~Gender)+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). We can also use two grouping variables; let’s use study site as a second grouping to create pairwise comparisons. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site)+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Notice that the x-axis labels overlap; we can fix this using theme() by rotating the x-axis label text using the argument axis.text.x. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site)+ theme(axis.text.x=element_text(angle = 90))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Or we could remove the axis labels entirely using element_blank() and rely on the legend colors. We can similarly remove the axis tick marks as well. The function element_blank() can be used with the theme() function in this fashion to remove specific components of the plot. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site)+ theme(axis.text.x=element_blank(), axis.ticks.x=element_blank())+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Let’s get back to faceting. As seen in the above examples, it can be implemented using facet_wrap() or facet_grid(). Both functions have the same structure when being called; specify the grouping variables x and y using y~x inside the function. First, let’s discuss facet_wrap() which is recommended when you have only one grouping variable. It allows the sub-plots to “wrap” to the next row or column instead of being forced to all be on a single dimension. When using facet_wrap() with grouping variable y, make sure to specify ~y or y.and not y (which generates an error). Using study site as the grouping variable, you can see the sub-plots wrap around to the next row. If you want to specify the layout of the sub-plots yourself, you can use the nrow and ncol to force the number of rows and columns respectively. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_wrap(~Study_Site)+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_wrap(Study_Site~.)+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). To arrange your sub-plots in a strict grid, use facet_grid(). With one variable, this essentially forces the sub-plots into a single dimension and thus is a more limited version of facet_wrap. With two grouping variables, this function arranges the sub-plots in a two dimensional grid as seen below. Note that when using y~x, y specifies the row variable and x the column variable. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site)+ theme(axis.text.x=element_text(angle = 90))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Study_Site~Gender)+ theme(axis.text.x=element_text(angle = 90))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). There are many options you can adjust with your facets, which is done using the theme() function. The arguments starting with “strip” refer to components of the facets, and they operate just like the other arguments of theme() do. For example, we could bold both the row and column facet labels as done below using strip.text. Again, see ?theme for the various options you can adjust. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site)+ theme(axis.text.x=element_text(angle = 90), strip.text=element_text(face=&quot;bold&quot;))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Also, note that the facet labels correspond to the level names of the factor variable you are using. If you try to facet with a numeric variable you will get an error if there are many unique values and if you facet with a character variable, it will be converted into a factor variable. Depending on how well formatted your dataset is, you may need to change these labels and may not want to change the actual variable values. This is done using the labeller argument. In this argument, supply a vector of the following form; c(old value 1=“new value 1”, old value 2=“new value 2”, …) where old value 1 is the first value you want to change and new value 1 is its replacement, and so on for all of the variable values (order does not matter). Note that all values of the variable must be specified, even those which you are not changing. Thus, it might be easier to simply change the levels of the variable itself using techniques discussed previously such as fct_recode(). See the below example. ggplot(data=aosi_data, aes(x=GROUP, y=V12.aosi.total_score_1_18, fill=GROUP))+ geom_boxplot()+ theme_bw()+ facet_grid(Gender~Study_Site, labeller = labeller( Gender=c(`Female`=&quot;F&quot;, `Male`=&quot;M&quot;), Study_Site=c(`PHI`=&quot;PHI&quot;,`SEA`=&quot;SEA&quot;,`STL`=&quot;St. Louis&quot;,`UNC`=&quot;UNC-CH&quot;) ))+ theme(axis.text.x=element_text(angle = 90), strip.text=element_text(face=&quot;bold&quot;))+ labs(title=&quot;AOSI Total Score at Month 12&quot;, subtitle=&quot;By ASD, gender, and site: Minimal Theme&quot;, caption=&quot;Created by KMD&quot;, x=&quot;ASD&quot;, y=&quot;AOSI Total Score (Month 12)&quot;) ## Warning: Removed 75 rows containing non-finite values (stat_boxplot). Finally, the order of the facet groups is set according to the order of the factor levels of the grouping variable (you can check this order using levels() as discussed previously). Recall, fct_relevel() can be used to quickly change the order of a factor variable. Note that using relabel or relevel to change the factor labels requires the variable to be a factor variable; this can easily be done using mutate() with factor() as discussed previously. 5.4.7 Exporting your plot You will likely want to save your final plot as an image file to be used outside of R. This can be done as follows. Run the code corresponding to your plot; you will see it appear in the lower right-hand window, under the “Plots” tab. Select “Export”. From there, you can either save as a PDF file, as an image file (.jpg, .png, etc.), or copy the picture to paste in another program (such as Word). "],
["working-with-tables-in-r.html", "6 Working with Tables in R 6.1 Intro 6.2 Creating Basic Tables: table() and xtabs() 6.3 Tabular Data Analysis", " 6 Working with Tables in R 6.1 Intro Tables are often essential for organzing and summarizing your data, especially with categorical variables. When creating a table in R, it considers your table as a specifc type of object (called “table”) which is very similar to a data frame. Though this may seem strange since datasets are stored as data frames, this means working with tables will be very easy since we have covered data frames in detail over the previous tutorials. In this chapter, we will discuss how to create various types of tables, and how to use various statistical methods to analyze tabular data. Throughout the chapter, the AOSI dataset will be used. 6.2 Creating Basic Tables: table() and xtabs() A contingency table is a tabulation of counts and/or percentages for one or more variables. In R, these tables can be created using table() along with some of its variations. To use table(), simply add in the variables you want to tabulate separated by a comma. Note that table() does not have a data= argument like many other functions do (e.g., ggplot2 functions), so you much reference the variable using dataset$variable. Some examples are shown below. By default, missing values are excluded from the counts; if you want a count for these missing values you must specify the argument useNA=“ifany” or useNA=“always”. The below examples show how to use this function. aosi_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, stringsAsFactors=FALSE, na.strings = &quot;.&quot;) # Table for gender table(aosi_data$Gender) ## ## Female Male ## 235 352 # Table for study site table(aosi_data$Study_Site) ## ## PHI SEA STL UNC ## 149 152 145 141 # Two-way table for gender and study site table(aosi_data$Gender, aosi_data$Study_Site) ## ## PHI SEA STL UNC ## Female 55 67 60 53 ## Male 94 85 85 88 # Notice order matters: 1st variable is row variable, 2nd variable is column variable # Let&#39;s try adding in the useNA argument table(aosi_data$Gender, aosi_data$Study_Site, useNA = &quot;ifany&quot;) ## ## PHI SEA STL UNC ## Female 55 67 60 53 ## Male 94 85 85 88 table(aosi_data$Gender, aosi_data$Study_Site, useNA = &quot;always&quot;) ## ## PHI SEA STL UNC &lt;NA&gt; ## Female 55 67 60 53 0 ## Male 94 85 85 88 0 ## &lt;NA&gt; 0 0 0 0 0 # Let&#39;s save one of these tables to use for later examples table_ex &lt;- table(aosi_data$Gender, aosi_data$Study_Site) Now let’s add row and column labels to the gender by study site table. For a table object, these labels are referred to as “dimnames” (i.e., dimension names) which can be accessed using the dimnames() function. Note that this is similar to the names() function with lists, except that now our table has multiple dimensions, each of which can have its own set of names. For a table, dimnames are stored as a list, with each list entry holding the group labels for the variable corresponding to that dimension. The name for each of these list entries will specify the actual label to be used in the table. By default, these names are blank, hence why the default table has no row and column labels. We can change this by specifying these names, using names() with dimnames(). dimnames(table_ex) ## [[1]] ## [1] &quot;Female&quot; &quot;Male&quot; ## ## [[2]] ## [1] &quot;PHI&quot; &quot;SEA&quot; &quot;STL&quot; &quot;UNC&quot; # we see the group labels. Note that each set of group labels in unnamed (blanks next to [[1]] and [[2]]). This is more clearly see by accessing these names explicitly using names() names(dimnames(table_ex)) ## [1] &quot;&quot; &quot;&quot; # Now, let&#39;s change these names and see how the table changes names(dimnames(table_ex)) &lt;- c(&quot;Gender&quot;, &quot;Site&quot;) names(dimnames(table_ex)) ## [1] &quot;Gender&quot; &quot;Site&quot; table_ex ## Site ## Gender PHI SEA STL UNC ## Female 55 67 60 53 ## Male 94 85 85 88 # Now the row and column labels appear, making the table easier to understand It also common to view these tabulations as percentages. This can be done by using prop.table(), which unlike table() takes in a table object as an argument and not the actual variables of interest. Note that any changes to dimnames that are done to the table object are kept when applying prop.table(). The output from prop.table() is also stored as an object of type table. # 2 Way Proportion Table prop_table_ex &lt;- prop.table(table_ex) prop_table_ex ## Site ## Gender PHI SEA STL UNC ## Female 0.09369676 0.11413969 0.10221465 0.09028961 ## Male 0.16013629 0.14480409 0.14480409 0.14991482 A second way of creating contingency tables is using the xtabs() function, which requires the stats package (which is included in R by default, though still load the package using library()). The function xtabs() creates a object of type xtabs and you will notice that the output of both xtabs() and tabel() is nearly identical. xtabs() has the following advantages: 1) row and column labels are included automatically, set to the variable names and 2) there is a data= argument, which means you just have to reference the variable names. With xtabs(), you do not list out the variables of interest separated by commas. Instead you use formula notation, which is ~variable1+variable2+… where variable1 and variable2 are the names of the variables of interest. You can add more then two variables (hence the …). See below for the two-way gender and site example. library(stats) table_ex_xtabs &lt;- xtabs(~Gender+Study_Site, data=aosi_data) table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC ## Female 55 67 60 53 ## Male 94 85 85 88 To create a table of proportions using xtab(), you first create the table of counts using xtab(), and then use the prop.table() function on this table object. This is exactly what was done when using table(). One useful function when creating tables is proportions is round(). As seen with the previous table of proportions, R will not round decimals by default. The round() function can be used for all types of R objects. The first argument is the object of values you want to round and the second argument is the number of decimal places to round to. prop_table_ex_xtabs &lt;- prop.table(table_ex_xtabs) prop_table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC ## Female 0.09369676 0.11413969 0.10221465 0.09028961 ## Male 0.16013629 0.14480409 0.14480409 0.14991482 prop_table_ex_xtabs &lt;- round(prop_table_ex_xtabs, 2) prop_table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC ## Female 0.09 0.11 0.10 0.09 ## Male 0.16 0.14 0.14 0.15 prop_table_ex &lt;- round(prop_table_ex, 2) prop_table_ex ## Site ## Gender PHI SEA STL UNC ## Female 0.09 0.11 0.10 0.09 ## Male 0.16 0.14 0.14 0.15 Lastly, we discuss how to add margin totals to your table. Whether using table() or xtab(), a simple way to add all margin totals to your table is with the function addmargins() from the stats package. Simply add your table or xtab object as the first argument to the addmargins() function, and a new table will be returned which includes these margin totals. This also works with tables of proportions. table_ex &lt;- addmargins(table_ex) table_ex_xtabs &lt;- addmargins(table_ex_xtabs) prop_table_ex &lt;- addmargins(prop_table_ex) prop_table_ex_xtabs &lt;- addmargins(prop_table_ex_xtabs) table_ex ## Site ## Gender PHI SEA STL UNC Sum ## Female 55 67 60 53 235 ## Male 94 85 85 88 352 ## Sum 149 152 145 141 587 table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC Sum ## Female 55 67 60 53 235 ## Male 94 85 85 88 352 ## Sum 149 152 145 141 587 prop_table_ex ## Site ## Gender PHI SEA STL UNC Sum ## Female 0.09 0.11 0.10 0.09 0.39 ## Male 0.16 0.14 0.14 0.15 0.59 ## Sum 0.25 0.25 0.24 0.24 0.98 prop_table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC Sum ## Female 0.09 0.11 0.10 0.09 0.39 ## Male 0.16 0.14 0.14 0.15 0.59 ## Sum 0.25 0.25 0.24 0.24 0.98 There are many packages which you can install with more advanced tools for creating and customizing contingency tables. We will cover some in the Chapter 9, though table() and xtabs() should suffice for exploratory analyses. 6.3 Tabular Data Analysis In this section, we detail some common statistical methods used to analyze contingency table data as well as how to implement these methods in R. These methods are defined and the statistics behind them are explained and then implementation in R is discussed and shown through examples. 6.3.1 Tests for Independence 6.3.1.1 Defining Independence Suppose we have two categorical variables, denoted \\(X\\) and \\(Y\\). Denote the joint distribution of \\(X\\) and \\(Y\\) by \\(f_{x,y}\\), the distribution of \\(X\\) by \\(f_x\\) and the distribution of \\(Y\\) by \\(f_y\\). Denote the distribution of \\(X\\) conditional on \\(Y\\) by \\(f_{x|y}\\) and the distribution of \\(Y\\) conditional on \\(X\\) by \\(f_{y|x}\\). In statistics, \\(X\\) and \\(Y\\) are independent if \\(f_{x,y}=f_{x}*f_{y}\\) (i.e., if the distribution of \\(X\\) and \\(Y\\) as a pair is equal to the distribution of \\(X\\) times the the distribution of \\(Y\\)). This criteria is the equivalent to \\(f_{x|y}=f_{x}\\) and \\(f_{y|x}=f_{y}\\) (i.e., if the distribution of \\(X\\) in the whole population is the same as the distribution of \\(X\\) in the sub-population defined by specific values of \\(Y\\)). As an example, suppose we were interested in seeing if a person voting in an election (\\(X\\)) is independent of their sex at birth (\\(Y\\)). If these variables were independent, we would expect that the percentage of women in the total population is similar to the percentage of women among the people who vote in the election. This matches with our definition of independence in statistics. 6.3.1.2 Chi-Square Test for Independence To motivate the concept of testing for independence, let’s consider the AOSI dataset. Let’s see if study site and gender are independent. Recall the contingency table for these variables in the data was the following. table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC Sum ## Female 55 67 60 53 235 ## Male 94 85 85 88 352 ## Sum 149 152 145 141 587 prop_table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC Sum ## Female 0.09 0.11 0.10 0.09 0.39 ## Male 0.16 0.14 0.14 0.15 0.59 ## Sum 0.25 0.25 0.24 0.24 0.98 From our definition of independence, it looks like gender and site are independent based on comparing the counts within each gender and site group as well as the population-level counts. Let’s conduct a formal test to see if there is evidence for independence. First, we cover the Chi-Square test. For all of these tests the null hypothesis is that the variables are independent. Under this null hypothesis, we would expect the following contingency table. expected_table &lt;- table_ex_xtabs sex_sums &lt;- expected_table[,5] site_sums &lt;- expected_table[3,] expected_table[1,1] &lt;- 149*(235/587) expected_table[2,1] &lt;- 149*(352/587) expected_table[1,2] &lt;- 152*(235/587) expected_table[2,2] &lt;- 152*(352/587) expected_table[1,3] &lt;- 145*(235/587) expected_table[2,3] &lt;- 145*(352/587) expected_table[1,4] &lt;- 141*(235/587) expected_table[2,4] &lt;- 141*(352/587) expected_table &lt;- round(expected_table,2) Where did these values come from? Take the Philadelphia study site column as an example (labeled PHI). As explained before, under independence, in Philadelphia we would expect the percentage of female participants to be the same as the percentage in the total sample There are 149 participants from Philadelphia, 235 females, and 587 total subjects in the sample. The total sample is about 40% female, so we would expect there to be approximately 0.40*149 or 59.6 females from the Philadelphia site and thus approximately 89.4 males. That is, the expected count is equal to (row total*column total)/sample size. All entries are calculated using this equation. Let’s look at the differences between the counts from the AOSI data and the expected counts. expected_table[-3,-5]-table_ex_xtabs[-3,-5] ## Study_Site ## Gender PHI SEA STL UNC ## Female 4.65 -6.15 -1.95 3.45 ## Male -4.65 6.15 1.95 -3.45 We can see that the differences are small considering the study site margins, so it there no evidence to suggest dependence. However, let’s do this more rigorously using a formal hypothesis test. For any hypothesis test, we create a test statistic and then calculate a p-value from this test statistic. Informally, the p-value measures the probability you would observe a test statistic value as or more extreme then the value observed in the dataset if the null hypothesis is true. For the Chi-Square test, the test statistic is equal to the sum of the squared differences between the observed and expected counts, divided by the expected counts. The distribution of this test statistic is approximately Chi-Square with \\((r-1)\\*(c-1)\\) degrees of freedom, where \\(r\\) is the number of row categories and \\(c\\) is the number of column categories. The approximation becomes more accurate as the large size grows larger and larger (to infinity). Thus, if the sample size is “large enough”, we can accurately approximate the test statistics distribution with this Chi-Square distribution. This is what is referred to by “large sample” or “asymptotic” statistics. Let’s conduct the Chi-Square test on AOSI dataset. This is done by using summary() with the contingency table object (created by table() or xtab()). Note that you cannot have the row and column margins in the table object when conducting this Chi-Square test, as R will consider these marginals are row and column categories. table_ex_xtabs &lt;- xtabs(~Gender+Study_Site, data=aosi_data) summary(table_ex_xtabs) ## Call: xtabs(formula = ~Gender + Study_Site, data = aosi_data) ## Number of cases in table: 587 ## Number of factors: 2 ## Test for independence of all factors: ## Chisq = 2.1011, df = 3, p-value = 0.5517 We see that the p-value is 0.55, which is very large and under a threshold of 0.05 is far from significance. Thus, we do not have evidence to reject the null hypothesis that gender and study site are independent. 6.3.1.3 Fisher’s Exact Test An alternative to the Chi-Square test is Fisher’s Exact Test. This hypothesis test has the same null and alternative hypothesis as the Chi-Square test. However, its test statistic has a known distribution for any finite sample size. Thus, no distributional approximation is required, unlike the Chi-Square test and thus it produces accurate p-values for any sample size. To conduct Fisher’s Exact Test, use the function fisher.test() from the stats package with the table or xtab object. The drawback to Fisher’s Exact Test is that it has a high computation time if the data has a large sample size; in that case, the approximation from the Chi-Square is likely accurate and this testing procedure should be used. Let’s run Fisher’s Exact Test on the gender by site contingency table. fisher.test(table_ex_xtabs) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: table_ex_xtabs ## p-value = 0.553 ## alternative hypothesis: two.sided Due to large sample size, we see that the p-value is very close to the p-value from the Chi-Square test, as expected. "],
["statistical-fundamentals.html", "7 Statistical Fundamentals 7.1 Intro 7.2 Statistical Inference", " 7 Statistical Fundamentals library(readr) library(tidyverse) 7.1 Intro This tutorial details fundamental statistical concepts, illustrated with IBIS data. The main purpose is two-fold 1) to develop a set of notation, definitions, and ideas that comprise the foundamentals behind standard statistical analyses and 2) to introduce some basic statistical methods and illustrate how they are carried out in R using IBIS data. The notation and concepts developed here will be referenced again in later tutorials, so those without formal statistical training would be inclined to read this tutorial. For those with a solid background in statistics, this tutorial should serve as a quick refresher as well as an reference for standard statistical terminology and notation. 7.2 Statistical Inference For variable \\(X\\), its distribution can be thought of as a function which states the probability of \\(X\\) equalling a specific value \\(x\\), for every possible \\(x\\). For example, if \\(X\\) is normal distributed, its distribution can be plotted as shown below. We can see that the probability is highest around \\(x=0\\), and then quickly decreases as you move away from \\(0\\). When given real data, often a histogram is used to visualize the variable’s distribution. For example, the histogram of AOSI total score 12 months in the AOSI cross sectional data is the following: data &lt;- read.csv(&quot;Data/Cross-sec_full.csv&quot;, stringsAsFactors=FALSE, na.strings = c(&quot;.&quot;, &quot;&quot;, &quot; &quot;)) hist(data$V12.aosi.total_score_1_18, xlab = &quot;AOSI Total Score&quot;, main=&quot;Histogram of AOSI Total Score at 12 months&quot;) We can see that it is most likely that AOSI total score will be between 0 and 10, with values higher then 10 unlikely. This type of distribution is called skewed, specifically right skewed as it has a long “tail” to the right and most of the probability to the left. A left skewed distribution is the opposite. We can also consider the distribution of our variable \\(X\\) conditional on value y of another variable Y. Conditional means “only on the population with Y=y”. This is referred to as the conditional distribution of \\(X\\) for \\(Y=y\\). For example, while we have created the histogram of AOSI total score for the entire sample, we may want to visualize the distribution of AOSI total score separately for each of the following diagnosis groups, High Risk: ASD, High Risk: Negative, and Low Risk: Negative. For example, we create the histogram for the High Risk: ASD group below. data_HRASD &lt;- data %&gt;% filter(GROUP==&quot;HR_ASD&quot;) hist(data_HRASD$V12.aosi.total_score_1_18, xlab = &quot;AOSI Total Score&quot;, main=&quot;Conditional Histogram of AOSI Total Score \\nat 12 months for High Risk: ASD group&quot;) # can see variable is considered to be a string by default in the data; need to force it to be numeric to create a histogram Statistical analyses are often designed to estimate certain characteristics of a variable’s (or many variables’) distribution. These characteristics are usually referred to as parameters. Often times, these parameters are the mean, variance, median/quantiles, or probabilities of specific values (for a discrete variable, i.e., probability of positive ASD diagnosis). We do not know the true values of these parameters, so we try to obtain an accurate approximation of them based on random samples (your data) from the distribution/population of interest. These approximations will vary from sample to sample due to the randomness behind the selection of these samples and their finite size. Thus, we also need to account for the variance of these approximations when completing our analyses. This analysis, composed of the estimation of the parameters as well as accounting for the variance of this estimation, is referred to statistical inference. 7.2.1 Parameter Estimation: Mean, Median, tutorial, Quantiles Here, we discuss the estimation of specific parameters that are usually of interest for continuous variables. For categorical variables, see Chapter 6. These parameters are the mean, median, variance, and quantiles. Often, you will see these estimates summarized in “Table 1” in research articles. To see how to create such summaries, see the Chapter 9. Here, we discuss the methodology behind their estimation as well as how to compute these estimates in R. Both the mean and median can intuitively be thought of as measures of the “center” of the distribution. The variance can be thought of a measure of the “spread” of the distribution. To illustrate variance, we plot two normal distributions both with mean 0 but one with a variance of 1 and another a variance of 4. The increase in the likely range of values with a variance of 4 is quite evident, and the probabilities are more spread out. Note that the standard deviation is just the square root of the variance. We estimate the distribution’s mean, median, variance, and quantiles using the usual sample mean, median, variance, and quantiles respectively. These can be calculated in R using the functions mean(), median(), var(), and quantile() respectively. Note: when using quantile(), the minimum and maximum values in the sample are also provided. We illustrate these functions below using AOSI total score at 12 months. Also, note the inclusion of na.rm=TRUE into these functions. By default, when calculating these estimates, if a missing value is encountered, R outputs NA (missing). Thus, we need to tell R to remove these missing values before computing the estimates. mean(data$V12.aosi.total_score_1_18, na.rm=TRUE) ## [1] 4.980469 var(data$V12.aosi.total_score_1_18, na.rm=TRUE) ## [1] 13.08377 median(data$V12.aosi.total_score_1_18, na.rm=TRUE) ## [1] 4 quantile(data$V12.aosi.total_score_1_18, na.rm=TRUE) ## 0% 25% 50% 75% 100% ## 0 2 4 7 22 However, we often want to see these estimates (and others) for many variables in our dataset without having to calculate each one separately. This can be done using summary(). These estimates are often referred to as summary statistics of the data. We compute summary statistics for a number of the variables in the dataset below. data_small &lt;- data %&gt;% select(V12.aosi.total_score_1_18, V06.aosi.total_score_1_18, V12.aosi.Candidate_Age) summary(data_small) ## V12.aosi.total_score_1_18 V06.aosi.total_score_1_18 ## Min. : 0.00 Min. : 1.000 ## 1st Qu.: 2.00 1st Qu.: 7.000 ## Median : 4.00 Median : 9.000 ## Mean : 4.98 Mean : 9.562 ## 3rd Qu.: 7.00 3rd Qu.:12.000 ## Max. :22.00 Max. :28.000 ## NA&#39;s :75 NA&#39;s :105 ## V12.aosi.Candidate_Age ## Min. : 0.00 ## 1st Qu.:12.20 ## Median :12.50 ## Mean :12.59 ## 3rd Qu.:12.90 ## Max. :16.70 ## NA&#39;s :75 There are also a number of packages which expand R’s functionality in computing these summary statistics. One recommended package is the Hmisc package. It includes the function describe() which is an improved version of of summary(); describe() is used below with the same variables. library(Hmisc) describe(data_small) ## data_small ## ## 3 Variables 587 Observations ## --------------------------------------------------------------------------- ## V12.aosi.total_score_1_18 ## n missing distinct Info Mean Gmd .05 .10 ## 512 75 20 0.99 4.98 3.901 1 1 ## .25 .50 .75 .90 .95 ## 2 4 7 10 12 ## ## Value 0 1 2 3 4 5 6 7 8 9 ## Frequency 23 51 66 68 72 51 34 40 27 18 ## Proportion 0.045 0.100 0.129 0.133 0.141 0.100 0.066 0.078 0.053 0.035 ## ## Value 10 11 12 13 14 15 16 17 20 22 ## Frequency 21 10 8 7 6 4 3 1 1 1 ## Proportion 0.041 0.020 0.016 0.014 0.012 0.008 0.006 0.002 0.002 0.002 ## --------------------------------------------------------------------------- ## V06.aosi.total_score_1_18 ## n missing distinct Info Mean Gmd .05 .10 ## 482 105 24 0.994 9.562 4.622 4 5 ## .25 .50 .75 .90 .95 ## 7 9 12 15 17 ## ## lowest : 1 2 3 4 5, highest: 20 21 22 24 28 ## --------------------------------------------------------------------------- ## V12.aosi.Candidate_Age ## n missing distinct Info Mean Gmd .05 .10 ## 512 75 37 0.994 12.59 0.7036 11.9 12.0 ## .25 .50 .75 .90 .95 ## 12.2 12.5 12.9 13.4 13.9 ## ## lowest : 0.0 11.4 11.6 11.7 11.8, highest: 14.9 15.1 15.6 15.9 16.7 ## --------------------------------------------------------------------------- 7.2.2 Accounting for estimation variance and hypothesis testing While these estimates provide approximations to the parameters of interest, they 1) have some degree of error and 2) vary from sample to sample. To illustrate this, 5 simulated datasets, each of size 100 and with a single variable are generated under a mean of 0. The sample mean is calculated for each of these 5 samples. We see that 1) these sample means differ across the samples and 2) none are exactly 0. Thus, we need to account for this variance when providing the approximations. data_sim &lt;- list() for(i in 1:5){ data_sim[[i]] &lt;- rnorm(100, 0, 1) } lapply(data_sim, mean) ## [[1]] ## [1] 0.1530002 ## ## [[2]] ## [1] -0.04917423 ## ## [[3]] ## [1] -0.2105315 ## ## [[4]] ## [1] -0.06945557 ## ## [[5]] ## [1] 0.05877266 7.2.3 Confidence intervals This is generally done by including a confidence interval with your approximation, more specially a \\(x\\)% confidence interval where \\(x\\) is some number between 0 and 100. Intuitively, you can interpret a confidence interval as a “reasonable” range of values for your parameter of interest based on your data. Increasing the percentage of the confidence interval increases its width, and thus increases the chances that the confidence interval from a sample will contain the true parameter. However, this increasing width also decreases the precision of the information you receive from the confidence interval, so there is a trade off. Generally, 95% is used (though this is just convention). As an example, we considering computing a confidence interval for the mean. Consider AOSI total score at 12 months from our dataset. We have already computed an estimate of the mean; let us add a confidence interval to this estimate. This can be done using the t.test() function (which has other uses which we cover later). t.test(data$V12.aosi.total_score_1_18) ## ## One Sample t-test ## ## data: data$V12.aosi.total_score_1_18 ## t = 31.156, df = 511, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 4.666411 5.294526 ## sample estimates: ## mean of x ## 4.980469 Let us focus on the bottom of the output for now; we cover the top later. We see that we approximate that the mean AOSI total score at 12 months in the population is 4.98 and a reasonable set of values for this mean in the population is 4.67 to 5.29 using a 95% confidence interval. 7.2.4 Hypothesis testing In order to come to a conclusion about a paramater from the sample information, hypothesis testing is generally used. The setup is the following: suppose we are interested in inferring if the mean for a variable is some value, say \\(\\mu_0\\). From the data we have, we would like to make an educated guess about the claim that the mean is \\(\\mu_0\\). The main principle in scientific research is that one makes a claim or hypothesis, conducts an experiment, and sees if the results of the experiment contradict the claim. This is the same reasoning that governs hypothesis testing. To start, we make a claim which we would “like” to refute (for example, a treatment has no effect). This claim is referred to as the null hypothesis. For example, consider the null hypothesis that the mean is \\(\\mu_0\\). We then define the contradiction to this claim, that the mean is NOT \\(\\mu_0\\), as the alternative hypothesis. Finally, we take the information from our data, and see if there is enough evidence to reject the null hypothesis in favor of the alternative. If there is not enough evidence, then we fail to reject the null. That is, we never accept the null or prove that the null is true. Recall that the null is our baseline claim that we are aiming to refute; if we accepted the null, then we would be “proving true” that which we initially claimed to hold. How do we determine if we have “enough evidence” to reject the null? The following process is generally used. First, we reduce our data down to a single value that is related to the hypothesis being tested. This value is called a test statistic. Then, we see how much the observed test statistic from our data deviates from the range of values we would expect if the null hypothesis were true. If this deivation is large enough, we decide to reject the null. Otherwise, we fail to reject. This is best explained by example. Consider our null hypothesis of the mean is \\(\\mu_0\\). Intuitively, we would calculate the sample mean and use it as our test statistic, and compare its value to \\(\\mu_0\\). If the sample mean is “close” to \\(\\mu_0\\), we would fail to reject, otherwise we would reject. For example, recall that for AOSI total score at 12 months, the sample mean ws 4.98. For our null hypothesis was that the population mean was \\(\\mu_0\\)=5, we probably would not have enough evidence reject the null. However, we need a formal way of measuring the deviation from the null, preferrably one that is independent of the variable’s units. The p-value serves as this measure. Formally, the p-value measures the probability of seeing a test statistic value that is as extreme or more extreme from the null hypothesis then the test statistic value you actually observed. Informally, you can think of a p-value as a unit agnostic measure of how much your data deivates from the null hypothesis. We calculate the p-value using the observed value of the test statistic as well as the test statistic’s distribution (also referred to as its sampling distribution); this distribution is how we are able to compute the probablity which the p-value reflects. To conduct the hypothesis test corresponding to the null hypothesis that the population mean is 0, a one sample t-test is often used. We will cover this in more detail later; here we use it to illustrate the above concepts. The t.test() function can also be used to conduct this hypothesis test. We consider testing if the mean AOSI total score at 12 months is 0. Note that we can choose a values different from 0 to use in our null hypothesis by adding the argument mu=x, where x is the value of interest. t.test(data$V12.aosi.total_score_1_18) ## ## One Sample t-test ## ## data: data$V12.aosi.total_score_1_18 ## t = 31.156, df = 511, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 4.666411 5.294526 ## sample estimates: ## mean of x ## 4.980469 The top part of the output contains the hypothesis test results. We see that the test statistic (denoted by t) is 31.156 and the corresponding p-value is essentially 0. Note that there is a one to one correspondence between the test statistic value and the p-value. Thus, reporting the p-value only is sufficient. We see that the p-value is very small; a small p-value implies that the data deviates from the null. However, to make a reject or fail to reject decision, we have to set a threshold for this p-value. Generally, people select 0.05 as a threshold; below 0.05 implies the evidence is strong enough to reject and above 0.05 implies the evidence is not strong enough. However this is just a rule of thumb and one could justify a different threshold. Furthermore, it may not be wise to make an all or nothing decision in terms of judging the scientific value of the results. Using a p-value threshold implies that a p-value of 0.051 has the same interpretation as a p-vale of 0.51 and that a p-value of 0.049 is “significant” evidence while a p-value of 0.051 is not. This dramatically limits the scientific information that the results are providing. Instead, it is better to interpret p-values more broadly and in terms of their strength of evidence. For example, a p-value of 0.06 or 0.025 may indicate “strong evidence” in favor of the alternative hypothesis and 0.10 or 0.12 may indicate “moderate evidence”. Simply reducing the interpretation to “significant” or “not significant” is not optimal and should be avoided. 7.2.5 Hypothesis Testing with Means Here we cover methods to conduct hypothesis testing with population means. You will likely recognize many of these methods; we cover t-tests and ANOVA. We illustrate these using AOSI total score at 12 months and diagnosis group (High Risk: ASD, High Risk: Negative, Low Risk: Negative) 7.2.5.1 T-tests: One Sample and Two Sample First, consider testing the null hypothesis that the mean AOSI total score at 12 months is 0 in the population. As discussed before, this comparison of a single mean to a value is generally done with a one sample t-test. Recall that anytime we do a hypothesis test, we require the following: 1) Test statistic 2) Distribution of the test statistic 3) P-value from this distribution and the test statistic’s observed value In this case, the test statistic is denoted by \\(T\\) since it has a \\(T\\) distribution. This test statistic \\(T\\) is equal to the sample mean minus the null value (often 0) and then divided by the spread of the sample mean (called standard error). \\(T\\) distributions are differentiated by the parameter called degrees of freedom (similar to how normal distributions are differentiated by their mean and variance). Using the value of \\(T\\) with the value of the degrees of freedom and the \\(T\\) distribution, components 1) and 2), we can calculate a p-value. All three of these components are provided in the output from t.test(), along with the alternative hypothesis of the test (that the mean is not 0). t.test(data$V12.aosi.total_score_1_18) ## ## One Sample t-test ## ## data: data$V12.aosi.total_score_1_18 ## t = 31.156, df = 511, p-value &lt; 2.2e-16 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 4.666411 5.294526 ## sample estimates: ## mean of x ## 4.980469 We see that under a p-value of about 0, we have strong evidence in favor of the mean AOSI total score at 12 months not equalling 0. Note that you can also see that 0 is quite outside the 95% confidence interval, indicating that 0 is not a reasonable value for the population mean based on this dataset (which points to the alternative hypothesis). It turns out that p-values and confidence intervals are generally linked together and will agree in this fashion by their design. Note that for this process to be valid, some assumptions are made. They are a) AOSI total score at 12 months is normally distributed and b) all observations are independent. If one or more of these is violated, the results from your one-sample t-test will be invalid. Now suppose we wanted to compare mean AOSI total score at 12 months between two diagnosis groups, such as High Risk: ASD and High Risk: Negative. Specifically, the null hypothesis will be that the means are the same between the two groups. The common corresponding test is called a two sample t-test. Again, we use the function t.test(). Here, we use y~x notation where x is the grouping variable to do the two sample test. You will again obtain a test statistic t, degrees of freedom, and a p-value. As in the one sample case, this test statistic also has a \\(T\\) distribution. data_HR &lt;- data %&gt;% filter(GROUP==&quot;HR_ASD&quot;|GROUP==&quot;HR_neg&quot;) t.test(V12.aosi.total_score_1_18~GROUP,data=data_HR) ## ## Welch Two Sample t-test ## ## data: V12.aosi.total_score_1_18 by GROUP ## t = 4.9977, df = 113.84, p-value = 2.119e-06 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 1.506931 3.486090 ## sample estimates: ## mean in group HR_ASD mean in group HR_neg ## 7.337500 4.840989 You can see R provides the following, starting from the top. 1) Test statistic t, degrees of freedom, and p-value 2) alternative hypothesis 3) 95% confidence interval for the difference in the means 4) sample means for each group We see that, as expected, based on a p-value about 0, we have strong evidence that the mean AOSI total scores at 12 months are different between High Risk: ASD and High Risk: Negative. This can also be seen in 0 being far away from the confidence interval and the sample means of 7.34 and 4.84 being quite different from one another. 7.2.5.2 ANOVA Suppose we want to compare more then two groups’ means. For example, suppose we want to compare the mean AOSI total score at 12 months for the High Risk: ASD, High Risk: Negative, and Low Risk: Negative groups. This is done using an ANOVA F test. The null hypothesis is that the mean AOSI total score at 12 months is same in all three groups. The alternative is that at least one group’s mean differs from the rest. We can conduct this test in R using aov(). Again, it uses formula notation (y~x). We conduct this test below. Note that many components are calculated by R for this test. To obtain the main values of interest, we must save the output as an object (which stores all of the components) and then use the function summary() on this object. aov_object &lt;- aov(V12.aosi.total_score_1_18~GROUP, data=data) summary(aov_object) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GROUP 3 599 199.81 16.68 2.39e-10 *** ## Residuals 508 6086 11.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 75 observations deleted due to missingness We see degrees of freedom are provided (3 and 508 in this example), the value of the test statistic (F value=16.68) and the corresponding p-value (2.39e-10 or essentially 0). For an ANOVA F test, the test statistic \\(F\\) has an \\(F\\) distribution. An \\(F\\) distribution is defined by two degrees of freedom parameters. Using this F statistic value and F distribution, the p-value can be calculated. While an ANOVA F test will infer if the groups differ overall, we would like to see which groups differ. This generally done by doing all of the pairwise comparisons. For our example, that would entail hypothesis tests for 1) HR: ASD vs HR: Negative, 2) HR: ASD vs LR: Negative, 3) HR: Negative vs LR: Negative, etc. for 6 total tests. Recall when comparing two groups’ means, we can use a two sample t-test. For each pairwise test, we use this two sample t-test. Then, we interpret the test results for each pair separately. This is frequently referred to as post-hoc analysis. It turns out that because we are conducting multiple hypothesis tests at once, we need to “correct” each p-value from these pairwise comparisons to account for this multiple comparison. Why this is the case is beyond the scope of these tutorials. Generally, the correction done is called Tukey’s Method, though other corrections such as Bonferroni or Holm’s Method can also be used. These corrected p-values can then be “validly” interpreted like usual p-values. To compute this post-hoc analysis in R using Tukey’s Method, use the function TukeyASD() with the object from aov(). TukeyHSD(aov_object) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = V12.aosi.total_score_1_18 ~ GROUP, data = data) ## ## $GROUP ## diff lwr upr p adj ## HR_neg-HR_ASD -2.4965106 -3.626229 -1.36679183 0.0000001 ## LR_ASD-HR_ASD -3.6708333 -8.917563 1.57589610 0.2728961 ## LR_neg-HR_ASD -3.3511986 -4.592245 -2.11015217 0.0000000 ## LR_ASD-HR_neg -1.1743227 -6.352589 4.00394397 0.9367305 ## LR_neg-HR_neg -0.8546880 -1.763793 0.05441738 0.0740168 ## LR_neg-LR_ASD 0.3196347 -4.884054 5.52332358 0.9985859 The output contains the following: 1) Difference in sample means between the groups (diff) 2) 95% confidence intervals for each mean difference (lwr and upr) 3) Corrected p-values for each pairwise comparison (p adj) This entire ANOVA analysis provides valid results under the following assumptions 1) All groups’ values are normally distributed with the same variance 2) All observations are independent (both within the groups and between the groups) The first assumption can be checked visually using a histogram and by estimating the variances with confidence intervals, or conducting hypothesis tests for equal variances (not covered here). The second assumption is verified based on the study from which the data originated. "],
["regression-analysis-methods.html", "8 Regression Analysis Methods 8.1 Intro 8.2 Linear Regression 8.3 Logistic regression 8.4 Mixed Models", " 8 Regression Analysis Methods full_data &lt;- read.csv(&quot;Data/Cross-sec_full.csv&quot;, stringsAsFactors=FALSE, na.strings = c(&quot;.&quot;, &quot;&quot;, &quot; &quot;)) library(ggplot2) library(tidyverse) 8.1 Intro Regression methods form the backbone of much of the analyses in research. In general, these methods are used to estimate associations between variables, espeically when one or more of these are variables are continuous. In this section, we cover linear regression, logistic regression, and mixed models. For most people, understanding these methods will be sufficient for the analyses required for their research. Since these methods are so often used, they are covered in great detail, so each reader should take into account their own knowledge of statistics when deciding which parts of this tutorial to read or skip over. Both the statistical motivation behind these methods and their implementation in R to data are covered, and understanding both concepts is essential when carrying out your own analyses. Furthermore, terminology is introduced to help standardize the language used with these concepts across the IBIS network. The full cross sectional dataset is used for illustration of these concepts throughout this tutorial. 8.2 Linear Regression 8.2.1 Motivation Suppose we are interested in the association between Mullen composite score (recorded as a standard score in the dataset) and Autism severity as measured by the AOSI, in children at month 12. Since both are continuous variables, a simple measure of this association is their correlation. Let’s see the scatterplot for these two variables. ggplot(data=full_data, aes(x=V12.mullen.composite_standard_score, y=V12.aosi.total_score_1_18))+ geom_point()+ labs(x=&quot;Mullen Composite Standard Score&quot;, y=&quot;AOSI Total Score&quot;, title=&quot;Scatterplot of Mullen Composite Standard Score \\nand AOSI Total Score at Month 12&quot;) # Note the use of \\n when specifying the title. If this is not included, R will force the whole string on one line, cutting off part of the title due to lack of space. Adding \\n forces a line break at this spot. This \\n string specifies a new line for any general string (so it can be used with the axis labels, legend title, captions or subtitles, etc.) At best, there appears to be a weak, negative relationship between these variables. Let’s estimate the correlation. This can be done in R using cor(). Note that cor() does not have a data= argument, so you must specify the variables of interest in the dataset$variable form discussed in previous tutorials. Also, by default, if any of the variables have missing values, an NA is returned by cor(). To remove any subjects missing one or more of the variables from the calculation, specify use=“pairwise.complete” as an argument in the function. We can also 1) conduct a formal hypothesis test for zero correlation and 2) calculate a 95% confidence interval for the correlation using the function cor.test(). Note that the default correlation measure is Pearson, though this can be changed in both functions using the measure= argument. Also, when conducting the hypothesis test or calculating the confidence interval, we must always take into account any distributional assumptions that are used in these calculations (as discussed in Chapters 6 and 7). For the hypothesis test and confidence interval calculations done here to be valid, one of the following needs to hold: 1) the two variables are jointly normally distributed or 2) sample size is “large enough” for the approximate distribution of the test statistic to be accurate. This approximate distribution is a T distribution with \\(n-2\\) degrees of freedom, where \\(n\\) denotes the sample sze. Hence, the test statistic value in the R output is denoted by t. cor(x=full_data$V12.mullen.composite_standard_score, y=full_data$V12.aosi.total_score_1_18) ## [1] NA cor(x=full_data$V12.mullen.composite_standard_score, y=full_data$V12.aosi.total_score_1_18, use=&quot;pairwise.complete&quot;) ## [1] -0.2964536 cor.test(x=full_data$V12.mullen.composite_standard_score, y=full_data$V12.aosi.total_score_1_18) ## ## Pearson&#39;s product-moment correlation ## ## data: full_data$V12.mullen.composite_standard_score and full_data$V12.aosi.total_score_1_18 ## t = -7.01, df = 510, p-value = 7.59e-12 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3735142 -0.2153293 ## sample estimates: ## cor ## -0.2964536 We see that there evidence from data of a moderate, negative correlation between these variables. However, using this pairwise correlation has its limitations as 1) it is a more “general” measure and 2) it does not account for other variables which may impact this association. To better illustrate the first point, consider the following scatterplots (created using simulated data). # simulate data: this is explained in tutorial 9 for those who are interested set.seed(123) E &lt;- rnorm(500) X &lt;- rnorm(500, mean=5) Y1 &lt;- 2*X+E Y2 &lt;- 6*X+E # will use default R plotting functions, which we did not cover plot(X, Y1, main=&quot;Scatterplot Example: Dataset 1 in black, Dataset 2 in blue&quot;, xlim = c(0, 8), ylim=c(0, 50), ylab=&quot;Y&quot;) points(X, Y2, col=&quot;blue&quot;) text(x=7, y=9,labels=paste(&quot;Cor=&quot;,round(cor(X, Y1),2),sep=&quot;&quot;)) text(x=7, y=35,labels=paste(&quot;Cor=&quot;,round(cor(X, Y2),2),sep=&quot;&quot;), col=&quot;blue&quot;) cor(X, Y1) ## [1] 0.8969942 cor(X, Y2) ## [1] 0.9872073 We can see that both examples have similar correlation, though the dataset denoted in blue has a much larger increase in variable \\(Y\\) as variable \\(X\\) increases. It would be useful to more explictly measure these “slopes” and adjust this measure based on other variables of interest which may impact the association. 8.2.2 Methodology 8.2.3 Overview Suppose variables \\(X\\) and \\(Y\\) are of interest, where \\(Y\\) is continuous (\\(X\\) can be discrete or continuous). With linear regression, we specify the following model between these variables: \\(Y=\\beta_0+\\beta_1X+\\epsilon\\). Where \\(\\epsilon\\) is commonly thought of as the “error term” for the model. We assume \\(\\epsilon\\) has mean zero and variance denoted by \\(\\sigma^2\\). For variables \\(X\\) and \\(Y\\), denote the mean of \\(Y\\) conditional on \\(X\\) by \\(\\mbox{E}(Y|X)\\). With linear regression, since the mean of \\(\\epsilon\\) is 0, \\(\\mbox{E}(Y|X)=\\beta_0+\\beta_1X\\) which is a linear function of \\(X\\). When we apply this model to data, we assume that each observation’s \\(\\epsilon\\) is independent, which implies that each observation’s outcome \\(Y\\) is independent as \\(Y\\) is a function of \\(\\epsilon\\). As a result, for data were there are multiple observations per subject, the linear regression model would not hold true and applying it would not produce valid results. We see that \\(\\beta_0\\) (the intercept) represents the mean of \\(Y\\) when \\(X\\) is 0 and \\(\\beta_1\\) (the slope) represents the change in mean of \\(Y\\) when \\(X\\) increases by 1. Based on this model, we want to estimate the parameters \\(\\beta_0\\) and \\(\\beta_1\\) with the slope serving as a measure of the association between \\(X\\) and \\(Y\\). We denote the estimates with \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\) respectively. This is generally done using least squares estimation. Since the estimates of the intercept and slope will vary across samples from a given populuation, we would like to account for this randomness. This generally done by conducting a hypothesis test and/or computing a confidence interval. In linear regression, the standard methods for computing a p-value from a hypothesis test or a confidence interval require one of two assumptions to hold: 1) \\(\\epsilon\\) is normally distributed or 2) the sample size is large enough for your intercept and slope estimates (\\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) respectively) to be approximately normally distributed. If one of these hold, the test statistic has (approximately if a large sample approximation is used) a T distribution with \\(n-p\\) degrees of freedom where \\(p\\) denotes the number of parameters in the model (\\(p=2\\) in this case). A p-value can then be calculated using this distribution, along with a confidence interval. As a result, when doing a linear regression analysis, we have a set of assumptions that we need to check using the data. Mean of \\(Y\\) given \\(X\\) is a linear function of \\(X\\) Across values of \\(X\\), the error terms have equal variances (called homoskedasticity) Across values of \\(X\\), the error terms are normally distributed or the sample size is large enough for the large sample approximation to be accurate All error terms/observations in the data are independent Assumptions 1), 2), and 3) can be evaluated from the data while 4) is usually evaluated based on the study design from which the data originated. Now, suppose we were also interested in adjusting these associations for another variable \\(Z\\) (can be continuous or categorical). We can specify the following linear regression model: \\(Y=\\beta_0+\\beta_1X+\\beta_2Z+\\epsilon\\). \\(\\mbox{E}(Y|X,Z)=\\beta_0+\\beta_1X+\\beta_2Z\\) where \\(\\epsilon\\) has the same properties as before. Statisticians often refer to \\(Y\\) as the outcome, \\(X\\) and \\(Z\\) as predictors or covariates, and \\(\\beta_0, \\beta_1\\), and \\(\\beta_2\\) as regression parameters. Notice that now we are doing inference on the mean of \\(Y\\) conditional on \\(X\\) and \\(Z\\). Futhermore, you see that \\(\\beta_1\\) represents the change in the mean of \\(Y\\) when \\(X\\) increases by 1 unit and \\(Z\\) is held fixed: \\(\\mbox{E}(Y|X+1,Z)-\\mbox{E}(Y|X,Z)\\) \\(=\\beta_0+\\beta_1(X+1)+\\beta_2Z-(\\beta_0+\\beta_1X+\\beta_2Z)\\) \\(=\\beta_0+\\beta_1X+\\beta_1+\\beta_2Z-(\\beta_0+\\beta_1X+\\beta_2Z)\\) \\(=\\beta_1\\) Thus, we are conducting inference about the relationship between \\(Y\\) and \\(X\\) controlling for \\(Z\\). You can control for any number of predictors; the interpretation of the regression parameters is the same as the one or two predictor scenarios discussed above. 8.2.4 Example 1: Continuous predictors Let’s conduct a linear regression analysis of the relationship between AOSI total score and Mullen composite at 12 months, controlling for age at the month 12 visit. The corresponding regression model is: \\(AOSI=\\beta_0+\\beta_1Mullen+\\beta_2Age+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. Anytime you are doing a regression analysis (including logistic regression and mixed models as discussed later), it is important to be able to explicitly write down the model you are using as we have done above. To apply the model to data and obtain estimates and confidence intervals (referred to as fitting the model) in R, you use the lm() function. The main syntax is the following: lm(y~x+z+…,data=…) where y is the name of the outcome variable, x and z are the predictors and data= is the argument where you specify the dataset to be used. The ~ symbol acts as the equal sign in the equation from the regression model. # Fit the model to data to obtain estimates lm(V12.aosi.total_score_1_18~V12.mullen.composite_standard_score +V12.mullen.Candidate_Age, data=full_data) ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ V12.mullen.composite_standard_score + ## V12.mullen.Candidate_Age, data = full_data) ## ## Coefficients: ## (Intercept) V12.mullen.composite_standard_score ## 12.34763 -0.07873 ## V12.mullen.Candidate_Age ## 0.04692 The estimated regression coefficients are returned. However, fitting the model produces many more results then just these estimates. To access the full results, first we need to save the lm() function output as a function. Then, we can access confidence intervals for the regression parameters, obtain p-values for hypothesis testing, obtain sum of squares measures and residuals, etc. This is done next in our AOSI example. # Fit the model to data and save results as object aosi_ex_1_fit &lt;- lm(V12.aosi.total_score_1_18~V12.mullen.composite_standard_score +V12.mullen.Candidate_Age, data=full_data) summary(aosi_ex_1_fit) ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ V12.mullen.composite_standard_score + ## V12.mullen.Candidate_Age, data = full_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.8582 -2.4493 -0.5611 1.9431 15.9197 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.34763 3.21106 3.845 0.000136 ## V12.mullen.composite_standard_score -0.07873 0.01125 -6.999 8.17e-12 ## V12.mullen.Candidate_Age 0.04692 0.23631 0.199 0.842687 ## ## (Intercept) *** ## V12.mullen.composite_standard_score *** ## V12.mullen.Candidate_Age ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.461 on 509 degrees of freedom ## (75 observations deleted due to missingness) ## Multiple R-squared: 0.08796, Adjusted R-squared: 0.08437 ## F-statistic: 24.54 on 2 and 509 DF, p-value: 6.67e-11 After saving the fit output, using the summary() with this object produces the standard results from a regression analysis. We now explain each element of this table. Residuals: These are the error terms for each subject, which are the “gaps” between their actual outcome value and their predicted outcome value based on the fitted line (defined by \\(\\hat{\\beta}_0+\\hat{\\beta_1}X+\\hat{\\beta_2}Z\\)). The minimum, 1st quartile, median, 3rd quartile, and maximum of these from the data are reported Coefficients: Here the regression parameter estimates and their standard errors are reported, as well as the test statistic and p-value corresponding to the hypothesis test that this parameter is 0. Recall each test statistic has a T distribution, hence the labels “t value” and “Pr(&gt;|t|)”. Residual standard error: Recall that the error term \\(\\epsilon\\) is assumed to have variance \\(\\sigma^2\\) for all observations in the data. The residual standard error is an estimate for \\(\\sigma\\). R-squared: The usual R-squared (“multiple”) and adjusted R-squared. Essentially, adjusted R-squared takes the usual R-squared and reduces it as the number of regression parameters in the model increases. Both reflect the same idea. F-statistic: We have discussed conducting a hypothesis test for one single regression parameter equalling 0. To test if more then one regression parameters equal to 0, the test statistic has an F distribution with \\(p-1\\) numerator degrees of freedom and \\(n-p-1\\) denominator degrees of freedom. The test statistic and p-value reported here specifically correspond to the test for all non-intercept regression parameters equalling 0. This is generally not a hypothesis test of interest, though statistical software often reports it. You will generally only be interested in the Coefficients section and maybe the R-squared results, though it is useful to have an idea of what the other terms in output represent. We see that there is evidence from the regression results of a negative association between AOSI total score and Mullen composite score at 12 months, controlling for Age, with a one unit increase in Mullen composite score corresponding to a 0.08 decrease in the mean AOSI total score at 12 months. From the very small p-value, there is strong evidence of a weak association. Furthermore, the two R-squared values are very small, around 0.08. This indicates that most of a subject’s AOSI score is coming from the error term (\\(\\epsilon\\)) compared to the mean term (\\(\\beta_0+\\beta_1Mullen+\\beta_2Age\\)). 8.2.5 Example 2: Categorical predictors Now, suppose one of the two predictors is a categorical variable. Note that if you use a predictor that measures counts (i.e., it not exactly continuous but can take any integer value or any integer value greater than or equal to 0), the interpretation of the regression model is essentially the same as the continuous case. Suppose we consider the following regression model: \\(AOSI=\\beta_0+\\beta_1Mullen+\\beta_2Diagnosis+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. where variable \\(Diagnosis\\) indicates clinical diagnosis of one of the following types: low risk:ASD (LR:ASD), low risk: negative (LR:Neg), high risk: ASD (HR:ASD) and high risk: negative (HR:Neg). To interpret the model, we require the \\(\\beta_0\\) to make sense and \\(Diagnosis\\) to be numeric. To accomplish 1), we need \\(Diagnosis=0\\) to make sense. One way of achieving these two goals is to have \\(Diagnosis\\) take values 0 (LR:ASD), 1 (LR:Neg), 2 (HR:ASD), or 3 (HR:Neg). Then, \\(\\beta_0\\) denotes the mean AOSI total score at 12 months when a subject has a 0 Mullen composite score and is diagnosed as LR:ASD. The issue is that \\(\\beta_1\\) measures the change in mean AOSI as \\(Diagnosis\\) changes by 1 and Mullen composite score is constant. If \\(Diagnosis\\) is coded in the above way, this implies that this change in mean AOSI is the same when going LR:ASD to LR:Neg as it is going from LR:Neg to HR:ASD. While this structure may be reasonable if the variable has an inherent ordering of least to most “severe”, in general with categorical variables it is a structure we would like avoid. Instead, we use “indicator variable” or dummy variable coding. With dummy variable coding, you represent the categorical variable with a set of binary (0 or 1) variables which indicate specific categories. For example, the previous regression model is replaced with the following: \\(AOSI=\\beta_0+\\beta_1Mullen+\\beta_2HRneg+\\beta_3LRasd+\\beta_4LRneg+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. where \\(HRneg=1\\) if \\(Diagnosis=HR:Negative\\) and \\(HRneg=0\\) otherwise, with \\(LRasd\\) and \\(LRneg\\) defined similarly. Thus, \\(Diagnosis=HR:ASD\\) is denoted by \\(HRneg=0, LRasd=0\\), and \\(LRneg=0\\). HR:ASD is referred to as the reference group. As a result, for each subject, only one of these variables will equal 1 and the others will equal 0. This structure allows group-specific comparisons; see the ANOVA and ANCOVA section of this chapter for more detail. We can see that \\(\\beta_0\\) denotes the mean AOSI total score for a subject with Mullen composite score of 0 and a diagnosis of HR:ASD, and \\(\\beta_1\\) denotes the change in the mean AOSI total score when Mullen composite score increases by 1 unit and diagnosis is help constant, at 12 months. For the diagnosis groups, \\(\\beta_2\\) denotes the change in the mean AOSI total score when diagnosis changes from HR:ASD to HR:Neg and Mullen composite score is held constant, at 12 months, with \\(\\beta_3\\) and \\(\\beta_4\\) having the same interpretations for LR:ASD and LR:Neg respectively. Now, we fit this model to the data. When using the lm() function to fit a model with categorical predictors, if the predictor is coded as a character variable or a factor variable, R will automatically do dummy variable coding as shown below. To view the reference level used, view the xlevels object from the saved model fit object; the first value is what R is using as the reference level. This can also be seen in the fit results table; the reference level will not have regression coefficient results as detailed previously. # Fit the model to data and save results as object aosi_ex_2_fit &lt;- lm(V12.aosi.total_score_1_18~V12.mullen.composite_standard_score +GROUP, data=full_data) summary(aosi_ex_2_fit) ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ V12.mullen.composite_standard_score + ## GROUP, data = full_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2913 -2.3751 -0.6146 2.1165 16.3076 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.85779 1.13129 11.366 &lt; 2e-16 ## V12.mullen.composite_standard_score -0.05988 0.01157 -5.176 3.27e-07 ## GROUPHR_neg -1.95569 0.44015 -4.443 1.09e-05 ## GROUPLR_ASD -3.16309 1.98818 -1.591 0.112 ## GROUPLR_neg -2.53927 0.49520 -5.128 4.18e-07 ## ## (Intercept) *** ## V12.mullen.composite_standard_score *** ## GROUPHR_neg *** ## GROUPLR_ASD ## GROUPLR_neg *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.377 on 507 degrees of freedom ## (75 observations deleted due to missingness) ## Multiple R-squared: 0.1354, Adjusted R-squared: 0.1285 ## F-statistic: 19.84 on 4 and 507 DF, p-value: 3.439e-15 aosi_ex_2_fit$xlevels ## $GROUP ## [1] &quot;HR_ASD&quot; &quot;HR_neg&quot; &quot;LR_ASD&quot; &quot;LR_neg&quot; To set your own reference level for the variable, first you must make sure it is a factor variable. You can make it one using factor() (see tutorials “Objects and Functions” and “dplyr”). Once the variable is a factor, you can set the reference level using relevel(), with the syntax relevel(variable, ref=…) where the reference level value is place in the ref= argument. In the example below, we set the reference level to LR:ASD per the model above. # Set reference level full_data_2 &lt;- full_data %&gt;% mutate(GROUP=factor(GROUP) %&gt;% relevel(ref=&quot;LR_ASD&quot;)) full_data_2$GROUP # is a factor, LR_ASD is the first level ## [1] HR_ASD HR_ASD HR_ASD HR_ASD HR_neg HR_neg HR_ASD HR_ASD HR_neg HR_ASD ## [11] HR_neg HR_neg HR_neg HR_neg LR_neg HR_neg HR_ASD HR_neg HR_neg HR_neg ## [21] HR_neg HR_neg HR_neg HR_ASD HR_ASD HR_neg HR_neg LR_neg LR_neg HR_neg ## [31] HR_neg LR_neg LR_neg HR_neg LR_neg HR_neg LR_neg HR_neg HR_ASD HR_neg ## [41] LR_neg LR_neg LR_neg HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg LR_neg ## [51] LR_neg LR_neg LR_neg LR_neg HR_ASD HR_neg LR_neg LR_neg HR_neg HR_neg ## [61] HR_neg LR_neg HR_ASD HR_ASD HR_ASD LR_neg HR_neg HR_neg HR_neg HR_ASD ## [71] HR_ASD LR_neg LR_neg HR_ASD LR_neg LR_neg HR_neg HR_neg HR_neg HR_neg ## [81] HR_neg HR_neg HR_neg HR_neg LR_neg HR_neg HR_ASD HR_neg HR_ASD HR_neg ## [91] HR_neg LR_neg HR_neg HR_neg HR_neg LR_neg HR_neg HR_neg HR_neg HR_neg ## [101] HR_neg HR_neg LR_neg HR_neg HR_neg HR_neg LR_neg HR_neg HR_neg HR_neg ## [111] HR_neg HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg ## [121] HR_ASD HR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg LR_neg HR_neg HR_neg ## [131] HR_neg HR_ASD LR_neg LR_neg LR_neg LR_neg LR_neg LR_neg LR_neg LR_neg ## [141] LR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg LR_neg HR_ASD HR_ASD HR_ASD ## [151] LR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg ## [161] HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg HR_ASD HR_neg LR_neg LR_neg ## [171] LR_neg HR_neg HR_neg HR_neg HR_ASD LR_neg HR_neg HR_neg LR_neg HR_neg ## [181] LR_neg HR_ASD LR_neg LR_neg HR_neg HR_ASD HR_neg HR_neg LR_neg LR_neg ## [191] HR_neg HR_neg LR_neg LR_neg LR_neg LR_neg HR_neg LR_neg HR_neg HR_neg ## [201] HR_neg HR_neg HR_neg LR_ASD HR_neg LR_neg LR_neg HR_neg HR_neg HR_neg ## [211] HR_neg HR_neg HR_neg HR_neg LR_neg LR_neg LR_neg LR_neg LR_neg HR_neg ## [221] HR_neg HR_neg HR_ASD HR_neg LR_neg HR_ASD LR_neg LR_neg LR_neg LR_neg ## [231] HR_neg HR_ASD HR_neg LR_neg HR_neg HR_neg HR_ASD LR_ASD LR_neg HR_ASD ## [241] LR_neg LR_neg LR_neg LR_neg HR_ASD HR_neg HR_ASD HR_neg HR_neg HR_neg ## [251] HR_neg LR_neg HR_neg HR_neg HR_ASD HR_neg LR_neg HR_neg LR_neg HR_neg ## [261] HR_neg HR_ASD HR_ASD HR_ASD HR_ASD HR_ASD HR_neg HR_ASD LR_neg LR_neg ## [271] LR_neg LR_neg HR_neg LR_neg LR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg ## [281] HR_neg HR_neg HR_ASD LR_neg LR_neg HR_neg HR_ASD HR_neg HR_ASD HR_ASD ## [291] HR_neg HR_neg HR_neg LR_neg HR_ASD HR_ASD HR_neg LR_neg HR_neg HR_neg ## [301] HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg ## [311] HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_ASD HR_ASD HR_neg ## [321] HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg HR_neg ## [331] HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg LR_neg HR_neg HR_neg ## [341] HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg HR_ASD LR_neg LR_neg HR_neg ## [351] HR_neg HR_ASD LR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_ASD HR_neg ## [361] HR_neg HR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg ## [371] HR_neg LR_neg LR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg ## [381] HR_ASD HR_ASD LR_neg HR_neg HR_ASD HR_neg HR_ASD HR_neg HR_neg HR_neg ## [391] LR_neg HR_neg LR_neg LR_neg LR_neg LR_neg LR_neg HR_neg LR_neg LR_neg ## [401] LR_neg LR_neg HR_neg LR_neg LR_neg LR_neg LR_neg HR_ASD LR_neg LR_neg ## [411] HR_neg HR_ASD HR_neg HR_ASD HR_neg HR_neg LR_neg HR_ASD HR_ASD HR_neg ## [421] HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg HR_neg LR_neg ## [431] LR_neg LR_neg LR_neg LR_neg LR_neg LR_neg HR_neg LR_neg HR_neg HR_neg ## [441] HR_neg HR_ASD HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_ASD HR_ASD ## [451] HR_neg HR_neg HR_ASD LR_neg LR_neg HR_ASD LR_neg HR_neg HR_ASD LR_neg ## [461] HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg HR_ASD LR_neg LR_neg HR_neg ## [471] HR_neg LR_neg HR_neg LR_neg HR_neg HR_neg LR_neg LR_neg LR_neg LR_neg ## [481] HR_neg LR_neg HR_neg LR_neg HR_neg HR_neg HR_neg LR_ASD HR_neg HR_ASD ## [491] HR_ASD HR_neg HR_neg HR_ASD HR_neg HR_ASD HR_neg HR_neg HR_neg HR_ASD ## [501] HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg HR_ASD HR_neg LR_neg LR_neg ## [511] LR_neg LR_neg HR_ASD HR_ASD LR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg ## [521] HR_neg LR_neg LR_neg LR_neg HR_neg HR_neg HR_ASD LR_neg HR_neg LR_neg ## [531] HR_neg LR_neg HR_neg LR_neg LR_neg LR_neg HR_neg HR_neg LR_neg HR_neg ## [541] LR_neg HR_ASD LR_neg HR_neg LR_neg HR_neg LR_neg HR_ASD HR_ASD LR_neg ## [551] HR_neg HR_neg HR_neg LR_neg LR_neg HR_neg HR_neg HR_ASD HR_neg HR_neg ## [561] HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg HR_neg LR_neg LR_neg HR_neg ## [571] LR_neg LR_neg HR_neg LR_neg LR_neg HR_neg LR_neg HR_ASD HR_neg HR_neg ## [581] LR_neg HR_neg LR_neg HR_neg LR_neg LR_neg LR_neg ## Levels: LR_ASD HR_ASD HR_neg LR_neg # Fit the model to data and save results as object aosi_ex_2_2_fit &lt;- lm(V12.aosi.total_score_1_18~V12.mullen.composite_standard_score +GROUP, data=full_data_2) summary(aosi_ex_2_2_fit) ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ V12.mullen.composite_standard_score + ## GROUP, data = full_data_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.2913 -2.3751 -0.6146 2.1165 16.3076 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.69470 2.27086 4.269 2.34e-05 ## V12.mullen.composite_standard_score -0.05988 0.01157 -5.176 3.27e-07 ## GROUPHR_ASD 3.16309 1.98818 1.591 0.112 ## GROUPHR_neg 1.20740 1.95985 0.616 0.538 ## GROUPLR_neg 0.62383 1.97034 0.317 0.752 ## ## (Intercept) *** ## V12.mullen.composite_standard_score *** ## GROUPHR_ASD ## GROUPHR_neg ## GROUPLR_neg ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.377 on 507 degrees of freedom ## (75 observations deleted due to missingness) ## Multiple R-squared: 0.1354, Adjusted R-squared: 0.1285 ## F-statistic: 19.84 on 4 and 507 DF, p-value: 3.439e-15 aosi_ex_2_2_fit$xlevels # LR_ASD is the first level as desired ## $GROUP ## [1] &quot;LR_ASD&quot; &quot;HR_ASD&quot; &quot;HR_neg&quot; &quot;LR_neg&quot; Recall when fitting the model using HR:ASD as the reference group, two of the diagnosis group coefficients were strongly significant from 0. However, with LR:ASD as the reference group, none were significant from 0 based on the coefficient estimate p-values. We will discuss later how to compute all pairwise group comparisons so that we can calculate both of these sets of results regardless of what the reference group is. When you included a categorical predictor in the model, it is common to test if all of the coefficients pertaining to the predictor’s various levels are equal to 0. This can thought of as analogous to an F test for an overall group difference when conducting an ANOVA, with the individual regression coefficient t tests akin to the pairwise “post-hoc” t tests with an ANOVA. In terms of the regression model, this F test considers the null hypothesis of \\(\\beta_2=\\beta_3=\\beta_4=0\\). Recall from Example 1, an F test is used in regression when testing if more then one regression coefficient equals 0. We can conduct this test using the aov() function with the model fit object returned by lm(), and saving this output as an object. Then, use summary() with this object to view the hypothesis test results, as shown in the example below. Note that as with lm(), the object returned by aov() includes many other components, though summary() will print the results we heed for this example. These additional components are accessed using $ as the object returned is a list. Notice that both return the same hypothesis test results; same test statistic, degrees of freedom, and p-value. # Using 1st model with HR:ASD as reference level f_test_fit_1 &lt;- aov(aosi_ex_2_fit) summary(f_test_fit_1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## V12.mullen.composite_standard_score 1 588 587.6 51.533 2.52e-12 ## GROUP 3 317 105.8 9.278 5.55e-06 ## Residuals 507 5781 11.4 ## ## V12.mullen.composite_standard_score *** ## GROUP *** ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 75 observations deleted due to missingness # Using 2nd model with LR:ASD as reference level f_test_fit_2 &lt;- aov(aosi_ex_2_2_fit) summary(f_test_fit_2) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## V12.mullen.composite_standard_score 1 588 587.6 51.533 2.52e-12 ## GROUP 3 317 105.8 9.278 5.55e-06 ## Residuals 507 5781 11.4 ## ## V12.mullen.composite_standard_score *** ## GROUP *** ## Residuals ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 75 observations deleted due to missingness # Some other components stored in f_test_fit_2 f_test_fit_2 # provides Sum of Squares table ## Call: ## aov(formula = aosi_ex_2_2_fit) ## ## Terms: ## V12.mullen.composite_standard_score GROUP Residuals ## Sum of Squares 587.580 317.369 5780.855 ## Deg. of Freedom 1 3 507 ## ## Residual standard error: 3.376697 ## Estimated effects may be unbalanced ## 75 observations deleted due to missingness Often, we are also interested in calculating and comparing least-squared means. Recall our linear regression model in this example is the following: \\(AOSI=\\beta_0+\\beta_1Mullen+\\beta_2LRneg+\\beta_3HRasd+\\beta_4HRneg+\\epsilon\\) Due to the use of dummy variable coding, the means for AOSI total score in the different diagnosis groups are the following: for LR: ASD Positive, \\(\\beta_0+\\beta_1Mullen\\) for LR: ASD Negative, \\(\\beta_0+\\beta_1Mullen+\\beta_2\\) for HR: ASD Positive, \\(\\beta_0+\\beta_1Mullen+\\beta_3\\) for HR: ASD Negative, \\(\\beta_0+\\beta_1Mullen+\\beta_4\\) We see that we have means within ASD diagnosis groups that are also based on their values of the other predictors in the model, in this case just Mullen composite score. To calculate the least-squared means for each diagnosis group, we simply plug-in specific values for Mullen composite score. Usually the mean or median value in the sample is used. In this sense, we are reporting the mean AOSI total scores for each ASD diagnosis group, controlled for the other predictors in the model. More specifically, we are reporting the mean AOSI total scores for each ASD diagnosis group, at the mean/median values of the other predictors. To calculate these in R, you require the lsmeans package. Then, you use the lsmeans() function with the object from lm() as an argument, along with ~x where x is the name of the categorical variable in the model. To see what values for the other predictors for the least-squared means calculations (in this example, Mullen composite score) are being used, specify the function ref.grid() with the object from lm() as an argument. When ref.grid() is called here, we can see that Mullen composite score = 101.1 is used (the mean Mullen score, see code below). Thus the estimated least-squared mean for LR:ASD Positive is \\(\\hat{\\beta}_0+\\hat{\\beta}_1*101.1\\), with the other estimated least-squared means defined similarly. You have additional options when calculating these means; see the documentation for the lsmeans package for more details. library(lsmeans) ref.grid(aosi_ex_2_2_fit) ## &#39;emmGrid&#39; object with variables: ## V12.mullen.composite_standard_score = 101.1 ## GROUP = LR_ASD, HR_ASD, HR_neg, LR_neg lsmeans(aosi_ex_2_2_fit, ~GROUP) ## GROUP lsmean SE df lower.CL upper.CL ## LR_ASD 3.64 1.950 507 -0.189 7.47 ## HR_ASD 6.80 0.391 507 6.035 7.57 ## HR_neg 4.85 0.201 507 4.454 5.24 ## LR_neg 4.26 0.285 507 3.706 4.82 ## ## Confidence level used: 0.95 mean(full_data_2$V12.mullen.composite_standard_score, na.rm=T) ## [1] 101.1749 # Equals 101. R is using mean Mullen composite score in calculations 8.2.6 Diagnostics Recall that the main assumptions when fitting a regression model to our data are the following: Mean of \\(Y\\) given \\(X\\) is a linear function of \\(X\\) Across values of \\(X\\), the error terms have equal variances (called homoskedasticity) Across values of \\(X\\), the error terms are normally distributed or the sample size is large enough for the large sample approximation to be accurate All error terms in data are independent which need to verified in our data. A sub-optimal method to verify 1) is to view a scatterplot of \\(Y\\) by \\(X\\), with some linear smoother such as LOESS (see Chapter 5). We are looking for evidence of a non-linear relationship using this linear smoother. However, this is sub-optimal because if other predictors as included in the model such as \\(Z\\), simply looking at \\(Y\\) by \\(X\\) does not provide information about the the relationship between \\(Y\\) and \\(X\\) and \\(Z\\) jointly. An example is provided below with the AOSI total score by Mullen composite score example. Verifying 4) is done based on the study design from which the data originated from. ggplot(data=full_data, aes(x=V12.mullen.composite_standard_score, y=V12.aosi.total_score_1_18))+ geom_point()+ geom_smooth(method=&quot;loess&quot;)+ labs(x=&quot;Mullen Composite Standard Score&quot;, y=&quot;AOSI Total Score&quot;, title=&quot;Scatterplot of Mullen Composite Standard Score \\nand AOSI Total Score at Month 12&quot;) To verify 2), we need to look at the residuals for the various subjects and visualize their variance. Consider Example 1 from before, where the model was: \\(AOSI=\\beta_0+\\beta_1Mullen+\\beta_2Age+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. Let \\(\\hat{\\beta_0}, \\hat{\\beta_1}, \\mbox{ and } \\hat{\\beta_2}\\) denote the estimated regression parameters, and \\(\\widehat{AOSI}\\) denote a predicted value of AOSI. One way of predicting AOSI from our model is the following: \\(\\widehat{AOSI}=\\hat{\\beta_0}+\\hat{\\beta_1}Mullen+\\hat{\\beta_2}Age\\) which we denote as the fitted value for AOSI. The difference between the subject’s predicted value from the model and their observed value from the data is denoted by: \\(\\hat{\\epsilon}=AOSI-\\widehat{AOSI}\\) and represents the “error” of our model, often referred to as the residual. To verify 2), we need a scatterplot of each subject’s residual by their fitted value, and observed their variance in the plot. To access the residuals, we can use the object created by lm(); one of the components stored is a vector of the residuals. Similarly, the fitted values can also be extracted from this object. In the example below, a dataset containing these residuals and fitted values is created using the function data.frame(). When given a set of set of vectors, this function will create a data frame with these vectors as columns/variables. Then, ggplot2 can be used to plot the residuals by the fitted values. fit &lt;- lm(V12.aosi.total_score_1_18~V12.mullen.composite_standard_score +V12.mullen.Candidate_Age, data=full_data) fit_data &lt;- data.frame(fit$residuals, fit$fitted.values) ggplot(data=fit_data, aes(y=fit.residuals, x=fit.fitted.values))+ geom_point()+ labs(x=&quot;Fitted Value&quot;, y=&quot;Residual&quot;, title=&quot;Scatterplot of residual by fitted value for AOSI regression model.&quot;) To verify the assumptions of the model, we are looking for the spread of the data to be constant across the x-axis; essentially, we want the variance of the residuals to be constant across all fitted values. More explicitly, we would like a plot similar to Case 1 in the image below; for Case 2, the residual variance is higher in the middle range of fitted values. To verify 3), we need to visualize the distribution of the residuals and compare it to a normal distribution. This is commonly done by creating a QQ plot of the residuals. Ideally, the data points should lie directly on the 45 degree line provided in the plot; deviation from this line indicates some deviation from the normal distribution. Often times, the lower and upper tails of the data will deviate from the line. qqnorm(y=fit_data$fit.residuals) qqline(y=fit_data$fit.residuals, datax = FALSE) In the AOSI total score by Mullen composite score example, the middle section of the data points fall on or very close to the line while there is noticable deviation at the upper and lower tails. The size of this deviation may be cause for concern, however due to large sample size (about 500 subjects), a large sample approximation to the normal distribution can be used. As a result, the deviation at the tails does not invalidate the regression model used. 8.2.7 ANOVA and ANCOVA One of the most common statistical analyses done is ANOVA. We have already discussed this analysis method in Chapter 7. Here, we show how ANOVA is equivalent to a specific case of linear regression. We also define and discuss ANCOVA. Note that ANCOVA is defined differently across certain scientific disciplines; the definition used here is one that is common in statistics. The main message of this section is that ANOVA and ANCOVA are simply special cases of a linear regression model. For ANOVA, we have observations from two or more groups. For example suppose we are interested in comparing AOSI total score at 12 months across diagnosis group at month 24. Suppose the diagnosis group categories are High Risk: ASD Negative, High Risk: ASD Positive, Low Risk: ASD Positive and Low Risk: ASD Negative. We want to compare the mean Mullen composite scores at 24 months across these groups. Consider the following linear regression model: \\(AOSI=\\beta_0+\\beta_1LRneg+\\beta_2HRasd+\\beta_3HRneg+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. where \\(LRneg, HRasd,\\) and \\(HRneg\\) are the dummy variables defined in Example 2 above. From this model, we can write down the mean AOSI scores for each group by setting the dummy variable to 1 for this group, and the others to 0(recall Low Risk: ASD Positive is the reference group from this model). \\(\\mbox{E}(AOSI|\\mbox{LR:ASD Positive})=\\beta_0\\) (all dummy variables =0) \\(\\mbox{E}(AOSI|\\mbox{LR:ASD Negative})=\\beta_0+\\beta_1\\) \\(\\mbox{E}(AOSI|\\mbox{HR:ASD Positive})=\\beta_0+\\beta_2\\) \\(\\mbox{E}(AOSI|\\mbox{HR:ASD Negative})=\\beta_0+\\beta_3\\) We can see that if we want to compare the means between LR: ASD Negative and LR:ASD Positive, we compare \\(\\beta_0+\\beta_1-(\\beta_0)=\\beta_1\\) to 0. If \\(\\beta_1=0\\), then the two groups’ means are equal and otherwise they are different. Similarly, to compare HR: ASD Positive and LR:ASD Positive, we compare \\(\\beta_2\\) to 0 and to compare HR: ASD Negative and LR:ASD Positive, we compare \\(\\beta_3\\) to 0. In fact, every pairwise group comparison results in comparing one of \\(\\beta_1, \\beta_2\\), and \\(\\beta_3\\) to 0. Further, we can see from the four means above that if all groups have the same mean, \\(\\beta_1=\\beta_2=\\beta_3=0\\). This is exactly the same framework as ANOVA; from this linear regression model, we can compare all of the groups’ means, as well as do all pairwise comparisons. Recall that we are assuming each subject’s residuals are independent and that each residual follows a normal distribution with the same variance. These assumptions are the same as the assumptions in ANOVA: independent observations, normally distributed, and equal variance across the groups. Finally, recall that in ANOVA, the overall group test for equal means uses a F distribution and that the pairwise tests use a T distribution. This is the same as in the above regression model. Since testing \\(\\beta_1=\\beta_2=\\beta_3=0\\) is testing multiple regression parameters, we have learned that the test uses a F distribution. As discussed before, it turns out that testing if a single regression coefficients equals 0 uses a T distribution. We can see the equality between ANOVA and the above regression model by running both analyses in R. When running a linear regression, for testing the null hypothesis of \\(\\beta_1=\\beta_2=\\beta_3=0\\), we see a test statistic value of 16.68 which has an F distribution with 3 and 599 degrees of freedom. When running ANOVA, we see the corresponding test statistic is exactly the same. # Regression lm_V_ANOVA_fit &lt;- lm(V12.aosi.total_score_1_18~GROUP, data=full_data_2) f_test_fit_lm_V_ANOVA &lt;- aov(lm_V_ANOVA_fit) summary(f_test_fit_lm_V_ANOVA) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GROUP 3 599 199.81 16.68 2.39e-10 *** ## Residuals 508 6086 11.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 75 observations deleted due to missingness # ANOVA ANOVA_fit &lt;- aov(V12.aosi.total_score_1_18~GROUP, data=full_data_2) summary(ANOVA_fit) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## GROUP 3 599 199.81 16.68 2.39e-10 *** ## Residuals 508 6086 11.98 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 75 observations deleted due to missingness To end the linear regression section, we discuss ANCOVA. With an ANCOVA analysis, you have an outcome variable \\(Y\\), categorical predictor \\(X\\), and continuous predictor \\(Z\\). The ANCOVA model is the following: \\(Y=\\beta_0+\\beta_1X+\\beta_2Z+\\beta_3X*Z+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. where \\(X*Z\\) is called an interactiom term. Let us unpack this model. Suppose \\(X\\) takes one of the two categories 0 or 1. For those with \\(X=0\\), their model is: \\(Y=\\beta_0+\\beta_2Z+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. Since \\(Z\\) is continuous, we can see that the subjects in this category have an intercept of \\(\\beta_0\\) and a slope of \\(\\beta_2\\). For those with \\(X=1\\), their model is: \\(Y=\\beta_0+\\beta_1+\\beta_2Z+\\beta_3Z+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. We can see that their intercept is \\(\\beta_0+\\beta_1\\) and their slope is \\(\\beta_2+\\beta_3\\). Thus with ANCOVA, are allowing a different linear relationship between \\(Y\\) and \\(Z\\) in our model, based on values of \\(X\\). As an example, consider modeling an outcome of AOSI total score at 12 months with ASD diagnosis at 24 months (positive or negative) and Mullen composite score at 12 months with an ANCOVA model. The corresponding linear regression model is \\(AOSI=\\beta_0+\\beta_1ASDpos+\\beta_2Mullen+\\beta_3ASDpos*Mullen+\\epsilon\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. We can see that the trend (or “slope”) between AOSI total score and Mullen composite score at 12 months is \\(\\beta_2\\) for ASD negative children and \\(\\beta_2+\\beta_3\\) for ASD positive children. Let’s fit this model to the data. # Regression lm_ANCOVA_fit &lt;- lm(V12.aosi.total_score_1_18~SSM_ASD_v24+V12.mullen.composite_standard_score+V12.mullen.composite_standard_score*SSM_ASD_v24, data=full_data_2) summary(lm_ANCOVA_fit) ## ## Call: ## lm(formula = V12.aosi.total_score_1_18 ~ SSM_ASD_v24 + V12.mullen.composite_standard_score + ## V12.mullen.composite_standard_score * SSM_ASD_v24, data = full_data_2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.0012 -2.3956 -0.6443 1.9384 16.6981 ## ## Coefficients: ## Estimate Std. Error ## (Intercept) 9.45201 1.31543 ## SSM_ASD_v24YES_ASD 9.65831 2.73602 ## V12.mullen.composite_standard_score -0.04770 0.01270 ## SSM_ASD_v24YES_ASD:V12.mullen.composite_standard_score -0.08101 0.02860 ## t value Pr(&gt;|t|) ## (Intercept) 7.185 2.4e-12 ## SSM_ASD_v24YES_ASD 3.530 0.000453 ## V12.mullen.composite_standard_score -3.755 0.000193 ## SSM_ASD_v24YES_ASD:V12.mullen.composite_standard_score -2.832 0.004805 ## ## (Intercept) *** ## SSM_ASD_v24YES_ASD *** ## V12.mullen.composite_standard_score *** ## SSM_ASD_v24YES_ASD:V12.mullen.composite_standard_score ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.365 on 508 degrees of freedom ## (75 observations deleted due to missingness) ## Multiple R-squared: 0.1399, Adjusted R-squared: 0.1348 ## F-statistic: 27.54 on 3 and 508 DF, p-value: &lt; 2.2e-16 We see that all of the regression parameters are highly significant from 0 based on the p-values and the fitted model is: \\(AOSI=9.45+9.66ASDpos-0.05Mullen-0.08ASDpos*Mullen+\\epsilon\\) This implies that ASD positive children have a slightly more negative association between AOSI and Mullen then ASD negative children (due to the -0.08 interaction term estimate) and that ASD positive children have a much higher AOSI baseline (baseline meaning Mullen composite score equalling 0). We can better visualize these findings by creating a scatterplot of AOSI total score and Mullen composite score at month 12, with a separate line of “best fit” for each ASD diagnosis. ggplot(mapping=aes(x=V12.mullen.composite_standard_score, y=V12.aosi.total_score_1_18, group=SSM_ASD_v24, color=SSM_ASD_v24), data=full_data_2)+ geom_point()+ geom_smooth(method =&quot;lm&quot;)+ labs(x=&quot;Mullen composite score at 12 months&quot;, y=&quot;AOSI total score at 12 months&quot;, title=&quot;ANCOVA example: AOSI by Mullen by ASD Diagnosis at 24 months&quot;, color=&quot;ASD Diagnosis&quot;) # legend is created by color= argument, so use color= argument in labs() to change lend title 8.3 Logistic regression While linear regression can be used with a continuous outcome, it may be of interest to analyze the assocication between a binary outcome (only two possible outcomes) and a set of covariates. In this case, logistic regression can be used. Logistic regression is very similar to linear regression, with the main difference being that the probability of a specific outcome is being modeled instead of the outcome’s mean value. 8.3.1 Methodology Suppose that outcome variable \\(Y\\) is binary, taking either 0 or 1 as a value, with \\(X\\) and \\(Z\\) being covariates (continuous or categorical). Suppose we model the relationship between \\(Y\\) and the covariates using a linear regression model: \\(Y=\\beta_0+\\beta_1X+\\beta_2Z+\\epsilon\\) \\(\\mbox{E}(Y|X,Z)=\\beta_0+\\beta_1X+\\beta_2Z\\) \\(\\mbox{E}(\\epsilon)=0\\), \\(Var(\\epsilon)=\\sigma^2\\), and all \\(\\epsilon\\) are independent. This creates a few complications. First, since the mean of a binary variable is the probability the variable equals 1, this model implies \\(\\mbox{Pr}(Y=1|X,Z)=\\beta_0+\\beta_1X+\\beta_2Z\\). However, a probability must be between 0 and 1. Thus, we need to add limits to this model so that for every value of \\(X\\) and \\(Z\\), \\(\\beta_0+\\beta_1X+\\beta_2Z\\) is between 0 and 1. We would like to avoid these complications. Furthermore, we need \\(Y\\) to be equal to 0 or 1. However, \\(\\epsilon\\) is continuous. Thus, we need to add limits to this model to ensure \\(\\beta_0+\\beta_1X+\\beta_2Z+\\epsilon\\) is always either 0 or 1 for every possible value of \\(X, Z\\), and \\(\\epsilon\\). Again, we would like to avoid these complications in our model. Instead, with logistic regression we model the following: Denote \\(\\mbox{Pr}(Y=1|X,Z)\\) by \\(p_{x,z}\\). Assume \\(\\mbox{logit}(p_{x,z})=\\beta_0+\\beta_1X+\\beta_2Z\\) where \\(\\mbox{logit}(x)=\\mbox{log}[x/(1-x)]\\) where log() indicates the natural log Notice that we are not specifying a model for \\(Y\\) itself as a function of predictors \\(X\\) and \\(Z\\), nor are we specifying a model for the mean of \\(Y\\). Instead, we are specifying a model for the logit of the probability \\(Y\\) is 1 given predictors \\(X\\) and \\(Z\\). Thus, we interpret the regression parameters \\((\\beta_0, \\beta_1, \\ldots)\\) with respect to the logit. How can we interpret this logit term? We define the odds for \\(Y\\) by \\(\\Pr(Y=1)/\\Pr(Y=0)\\) \\(=\\Pr(Y=1)/[1-\\Pr(Y=1)]\\) which is between 0 and infinity, with higher/lower odds reflecting a higher/lower probability of \\(Y\\) equalling 1. Furthermore, log(\\(x\\)) is between negative infinity and positive infinity for any value of \\(x&gt;0\\) and is an increasing function of x (as \\(x\\) increases/decreases, log(\\(x\\)) increases/decreases; see plot below). curve(log(x), from=0.001, to=100, xlab=&quot;x&quot;, ylab=&quot;log(x)&quot;) As a result, we are actually modelling the log of the odds of \\(Y\\) given \\(X\\) and \\(Z\\). Specifically, \\(\\beta_0\\) denotes the log odds of \\(Y=1\\) when \\(X\\) and \\(Z\\) are 0, \\(\\beta_1\\) denotes the change in the log odds of \\(Y=1\\) when \\(X\\) increases by 1 unit and \\(Z\\) is constant, and \\(\\beta_2\\) denotes the change in the log odds of \\(Y=1\\) when \\(Z\\) increases by 1 unit and \\(X\\) is constant. This has a useful interpretation as explained above and it is a continuous value that can be any number. Thus, this model is used for regression with binary outcomes. The assumptions that we make when using this model are the following 1) All observations are independent 2) The linear form is correct (i.e., the log odds is a linear function of \\(X\\) and \\(Z\\)) As in linear regression, the validity of 1) is based on the study design from which the data originated from. The validity of 2) is hard to check in data due to the outcome being discrete (so a scatterplot would not be very informative). 8.3.2 Example 1: Continuous Covariates For examples of logistic regression, we consider outcome variable \\(ASD\\) which is 1 if a child has been clinically diagnosed with ASD at 24 months and 0 otherwise. Suppose we want to analyze the association between ASD status at 24 months and AOSI total score at 12 months, controlling for Mullen composite score at 12 months. Essentially, this would be measuring the association between ASD status and AOSI score beyond AOSI’s association with ASD through its association with Mullen composite score. The logistic regression model is: \\(\\mbox{logit}[\\Pr(\\mbox{ASD at 24 months}|AOSI_{12}, Mullen_{12})]=\\beta_0+\\beta_1AOSI_{12}+\\beta_2Mullen_{12}\\) To fit the logistic regression model to the data, the glm() function is used. It operates exactly like lm(), however you also need to specify family=binomial as an argument. This tells R you want to fit a logistic regression model. Note that your outcome variable has to either 1) be coded as 0 or 1, or 2) be a factor variable with two levels when using glm(). Often times, the variable will be a character variable which will result in an error in R. This can be fixed by using factor() with glm(); this will not change the actual variable in the dataset (it will still be a character). However when fitting the logistic regression model, it will be considered a factor variable as desired. After saving the output returned by glm() as an object, you can use the summary() function with this object to return the main results. # Fit logistic regression model logistic_fit_1 &lt;- glm(SSM_ASD_v24~V12.aosi.total_score_1_18+ V12.mullen.composite_standard_score, data=full_data, family=binomial) ## Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 # creates error, SSM_ASD_v24 is a character variable logistic_fit_1 &lt;- glm(factor(SSM_ASD_v24)~V12.aosi.total_score_1_18+ V12.mullen.composite_standard_score, data=full_data, family=binomial) # no error due to use of factor() summary(logistic_fit_1) ## ## Call: ## glm(formula = factor(SSM_ASD_v24) ~ V12.aosi.total_score_1_18 + ## V12.mullen.composite_standard_score, family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.0002 -0.5847 -0.4436 -0.3207 2.6519 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.946041 1.021964 1.904 0.0569 ## V12.aosi.total_score_1_18 0.136239 0.033790 4.032 5.53e-05 ## V12.mullen.composite_standard_score -0.044525 0.009891 -4.502 6.74e-06 ## ## (Intercept) . ## V12.aosi.total_score_1_18 *** ## V12.mullen.composite_standard_score *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 453.79 on 511 degrees of freedom ## Residual deviance: 398.41 on 509 degrees of freedom ## (75 observations deleted due to missingness) ## AIC: 404.41 ## ## Number of Fisher Scoring iterations: 5 There is strong evidence that both AOSI total score and Mullen composite score at 12 months are associated with ASD diagnosis at 24 months, with AOSI being positively associated and Mullen negatively associated. Note that with logistic regression, the test statistic corresponding to the hypothesis test of the regression coefficient equalling 0 is approximately standard normal (often denoted by Z), with the approximation’s accuracy improving as the sample size increases. Thus, R denotes the test statistic value by “z value” and corresponding p-value by “Pr(&gt;|z|)”. The other pieces of the output are not as often used, though AIC is sometimes mentioned as a measure of how well the model “fits” the data (similar to how R-squared is sometimes used). 8.3.3 Example 2: Categorical Covariates Now, suppose we use AOSI total score at 12 months and study site as covariates in the model. As with linear regression, since study site is categorical, we will use dummy variable coding. The resulting logistic regression model is \\(\\mbox{logit}[\\Pr(\\mbox{ASD at 24 months}|AOSI_{12}, Site)]=\\beta_0+\\beta_1AOSI_{12}+\\beta_2Site_{SEA}+\\) \\(\\beta_3Site_{STL}+\\beta_4Site_{UNC}\\) where \\(Site_{SEA}, Site_{STL},\\) and \\(Site_{UNC}\\) are dummy variables for Seattle, St. Louis, and UNC respectively with Philadelphia as the reference group. We can see that \\(\\beta_0\\) denotes the log odds of ASD at month 24 when AOSI total score at month 12 is 0 and the study site is Philadelphia (the reference group). Furthermore, \\(\\beta_1\\) denotes the change in the log odds of ASD at month 24 when AOSI total score increases by 1 unit and study site is held constant, \\(\\beta_2\\) denotes the change in the log odds of ASD at month 24 when study site changes from Philadelphia to Seattle and AOSI total score at 12 months is constant, etc. We now fit this model to the data; recall for categorical covariates, R will automatically do dummy variable coding. # Fit logistic regression model logistic_fit_2 &lt;- glm(SSM_ASD_v24~V12.aosi.total_score_1_18+Study_Site, data=full_data, family=binomial) ## Error in eval(family$initialize): y values must be 0 &lt;= y &lt;= 1 # creates error, SSM_ASD_v24 is a character variable logistic_fit_2 &lt;- glm(factor(SSM_ASD_v24)~V12.aosi.total_score_1_18+Study_Site, data=full_data, family=binomial) # no error due to use of factor() summary(logistic_fit_2) ## ## Call: ## glm(formula = factor(SSM_ASD_v24) ~ V12.aosi.total_score_1_18 + ## Study_Site, family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.8216 -0.5982 -0.4783 -0.3929 2.3559 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.71066 0.32923 -8.233 &lt; 2e-16 *** ## V12.aosi.total_score_1_18 0.18013 0.03168 5.687 1.3e-08 *** ## Study_SiteSEA 0.19587 0.34450 0.569 0.570 ## Study_SiteSTL -0.07735 0.35943 -0.215 0.830 ## Study_SiteUNC 0.00763 0.35218 0.022 0.983 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 453.79 on 511 degrees of freedom ## Residual deviance: 419.48 on 507 degrees of freedom ## (75 observations deleted due to missingness) ## AIC: 429.48 ## ## Number of Fisher Scoring iterations: 4 There is strong evidence of AOSI total score at 12 months being positively associated with ASD at 24 months, controlling for study site. 8.3.4 Prediction Along with conducting inference on associations, logistic regression is often used for prediction of binary outcomes. Recall, with logistic regression, we are modelling the probability of the outcome. Specifically, the logistic regression model is \\(\\mbox{logit}(p_{x,z})=\\beta_0+\\beta_1X+\\beta_2Z\\) where we denote \\(\\mbox{Pr}(Y=1|X,Z)\\) by \\(p_{x,z}\\). With some algebra, from this model, we can express a function for the probability of the outcome with the following: \\(\\Pr(Y=1|X,Z)=\\frac{e^{\\beta_0+\\beta_1X+\\beta_2Z}}{1+e^{\\beta_0+\\beta_1X+\\beta_2Z}}\\) where \\(e^x\\) is the exponential function. As a result, after estimating the regression parameters (denoted by \\((\\hat{\\beta_0}, \\hat{\\beta_1}, \\hat{\\beta_2})\\), we obtain the following estimate for the probabillity of the outcome, denoted by \\(\\widehat{\\Pr(Y=1|X,Z)}\\): \\(\\widehat{Pr(Y=1|X,Z)}=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}X+\\hat{\\beta_2}Z}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}X+\\hat{\\beta_2}Z}}\\). So for each subject, we have an estimate of the likelihood of an outcome of \\(Y=1\\) based on their predictors \\(X\\) and \\(Z\\). Using this estimate, we can predict each subject’s outcome by setting a threshold for this estimated probability. For example, we could decide if a subject’s probability of \\(Y=1\\) is above \\(50\\%\\), we will predict \\(Y=1\\) and otherwise predict \\(Y=0\\). Let’s consider the example where we predict ASD at 24 months from AOSI total score at 12 months. For simplicity, we do not include study site as predictor. First, let’s fit the corresponding logistic regression model in R to the data. # Fit logistic regression model logistic_fit_pred &lt;- glm(factor(SSM_ASD_v24)~V12.aosi.total_score_1_18, data=full_data, family=binomial) summary(logistic_fit_pred) ## ## Call: ## glm(formula = factor(SSM_ASD_v24) ~ V12.aosi.total_score_1_18, ## family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7430 -0.6077 -0.4732 -0.3988 2.3400 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.67093 0.23679 -11.280 &lt; 2e-16 *** ## V12.aosi.total_score_1_18 0.17923 0.03149 5.691 1.27e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 453.79 on 511 degrees of freedom ## Residual deviance: 420.13 on 510 degrees of freedom ## (75 observations deleted due to missingness) ## AIC: 424.13 ## ## Number of Fisher Scoring iterations: 4 Rounding the estimates, we see that the formula for the estimated probability of ASD at 24 months is \\(\\widehat{\\Pr(\\mbox{ASD at 24 months}}|AOSI_{12})=\\frac{e^{\\hat{\\beta_0}+\\hat{\\beta_1}AOSI_{12}}}{1+e^{\\hat{\\beta_0}+\\hat{\\beta_1}AOSI_{12}}}\\) \\(=\\frac{e^{-2.67+0.18AOSI_{12}}}{1+e^{-2.67+0.18AOSI_{12}}}\\) From this estimated model, we can have R calculate each subject’s estimated probabilities of ASD at 24 months (done by plugging in their AOSI total score at 12 months into the formula) using the predict() function. The fit object from the glm() function is passed into predict(), along with the argument type=“response” to specify R to return the estimated probabilities. # Obtain estimated probabilities from model fit logistic_est_probs &lt;- predict(logistic_fit_pred, type=&quot;response&quot;) From these probabilities, we can predict ASD diagnosis. Let’s use a threshold of 0.5 and save the corresponding predictions. We will also see the breakdown of the predicted ASD diagnosis using table() (see Chapter 6 for more information on this function). # Obtain predictions from model fit using a 50% threshold logistic_predict &lt;- ifelse(logistic_est_probs&gt;0.5, &quot;YES_ASD&quot;, &quot;NO_ASD&quot;) table(logistic_predict) ## logistic_predict ## NO_ASD YES_ASD ## 502 10 table(full_data$SSM_ASD_v24) ## ## NO_ASD YES_ASD ## 489 98 Now, let us compare the predicted ASD diagnoses to the actual diagnoses in the data. This is commonly done using a confusion matrix. While this can be done manually, the package caret has a function called confusionMatrix() which creates and formats a confusion matrix with a single function. Make sure you first install the caret package. We now create the confusion matrix for the above ASD prediction model. Note that some subjects have a missing AOSI total score at 12 months; with logistic regression, any subjects with missing outcome or covariate values are removed from the analysis. Thus, the estimated probabilities are only computed for the subjects used in the analysis, which only consists of those with complete data for the variables in the model. When using confusionMatrix(), you must include 1) the vector of predicted outcomes (created above), 2) the vector of actual outcomes (created by pulling out the outcome variable from the dataset used when fitting the logistic regression model), and 3) the value of the outcome that denotes a “positive” result (ex. diagnosis of ASD) as arguments. See below for the example with the ASD logistic regression results. Note that these vectors supplied to confusionMatrix() must be of factor type. This can be satisfied by using the function factor() with the vectors (see example below). Note that confusionMatrix() also computes measures such as sensitivity, specificty, positive predictive value (PPV), negative predictive value (NPV), etc. It also prints what is being used as the “positive” result at the bottom of the output. # load caret library(caret) # Remove subjects with missing AOSI total score from data set full_data_complete &lt;- full_data %&gt;% filter(is.na(V12.aosi.total_score_1_18)==0) %&gt;% select(SSM_ASD_v24, V12.aosi.total_score_1_18) # create confusion matrix confusionMatrix(data=factor(logistic_predict), reference = factor(full_data_complete$SSM_ASD_v24), positive = &quot;YES_ASD&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction NO_ASD YES_ASD ## NO_ASD 422 80 ## YES_ASD 7 3 ## ## Accuracy : 0.8301 ## 95% CI : (0.7947, 0.8616) ## No Information Rate : 0.8379 ## P-Value [Acc &gt; NIR] : 0.7085 ## ## Kappa : 0.0307 ## ## Mcnemar&#39;s Test P-Value : 1.171e-14 ## ## Sensitivity : 0.036145 ## Specificity : 0.983683 ## Pos Pred Value : 0.300000 ## Neg Pred Value : 0.840637 ## Prevalence : 0.162109 ## Detection Rate : 0.005859 ## Detection Prevalence : 0.019531 ## Balanced Accuracy : 0.509914 ## ## &#39;Positive&#39; Class : YES_ASD ## To do this prediction analysis, we used a \\(50\\%\\) threshold. However we could choose a different probability threshold. For example, if an outcome has a high cost involved (ex. a serious disease), an estimated probability below \\(50\\%\\) may be enough to predict \\(Y=1\\). Thus, we may want to see how the prediction model preforms across the various thresholds we could choose (\\(0\\%\\) to \\(100\\%\\)). This is usually done by constructing an ROC curve. When created in R, an ROC curve has specificity on the x-axis and sensitivity on the y-axis. For each threshold, there will be a corresponding confusion matrix and thus a corresponding specificity and sensitivity For our example, with a threshold of \\(50\\%\\) we had a specificity of 0.98 and a sensitivity of 0.04, or (0.98, 0.04) as an x,y pair. Let us see the confusion matrix in our ASD example with the logistic regression prediction model if we used a threshold of \\(40\\%\\) instead. # Obtain predictions from model fit using a 50% threshold logistic_predict &lt;- ifelse(logistic_est_probs&gt;0.4, &quot;YES_ASD&quot;, &quot;NO_ASD&quot;) ftable(logistic_predict) ## logistic_predict NO_ASD YES_ASD ## ## 489 23 ftable(full_data$SSM_ASD_v24) ## NO_ASD YES_ASD ## ## 489 98 # load caret library(caret) # Remove subjects with missing AOSI total score from data set full_data_complete &lt;- full_data %&gt;% filter(is.na(V12.aosi.total_score_1_18)==0) %&gt;% select(SSM_ASD_v24, V12.aosi.total_score_1_18) # create confusion matrix confusionMatrix(data=factor(logistic_predict), reference = factor(full_data_complete$SSM_ASD_v24), positive = &quot;YES_ASD&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction NO_ASD YES_ASD ## NO_ASD 416 73 ## YES_ASD 13 10 ## ## Accuracy : 0.832 ## 95% CI : (0.7968, 0.8634) ## No Information Rate : 0.8379 ## P-Value [Acc &gt; NIR] : 0.6667 ## ## Kappa : 0.1273 ## ## Mcnemar&#39;s Test P-Value : 1.99e-10 ## ## Sensitivity : 0.12048 ## Specificity : 0.96970 ## Pos Pred Value : 0.43478 ## Neg Pred Value : 0.85072 ## Prevalence : 0.16211 ## Detection Rate : 0.01953 ## Detection Prevalence : 0.04492 ## Balanced Accuracy : 0.54509 ## ## &#39;Positive&#39; Class : YES_ASD ## We see that for a \\(40\\%\\) threshold, we have specificity, sensitivity pair of (0.97, 0.12). You can imagine repeating these calculations for each threshold between \\(0\\%\\) and \\(100\\%\\), and plotting the corresponding specificity, sensitivity points. This plot is an ROC curve. In R, can you create an ROC curve from your logistic regression prediction model using the pROC package (which must be installed) which includes the roc() function. To create an ROC curve, 1) specify the outcome vector from the dataset used to create the prediction model as the response= argument then 2) specify the vector of estimated probabilities from the prediction model using the predictor= argument. To also calculate AUC (Area Under the Curve), specify the auc=TRUE argument. You must save the output created by roc() as an object, and then use the function plot() with this object to visualize the ROC curve. You can see that the output from roc() is a list which holds a variety of results related to ROC curves (sensitivities, specificities, thresholds corresponding to these, etc.) This is similar to how lm() and glm() worked; first you compute the output of interest and save it as an R object, and use a different function to visualize the main results (summary() was used with lm() and glm()). See the example below based on the ASD logistic regression model. # load pROC package library(pROC) # Remove subjects with missing AOSI total score from data set full_data_complete &lt;- full_data %&gt;% filter(is.na(V12.aosi.total_score_1_18)==0) %&gt;% select(SSM_ASD_v24, V12.aosi.total_score_1_18) # create ROC curve matrix roc_object &lt;- roc(response = full_data_complete$SSM_ASD_v24, predictor=logistic_est_probs, auc=TRUE, auc.polygon=TRUE) plot(roc_object, print.auc=TRUE, grid=TRUE, print.thres=&quot;best&quot;, print.thres.best.method=&quot;closest.topleft&quot;) # Print out each threshold and its corresponding sensitivity and specificity cbind(thresholds=roc_object$thresholds, sensitivities=roc_object$sensitivities, specificities=roc_object$specificities) ## thresholds sensitivities specificities ## [1,] -Inf 1.00000000 0.00000000 ## [2,] 0.07057592 0.98795181 0.05128205 ## [3,] 0.08326787 0.96385542 0.16550117 ## [4,] 0.09800015 0.90361446 0.30769231 ## [5,] 0.11501009 0.78313253 0.44289044 ## [6,] 0.13452975 0.69879518 0.59440559 ## [7,] 0.15677255 0.61445783 0.69696970 ## [8,] 0.18191642 0.54216867 0.76223776 ## [9,] 0.21008421 0.40963855 0.82983683 ## [10,] 0.24132236 0.32530120 0.87645688 ## [11,] 0.27558004 0.28915663 0.91142191 ## [12,] 0.31269154 0.20481928 0.94405594 ## [13,] 0.35236505 0.16867470 0.96037296 ## [14,] 0.39418119 0.12048193 0.96969697 ## [15,] 0.43760346 0.08433735 0.97902098 ## [16,] 0.48200136 0.03614458 0.98368298 ## [17,] 0.52668471 0.02409639 0.99067599 ## [18,] 0.57094550 0.01204819 0.99533800 ## [19,] 0.65330585 0.01204819 0.99766900 ## [20,] 0.74740883 0.00000000 0.99766900 ## [21,] Inf 0.00000000 1.00000000 You can see the ROC curve also includes a 45 degree line. This 45 degree line denotes where specificity and sensitivity as always the same. An equal specificity and sensitivity would result if you simply predict subjects’ outcomes completely at random (i.e., flip a coin to decide each person’s prediction). As a result, we would want our prediction model to be “better” then the “completely random” prediction model based on its distance from this 45 degree line. The main way of measuring this distance is using area under the curve (AUC). As the name suggests, AUC measures how much space there is under the ROC curve. You can see that the 45 degree line would have an AUC of 0.50, thus we would like our prediction model to have an AUC much higher then 0.50. How much higher? There various rules of thumb that people have created, such as it should be higher then 0.70. However, as with all rules of thumb, this should not be strictly for every analysis. One may also be interested in what threshold provides the “best” prediction performance. There are are many ways of defining what is “best”, though a common method is to mark the threshold corresponding the lop left point on the ROC curve as the “best” performance. This method weights sensitivity and specificity equally. In R, this “best”\" threshold is added to the ROC curve plot by including the arguments print.thres=“best” and print.thres.best.method=“closest.topleft” into the plot() function call. This adds a labeled point on the graph at the top left point on the ROC curve with the following label (from left tot right); 1) the probabilitiy threshold, 2) corresponding specificity and 3) corresponding sensitivity where 2) and 3) are in parentheses. plot(roc_object, print.auc=TRUE, grid=TRUE, print.thres=&quot;best&quot;, print.thres.best.method=&quot;closest.topleft&quot;) 8.4 Mixed Models 8.4.1 Motivation Recall in our discussion of linear regression, we assumed that each observation’s residual was independent and had the same variance (denoted \\(\\sigma^2\\)). However, there will be datasets where this assumption will be problematic. The most common example is where you have multiple observations from the same subject. As a result, it is likely that these residuals are correlated. Furthermore, we may want to model subject-specific variance instead of assuming the same variance for all subjects. This is generally done using mixed models. Suppose we are interested in the relationship between outcome \\(Y\\) and covariates \\(X\\) and \\(Z\\). Suppose further that we have \\(n\\) subjects, each of which have \\(m\\) observations. We will let \\(Y_{ij}, X_{ij}\\), and \\(Z_{ij}\\) denote \\(Y, X\\), and \\(Z\\) for subject \\(i\\) at observation \\(j\\) respectively. The random intercept-only mixed model corresponding to this is the following: \\(Y_{ij}=\\beta_{0}+\\beta_{1}X_{ij}+\\beta_{2}Z_{ij}+\\phi_{i}+\\epsilon_{ij}\\) where \\(\\phi_i\\) are independent across index \\(i\\) and \\(\\epsilon_{ij}\\) are independent across \\(i\\). The \\(\\epsilon_{ij}\\) are observation-specific residuals, \\(\\phi_{i}\\) are referred to as the random intercept, and \\(\\beta_{0}, \\beta_{1}\\), and \\(\\beta_{2}\\) are referred to as the fixed effects. When \\(X\\) and \\(Z\\) are 0, we have \\(Y_{ij}=\\beta_{0}+\\phi_{i}+\\epsilon_{ij}\\). We can see that \\(b_i\\) represents subject-specific differences in the outcome when \\(X\\) and \\(Z\\) are 0, beyond the differences in residuals (see below for a more detailed explanation). Recall with the previously detailed regression methods, we did not model observation/subject-specific means; observations with the same \\(X\\) and \\(Z\\) had the same mean in our model. It is generally assumed that both the residuals and random effects has mean 0. As a result, we have the following model for the mean outcome across the population with covariate values \\(X\\) and \\(Z\\): \\(\\mbox{E}(Y|X,Z)=\\beta_{0}+\\beta_{1}X+\\beta_{2}Z\\) which is the same as for linear regression. We can model subject-specific means by conditioning on their random effects. \\(\\mbox{E}(Y_{i,j}|X_{ij},Z_{ij},b_i)=\\beta_{0}+\\beta_{1}X_{ij}+\\beta_{2}Z_{ij}+b_i.\\) Recall with linear regeression, we did not have this separate model for each subject’s mean. That is, the mean outcomes for two subjects with the same covariate values were modeled equivalently. These random effects allow for the mean outcomes for these two subjects to be modeled differently. Notice that for subject \\(i\\), one way their observations across index \\(j\\) are tied together is by random effect \\(\\phi_{i}\\). As a result, one way we have incorporated the correlation within subjects is through \\(\\phi_{i}\\). A second way we can incorporate this correlation is through correlated \\(\\epsilon_{ij}\\). We have already assumed that the \\(\\phi_{i}\\) are independent; we now need to specify the dependence structure for \\(\\epsilon_{ij}\\). There are various options we can specify. The most general structure is to make no assumptions at all; this is called unstructured. The most extreme structure is to assume the \\(\\epsilon_{ij}\\) are independent across \\(i\\) and \\(j\\) (subjects and observations). Notice this is the same assumption as was made for the \\(\\epsilon\\) in the linear regression model. This structure is commonly referred to as the independence structure. Note that even if we assuming independent \\(\\epsilon_{ij}\\), within-subject correlation is still being represented with \\(\\phi_i\\). Along with decomposing the means into population-level and subject-level, we can also decompose the variance of the outcome. Since we are considering multiple subjects, each of which having multiple observations, you could consider how the outcome varies both between the subjects as well as between the observations within a given subject. These are referred to as the between-subject variance and within-subject variance. More specifically, you could imagine a dataset where the subjects have similar data when comparing one another, however within each subject the data from time point to time point varies greatly. In this case, the between-subject variance would be low while the within-subject variance would be high. For example, consider the data below. We can see that the boxplots between the subjects are similar (between-subject variance), though within a given subject there is noticable variation in their outcomes across their observations (within-subject variance). Mixed models allow these two variance components to be modeled separately based on the chosen covariance structures for the random effects and the residuals. Please consult a statistican for detail on how to specify and estimate these components explicitly with your mixed model. Now, we have only considered a subject-specific intercept. However, we can also consider subject-specific effects with respect to the model’s covariate \\(X\\) and \\(Z\\). Consider the following model: \\(Y_{ij}=\\beta_{0}+\\beta_{1}X_{ij}+\\beta_{2}Z_{ij}+\\phi_{0i}+\\phi_{1i}X_{ij}+\\epsilon_{ij}\\) where \\(\\phi_{0i}\\) and \\(\\phi_{1i}\\) are independent across index \\(i\\) and \\(\\epsilon_{ij}\\) are independent across \\(i\\). This is referred to as a random slope model. We can also specify a \\(\\phi\\) for variable \\(Z\\). Now, \\(\\phi_{0i}\\) represents the random intercept, \\(\\beta_{1}\\) represents the population level changes in the mean outcome when \\(X\\) increases by 1 controlling for \\(Z\\), and \\(\\phi_{1i}\\) represents subject-specific differences in the mean outcome change when \\(X\\) increases by 1 controlling for \\(Z\\) from this population level change. We have discussed specifying a dependence structure for \\(\\epsilon_{ij}\\) within each subject; we can also specify a dependence structure for \\(\\phi_{0i}\\) and \\(\\phi_{1i}\\) within subject \\(i\\). In seems likely that these random effects are independent between subjects, however are correlated within a subject. These structures are the same as was discussed for \\(\\epsilon_{ij}\\); unstructured, independence, among others. As you can see, there are many things that go into a mixed model. You must specify the fixed effects, the random effects (intercept, slopes, etc.), and the dependence structures for \\(\\epsilon_{ij}\\) and your random effects. Generally, unstructured is chosen for \\(\\epsilon_{ij}\\) and for the random effects. However, due to it’s generality, unstructured has many parameters. This can be a problem depending on the size of the data set, causing these parameters to be estimated inaccurately and with high variance. As a result, a more specified structure may be required or advantageous. 8.4.2 Example: Mullen composite and Visit When running a mixed model in R, the data must be long form. That is, each observation for a subject must be a separate row in your data. The R code below converts the dataset to long form (this is the same code as was used in the long form example in Chapter 4). mixed_model_data &lt;- full_data %&gt;% select(Identifiers, GROUP, V36.mullen.composite_standard_score:V12.mullen.composite_standard_score) vars_to_convert &lt;- names(mixed_model_data)[c(-1,-2)] mixed_model_data &lt;- mixed_model_data %&gt;% gather(variable, var_value, vars_to_convert) %&gt;% separate(variable,c(&quot;Visit&quot;,&quot;Variable&quot;),sep=3) %&gt;% spread(key=Variable, value=var_value) %&gt;% plyr::rename(c(&quot;.mullen.composite_standard_score&quot;=&quot;Mullen_Composite_Score&quot;)) %&gt;% mutate(ASD_Diag = factor(ifelse(grepl(&quot;ASD&quot;, GROUP), &quot;ASD_Pos&quot;, &quot;ASD_Neg&quot;)), Visit=factor(Visit), Visit_Num=as.numeric(Visit)-1) %&gt;% arrange(Identifiers, Visit) Now, let us fit model Mullen composite score as the outcome and visit number (1st, 2nd, 3rd, or 4th) and ASD diagnosis (positive or negative) as covariates. Note that in order for the intercept in the model to be interpretable, we code visit so that 0 reflects the first visit, 1 reflects the second visit, etc. We fit the following random intercept-only model: \\(Mullen_{ij}=\\beta_{0}+\\beta_{1}Visit_{ij}+\\beta_{2}Group_{ij}+\\phi_{i}+\\epsilon_{ij}\\) where \\(\\phi_i\\) are independent across index \\(i\\) and \\(\\epsilon_{ij}\\) are independent across \\(i\\). To fit this model in R, we can either use the lme4 package or the nlme package. The nlme package is more flexible, so it is covered here. The function used from this package is called lme(), and it works similarly to the lm(). The arguments are as follows, in order of appearance in the function: fixed: Works the same as the model argument in lm(). Here you specify the fixed effects of the model using the usual y~x notation data: Specify the dataset random: Here you specify the random effects for your model using the ~ notation. To specify a random intercept, use random = ~ 1|ID where ID is the identifier variable. You must have a variable identifying the subjects in your data when running a mixed model. To specify a random intercept and a random slope for variable x, use random = ~ 1+x|ID. To include more random slopes, just add +z+… where z is one of the other covariate. correlation: specify the dependence structure for the residuals (recall the Motivation section). The default is the independence structure with equal variances for each observation’s residual within a subject. For unstructured, specify correlation=corSymm()). Note that lme() uses unstructured covariance for the random effects by default (changing this structure is not covered here as unstructured is generally sufficient for the random effects). Let us fit the example model detailed above in R. As with the previous regression analyses, we save the output into an object; this unables us to see all of the different computations that R does for the mixed model. library(nlme) mixed_fit &lt;- lme(Mullen_Composite_Score~Visit_Num + ASD_Diag, data=mixed_model_data, random = ~1|Identifiers, na.action = na.omit) We must make sure all of our variables are either numeric or factor variables. This can either be done by changing the dataset itself, or by specifying as.numeric() and factor() in the lme() function. By default, R will output an error if the variables in the model have any missing values. This can be changed using na.action = na.omit, which removes observations with missing values from the analysis (was used above). As with the previous regression analyses, we can use summary() with the saved object to see important results from the model fit (estimates, standard errors, etc.). It also prints out Model fit statistics (AIC, BIC, log likelihood) Standard deviations and correlations for your random effects Standard deviations and correlations for your residuals (depending on the chosen structure) The number of observations used in the dataset as well as number of subjects (referred to as “Number of Groups”). To view the dependence structure (also called covariance matrix) of the random effects, use getVarCov() with the saved outputted from lme(). Note that since we only have a single random effect (the random intercept), this matrix is only a single entry: the variance (standard deviation is also reported) of the random intercept. To view the dependence structure of a subject’s residuals, add the arguments type=“conditional” and specify the subject of interest with the argument individuals=“…” where … is replaced by the value of the ID variable corresonding to this subject. By default, R assumes all subjects has the same dependence structure for their residuals so you should see the same result when you change the value of the ID variable. Futher, recall that by default, R specifies independence structure with equal variance across observations for the residuals (see Motivation section). The results from our example fit this structure as seen below. summary(mixed_fit) ## Linear mixed-effects model fit by REML ## Data: mixed_model_data ## AIC BIC logLik ## 15192.52 15220.14 -7591.262 ## ## Random effects: ## Formula: ~1 | Identifiers ## (Intercept) Residual ## StdDev: 7.442339 13.07822 ## ## Fixed effects: Mullen_Composite_Score ~ Visit_Num + ASD_Diag ## Value Std.Error DF t-value p-value ## (Intercept) 101.63063 0.6213158 1264 163.57322 0e+00 ## Visit_Num 1.14286 0.3058513 1264 3.73667 2e-04 ## ASD_DiagASD_Pos -15.21819 1.1648815 585 -13.06415 0e+00 ## Correlation: ## (Intr) Vst_Nm ## Visit_Num -0.640 ## ASD_DiagASD_Pos -0.292 -0.036 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.103657320 -0.570984559 -0.004197719 0.595802345 3.191924873 ## ## Number of Observations: 1852 ## Number of Groups: 587 getVarCov(mixed_fit) ## Random effects variance covariance matrix ## (Intercept) ## (Intercept) 55.388 ## Standard Deviations: 7.4423 getVarCov(mixed_fit, type=&quot;conditional&quot;, individuals=&quot;1&quot;) ## Identifiers 1 ## Conditional variance covariance matrix ## 1 2 3 4 ## 1 171.04 0.00 0.00 0.00 ## 2 0.00 171.04 0.00 0.00 ## 3 0.00 0.00 171.04 0.00 ## 4 0.00 0.00 0.00 171.04 ## Standard Deviations: 13.078 13.078 13.078 13.078 Now, let’s unpack these results so we completely understand our fitted model. First, let’s discussion the regression parameter results. From our estimates, the fitted model based on the data is \\(Mullen_{ij}=86.41+1.14Visit_{ij}-15.22Group_{ij}+\\phi_{i}+\\epsilon_{ij}\\). Note that all regression parameters are highly significant from 0 based on the reported p-values (all three are ~0). We can view each subject’s random intercept (\\(\\phi_{i}\\)) using random.effects() with the lme object. What is returned is a matrix; we print out only the first 10 subjects’ random intercepts. random.effects(mixed_fit)[1:10,] ## [1] -4.8683584 -11.0760256 -2.7521082 -3.4649297 0.3931966 ## [6] -7.1861067 4.5842258 0.4928088 0.2574965 3.4879100 We can see then that the first subject in the dataset (Identifier PHI0000) has the following model: \\(Mullen_{1j}=86.41+1.14Visit_{1j}-15.22Group_{1j}-4.86+\\epsilon_{1j}\\). Note that when fitting this model, the visit variable reflected the order of the visit (1st=0, 2nd=1, 3rd=2, 4th=3) and was fit linearly as a continuous variable. Thus, 1.14 reflects a 1.14 unit increase in the mean Mullen composite score as the child goes from one visit to the next, controlling for ASD diagonsis at Month 24. This may not be the optimal coding for visit, especially since the gap in months between the visits is not uniform. This coding was only done for simplicity to facilitate an illustration of mixed models. We can also see from the results of summary() that ASD negative is the reference level, and that a change to ASD positive, holding visit number constant, decreases mean Mullen composite score by 15.22 units. The intercepts in the model can be interpreted as follows. Suppose the child is ASD negative, i.e., \\(Group_{ij}=0\\). Then their model is \\(Mullen_{ij}=86.41+1.14Visit_{ij}+\\phi_i+\\epsilon_{1j}\\). The total intercept for subject \\(i\\) is \\(86.41+\\phi_i\\), where we assume the mean of \\(\\phi_i\\) is 0. Thus, we see that the fixed intercept of \\(86.41\\) can be viewed as the “population-level” baseline, with \\(\\phi_i\\) being subject \\(i\\)’s deviation in their baseline from the population. In this case, since a visit at 6 months is reflected by \\(Visit_{ij}=0\\), “baseline” refers to the mean Mullen composite score at the 6 month visit. We see for subject 1, their predicted random intercept is -4.86, implying that this subject has a baseline mean which is 4.86 units below the population’s baseline mean. Suppose the child is ASD positive, i.e., \\(Group_{ij}=1\\). Then their model is \\(Mullen_{ij}=86.41+1.14Visit_{ij}-15.22+\\phi_i+\\epsilon_{1j}\\). Thus, we see that \\(\\phi_i\\) has the same interpretation, and the population-level baseline in this case is 86.41-15.22. That is, the ASD negative and ASD positive populations have different “baselines” as reflected by the regression parameter \\(\\beta_2\\). We can plot each subject’s fitted model (i.e., ignoring their residuals \\(\\epsilon_{ij}\\)) and visualize these interpretations for all subjects with the following code. mixed_effect_data_complete &lt;- mixed_fit$data[complete.cases(mixed_fit$data),] mixed_effect_data_complete &lt;- data.frame(mixed_effect_data_complete, &quot;predicted_value&quot;=predict(mixed_fit, newdata = mixed_effect_data_complete)) ggplot(data=mixed_effect_data_complete, mapping=aes(x=Visit, y=predicted_value, color=ASD_Diag, group=Identifiers))+ geom_point()+ geom_line()+ labs(y=&quot;Fitted Value: Mullen Composite&quot;, title=&quot;Mixed model fitted values without interaction term&quot;) To create this plot, we had to do the following 1) Remove all missing values from the dataset used to fit our mixed model. Recall when fitting the mixed model, we had R remove all subjects with missing values from the analysis 2) Add the fitted values from the estimated model to this dataset. To obtain the fitted values from the mixed model, we use the predict() function as with logistic regression. Recall mixed_fit is the name of the object which contains the mixed model output 3) Call ggplot() to create the plot. We colored the subjects by their ASD diagnosis for better visualization (color=ASD_Diag), and had to specify each subject as a “group” so that R connected each subject’s points with a line (group=Identifiers). Finally, we called both geom_point() and geom_line() to create a scatterplot with connected points Note that no interaction terms between visit and ASD diagnosis were included It may make sense to include an interaction term between these variables; this would imply that the change in Mullen composite score over time is different between the ASD diagnosis groups. That model would be the following: \\(Mullen_{ij}=\\beta_{0}+\\beta_{1}Visit_{ij}+\\beta_{2}Group_{ij}+\\beta_{3}Group_{ij}*Visit_{ij}+\\phi_{i}+\\epsilon_{ij}\\) where \\(\\phi_i\\) are independent across index \\(i\\) and \\(\\epsilon_{ij}\\) are independent across \\(i\\). We see that for a child who is ASD negative, their model is \\(Mullen_{ij}=\\beta_{0}+\\beta_{1}Visit_{ij}+\\phi_{i}+\\epsilon_{ij}\\) implying that their intercept is \\(\\beta_{0}+\\phi_{i}\\) and their slope with respect to visit number is \\(\\beta_1\\). For a child who is ASD positive, their model is \\(Mullen_{ij}=\\beta_{0}+\\beta_{1}Visit_{ij}+\\beta_{2}+\\beta_{3}Visit_{ij}+\\phi_{i}+\\epsilon_{ij}\\) so we see that their intercept is \\(\\beta_{0}+\\beta_{2}+\\phi_{i}\\) and their slope is \\(\\beta_1+\\beta_3\\). Thus, the difference in the trends with visit between ASD positive is ASD negative is \\(\\beta_3\\). Without this interaction term, this difference was not modeled. We fit this model to the data and create the same fitted values plot as was done above. # Fit model mixed_fit_interact &lt;- lme(Mullen_Composite_Score~Visit_Num + ASD_Diag + Visit_Num*ASD_Diag, data=mixed_model_data, random = ~1|Identifiers, na.action = na.omit) # Print out results summary(mixed_fit_interact) ## Linear mixed-effects model fit by REML ## Data: mixed_model_data ## AIC BIC logLik ## 15084.22 15117.35 -7536.108 ## ## Random effects: ## Formula: ~1 | Identifiers ## (Intercept) Residual ## StdDev: 7.851045 12.49571 ## ## Fixed effects: Mullen_Composite_Score ~ Visit_Num + ASD_Diag + Visit_Num * ASD_Diag ## Value Std.Error DF t-value p-value ## (Intercept) 99.64991 0.6398681 1263 155.73507 0.0000 ## Visit_Num 2.66635 0.3250264 1263 8.20348 0.0000 ## ASD_DiagASD_Pos -3.89798 1.5796181 585 -2.46767 0.0139 ## Visit_Num:ASD_DiagASD_Pos -8.02574 0.7498930 1263 -10.70251 0.0000 ## Correlation: ## (Intr) Vst_Nm ASD_DA ## Visit_Num -0.660 ## ASD_DiagASD_Pos -0.405 0.267 ## Visit_Num:ASD_DiagASD_Pos 0.286 -0.433 -0.669 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -3.3559289829 -0.5953731509 0.0003434236 0.5877083391 3.1766641043 ## ## Number of Observations: 1852 ## Number of Groups: 587 # Create plot mixed_effect_data_complete &lt;- mixed_fit_interact$data[complete.cases(mixed_fit_interact$data),] mixed_effect_data_complete &lt;- data.frame(mixed_effect_data_complete, &quot;predicted_value&quot;=predict(mixed_fit_interact)) ggplot(data=mixed_effect_data_complete, mapping=aes(x=Visit, y=predicted_value, color=ASD_Diag, group=Identifiers))+ geom_point()+ geom_line()+ labs(y=&quot;Fitted Value: Mullen Composite&quot;, title=&quot;Mixed model fitted values with interaction term&quot;) Now, we see ASD negative children have an increasing Mullen composite over time on average while ASD positive children have a decreasing Mullen composite over time on average. Finally, in order to illustrate a simpele example of a random slope model, we now include visit as a random effect to the interaction term model discussed above. This will model subject-specific slopes across time (recall in plot above, slopes were the same within a diagnosis group). The corresponding model is \\(Mullen_{ij}=\\beta_{0}+\\beta_{1}Visit_{ij}+\\beta_{2}Group_{ij}+\\beta_{3}Group_{ij}*Visit_{ij}+\\phi_{0,i}+\\phi_{1,i}*Visit_{ij}+\\epsilon_{ij}\\) where \\((\\phi_{0,i}, \\phi_{1,i})\\) is independent across index \\(i\\) and \\(\\epsilon_{ij}\\) are independent across \\(i\\). Note that for index \\(i\\), we are not assuming \\(\\phi_{0,i}\\) and \\(\\phi_{1,i}\\) are independent; only that the random effects are independent between subjects and not within subjects. We can see that the slope with respect to time for subject \\(i\\) is \\(\\beta_{1}+\\beta_{3}Group_{ij}+\\phi_{1,i}*Visit_{ij}\\). We fit this model to the data with an unstructured covariance structure for the random effects. ## Fit model mixed_fit_interact_rand_slope &lt;- lme(Mullen_Composite_Score~Visit_Num + ASD_Diag + Visit_Num*ASD_Diag, data=mixed_model_data, random = ~1+Visit_Num|Identifiers, na.action = na.omit) ## Print out results summary(mixed_fit_interact_rand_slope) ## Linear mixed-effects model fit by REML ## Data: mixed_model_data ## AIC BIC logLik ## 14924.86 14969.04 -7454.431 ## ## Random effects: ## Formula: ~1 + Visit_Num | Identifiers ## Structure: General positive-definite, Log-Cholesky parametrization ## StdDev Corr ## (Intercept) 6.256243 (Intr) ## Visit_Num 5.483750 -0.188 ## Residual 10.694466 ## ## Fixed effects: Mullen_Composite_Score ~ Visit_Num + ASD_Diag + Visit_Num * ASD_Diag ## Value Std.Error DF t-value p-value ## (Intercept) 99.67841 0.5430283 1263 183.56026 0.0000 ## Visit_Num 2.74016 0.3867271 1263 7.08552 0.0000 ## ASD_DiagASD_Pos -3.79570 1.3408253 585 -2.83087 0.0048 ## Visit_Num:ASD_DiagASD_Pos -8.24438 0.9162890 1263 -8.99758 0.0000 ## Correlation: ## (Intr) Vst_Nm ASD_DA ## Visit_Num -0.581 ## ASD_DiagASD_Pos -0.405 0.235 ## Visit_Num:ASD_DiagASD_Pos 0.245 -0.422 -0.574 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.828231978 -0.557034760 0.008772265 0.551159808 3.156926759 ## ## Number of Observations: 1852 ## Number of Groups: 587 ## Create plot mixed_effect_data_complete &lt;- mixed_fit_interact_rand_slope$data[ complete.cases(mixed_fit_interact_rand_slope$data),] mixed_effect_data_complete &lt;- data.frame(mixed_effect_data_complete, &quot;predicted_value&quot;=predict(mixed_fit_interact_rand_slope, newdata = mixed_effect_data_complete)) # Notice above, we create variable &quot;predicted_value&quot; to hold each subject&#39;s fitted values, including random effect realizations, in dataset called mixed_effect_data_complete (&quot;complete&quot; meaning all subjects with missing data for variables in model are removed from dataset to be used in plot) ggplot(data=mixed_effect_data_complete, mapping=aes(x=Visit, y=predicted_value, color=ASD_Diag, group=Identifiers))+ geom_point()+ geom_line()+ labs(y=&quot;Fitted Value: Mullen Composite&quot;, title=&quot;Mixed model fitted values with interaction term\\nand random slope for visit&quot;) # Use of \\n forces new line for plot title We see that for most subjects, their slopes follow the general trend for their diagnosis group. Since this model is a little more complicated, let’s dig into the output provided in the summary() command. Since the previous models are simpler, understanding these results will teach the concepts to understand all of the output for other models. First, we see a table with AIC, BIC, and negative loglikelihood (logLik) results for the model. These three measures are different ways of quantifying the “goodness of fit” for our model with respect to the analyzed dataset. For all three, higher implies a better correspondance between the data and the data one woudl expect to see under the chosen mixed model. For logLik, higher corresponds to “less negative”/closer to zero. Next, we see a table entitled “Random effects:” with standard deviation and correlation values. This table provides the estimated standard deviation for each of the random effects and the estimated correlation for each pair of random effects (i.e., the correlation between the random intercept and random slope for visit in this example). Notice that for the earlier mixed models with only a single random intercept, the correlation entry was not provided as these models only had one random effect. We then see a table entitled “Fixed effects:”, which includes the usual calculations for the fixed effects (i.e., beta coefficients). The following table entitled “Correlation:” provides the estimated correlations between each pair of estimates for the fixed effects (between the estimates not the fixed effects themselves). These are generally not important. Finally, we are provided a summary of the spread of the residuals. 8.4.3 Interpreting results: time dependent covariates Recall in the examples above, the covariates in our models either 1) changed predictably over time (visit number) or 2) were constant over time (ASD diagnosis at 24 months). However, for covariates that change randomly over time (for example Vineland scores), interpreting the regression parameters requires more thought. Such covariates are referred to as time dependent covariates. Further, although random intercepts were discussed here, one can also include random effects tied to your model’s covariates. Such random effects are referred to as random slopes. Interpreting these models beyond the scope of these tutorials, so always consult a statistican if you are 1) think of including time dependent covariates and/or 2) including random effects on covariates (i.e., random slopes) in your mixed model. In general, if you have data which includes multiple observations for a given subject or if you are considering the use of any mxed model, please consult a statistican. "],
["documenting-your-results-with-r-markdown.html", "9 Documenting your results with R Markdown 9.1 Intro 9.2 Starting Your R Markdown 9.3 Understanding the R Markdown editor 9.4 Creating your document from the R Markdown file 9.5 Creating tables with R Markdown 9.6 Practice with R Markdown", " 9 Documenting your results with R Markdown 9.1 Intro Another benefit with using R is the ability to pair your statistical analysis with a method of easily documenting the results from it. With R Markdown, you can easily create a document which combines your code, the results from your code, as well as any text or outside images that accompany the analysis. This tutorial details how to use R Markdown; in fact, all of these tutorials were created using it. Thus, you can also supplement the information in this tutorial by studying the included R Markdown files which created all of the other tutorials. 9.2 Starting Your R Markdown To create a new R Markdown file, in R Studio select File&gt;New File&gt;R Markdown… . Then, you will see a window pop-up titled “New R Markdown”. Here, you specify the type of file you wish to create. The standard type is a document (HTML, PDF, or Word) but you have some other options such as creating a slide show (Presentation). In these tutorials, we just cover documents. Within the Document type, you must select what file type you would like. Since it does not have traditional “page” separators like PDF and Word do, HTML is generally recommended and will be covered in these tutorials. You can also choose a title and author for your document using their respective fields. Finally, select Ok to create your new R Markdown file. You will see it appear as a tab in your R Studio session, similar to when a new script is created. 9.3 Understanding the R Markdown editor When creating a new HTML-type R Markdown document, you should see the following window in your R Studio session: which looks similar to a script. Here is where you edit and create the content of your R Markdown. A R Markdown has three main components: the Prelude, the Chunk, and the Non-chunk, which are labeled in the above document for reference. 9.3.1 Prelude In the Prelude, you specify settings and headers for your R Markdown. Some headers you can specify include the title, author, and date. The main setting you may alter is the output setting, which determines which file types are used when creating your document. You can specify multiple file types, or leave it as the single type you chose when creating your Markdown (HTML in the above example). For example, to add a PDF type as an output, add in pdf_document: default below output::. 9.3.2 Chunk A chunk is a specially marked part of your Markdown document where you place R code to have run. You will generally have multiple chunks throughout your Markdown which represent specific components of your analysis. When you create your Markdown file and turn it into a document, these chunks are run in order and any output from them is shown in the document, in the order that their respective chunk appears. A chunk is marked using {r name} to start, as in the following example: You can see that the chunk is shaded in gray and adds in a few icons on the upper right of this shaded space. A chunk has a few components. First, you must name your chunk, which allows you to easily refer to what the code in the chunk does and to facilitate the organization of your document. In the above example, “name” was used as the chunk’s name. Note that each chunk must have a different name or else you will receieve an error when the R Markdown file is compiled. You can also specify a number of options for your chunk inside {r …}, where each option is placed in … and separated by a comma. These options are specified just like arguments in a R function. Note that the chunk’s name is technically an option, so you must separate it from the other options using a comma. Some common options include: echo=TRUE or FALSE If TRUE is selected, the actual code in the chunk appears in your document along with the results it produced. If FALSE is selected, the code does not appear and only the output does warning=TRUE or FALSE If TRUE is selected (is by default), all outputted warnings from the code are included in the document. If FALSE is selected, these wanrings are not included. include=TRUE or FALSE If TRUE is selected (is by default), the output from the code is included in the document. If FALSE is selected, the code is still run but the output is not included in the document. This can be useful if you use certain calculations from this chunk in later chunks and the actual output from this chunk is not of interest. Some others include eval= and messages=; see online for more details and more options that you can set. Inside this shaded space is where you place your code for that part of the analysis. This works exactly like how writing a script works; you are simply writing the usual R code. Finally, let us discuss the icons on the upper right side. The left-most option can be avoided as it simply lets you alter the settings of the chunk instead of using the options through {r }. The next option has R run all of the chunks above this chunk, which may be necessary when this chunk’s code depends on the chunks above it. The last option runs only the current chunk, which is necessary to view the output of the chunk and/or verify the code within it is operating correctly as you create your R Markdown document. Note that when you run a chunk, its output will be shown right below it. To obtain everything that the code in the chunk produces (output, warnings, errors, etc.), make sure to select the Console tab in the lower-left hand window, which contains the usual R console. We will discuss the R Markdown tab later. Note that to specify options for all chunks in your R Markdown (i.e., “global options”), include the following command in a separate chunk at the top of the document (but below the Prelude): knitr::opts_chunk$set(…) where the options of interest are placed as argument separated by commas. You can override these global options for specific chunks but setting the override in the chunk(s) of interest as discussed above. See the .RMD files which created these tutorials for further detail. 9.3.3 Non-chunk The white space outside of your Prelude is where you place the non-R code components of your document. This space largely works like a Word document; you can place text, images, tables, etc. inside of it. This combination of R code, its results, and the Word document-like capabilities is what allows you to create a comprehensive report for your analysis within a single file. You can also format the text which appears in this space. Surrounding your text with ** ** bolds the text, and with * * italicizes the text. To add a subscript, use _ and to add a superscript use ^ . To mark a webpage link, use /linked phrase where “linked phrase” is the text that appears in the document and () holds the actual URL. Note that when adding in links to webpages, if you are creating a document in HTML format, you will have to add self_contained: no to your Prelude below html_document: default. To add images, use !optional caption text where [] specifies the image caption (which can be empty) and () holds the file name for the image. If the image is in the same folder as the Markdown file, you can just include the file name. Otherwise, you must provide the whole file path (or use relative path names if you are familar with them). Along with usual text, you can also specify headers within this space. To start a header, use # and type your header text next to it. Using ## creates a sub header, ### creates a sub-sub header, etc. Note that these headers must be on their own lines in your R Markdown file to be recognized as headers and not just the #, ##, etc. characters. See the R Markdown Cheatsheet for more capabilities as well as for a review of what was discussed above. 9.4 Creating your document from the R Markdown file When you have finished creating your R Markdown file, you can now “compile” it to create the final document. You can think of your R Markdown file as the “instructions” the software uses to create the final document, just like scripts are the “instructions” R uses to complete your analysis. Thus, these R Markdown files are essentially more complex scripts and are denoted by the file type .Rmd instead of .R . With your R Markdown file open in R studio, you create the corresponding document using the Knit button at the top. This will compile your R Markdown into the document file types that you specified in the Prelude. You will see the progress of the compilation as well as any errors or other messages created during the compilation process in the lower left-hand window of R Studio, in the R Markdown tab. 9.5 Creating tables with R Markdown One of the most useful features of R Markdown is the ability to create publication-quality tables. When combined with the plots from ggplot2, this enables you to create detailed reports in R Markdown that display all of your results in a publication-quality format. Recall from our past tutorials that when you output tables with your R script, the format is crude looking. library(stats) aosi_data &lt;- read.csv(&quot;Data/cross-sec_aosi.csv&quot;, stringsAsFactors=FALSE, na.strings = &quot;.&quot;) table_ex_xtabs &lt;- xtabs(~Gender+Study_Site, data=aosi_data) table_ex_xtabs ## Study_Site ## Gender PHI SEA STL UNC ## Female 55 67 60 53 ## Male 94 85 85 88 In this section, we will detail how to create custom tables to summarize a variety of standard analyses, as well as how to output them in a visually pleasing format using kable(). 9.5.1 Summary statistics First, we discuss some functions which create publication quality summary statistics tables. We have discussed previously how to create some crude-looking summary statistics tables in R (see Chapter 7), however here we discuss some functions which take advantage of R Markdown to better display these results. The first function we discuss is the function describe() from the Hmisc package, which was discussed in Chapter 7. When used in R Markdown, the results look the same as when used in a usual R script. library(Hmisc) describe(aosi_data) ## aosi_data ## ## 8 Variables 587 Observations ## --------------------------------------------------------------------------- ## Identifiers ## n missing distinct Info Mean Gmd .05 .10 ## 587 0 587 1 294 196 30.3 59.6 ## .25 .50 .75 .90 .95 ## 147.5 294.0 440.5 528.4 557.7 ## ## lowest : 1 2 3 4 5, highest: 583 584 585 586 587 ## --------------------------------------------------------------------------- ## GROUP ## n missing distinct ## 587 0 4 ## ## Value HR_ASD HR_neg LR_ASD LR_neg ## Frequency 95 318 3 171 ## Proportion 0.162 0.542 0.005 0.291 ## --------------------------------------------------------------------------- ## Study_Site ## n missing distinct ## 587 0 4 ## ## Value PHI SEA STL UNC ## Frequency 149 152 145 141 ## Proportion 0.254 0.259 0.247 0.240 ## --------------------------------------------------------------------------- ## Gender ## n missing distinct ## 587 0 2 ## ## Value Female Male ## Frequency 235 352 ## Proportion 0.4 0.6 ## --------------------------------------------------------------------------- ## V06.aosi.Candidate_Age ## n missing distinct Info Mean Gmd .05 .10 ## 482 105 38 0.996 6.595 0.7179 5.800 6.000 ## .25 .50 .75 .90 .95 ## 6.100 6.400 6.900 7.500 8.095 ## ## lowest : 5.1 5.5 5.6 5.7 5.8, highest: 8.7 8.9 9.0 9.3 9.4 ## --------------------------------------------------------------------------- ## V06.aosi.total_score_1_18 ## n missing distinct Info Mean Gmd .05 .10 ## 482 105 24 0.994 9.562 4.622 4 5 ## .25 .50 .75 .90 .95 ## 7 9 12 15 17 ## ## lowest : 1 2 3 4 5, highest: 20 21 22 24 28 ## --------------------------------------------------------------------------- ## V12.aosi.Candidate_Age ## n missing distinct Info Mean Gmd .05 .10 ## 512 75 37 0.994 12.59 0.7036 11.9 12.0 ## .25 .50 .75 .90 .95 ## 12.2 12.5 12.9 13.4 13.9 ## ## lowest : 0.0 11.4 11.6 11.7 11.8, highest: 14.9 15.1 15.6 15.9 16.7 ## --------------------------------------------------------------------------- ## V12.aosi.total_score_1_18 ## n missing distinct Info Mean Gmd .05 .10 ## 512 75 20 0.99 4.98 3.901 1 1 ## .25 .50 .75 .90 .95 ## 2 4 7 10 12 ## ## Value 0 1 2 3 4 5 6 7 8 9 ## Frequency 23 51 66 68 72 51 34 40 27 18 ## Proportion 0.045 0.100 0.129 0.133 0.141 0.100 0.066 0.078 0.053 0.035 ## ## Value 10 11 12 13 14 15 16 17 20 22 ## Frequency 21 10 8 7 6 4 3 1 1 1 ## Proportion 0.041 0.020 0.016 0.014 0.012 0.008 0.006 0.002 0.002 0.002 ## --------------------------------------------------------------------------- However, we can force the output to be of HTML format to match with our HTML formatted R Markdown document using the function html() which is also in the Hmisc package. This greatly improves the look of the output. We combine it with the pipe operator (see Chapter 4) to make the code more efficient. This html() function can be used for most output to force it to HTML form when you compile your R Markdown file into an HTML document. library(tidyverse) describe(aosi_data) %&gt;% html() .earrows {color:silver;font-size:11px;} fcap { font-family: Verdana; font-size: 12px; color: MidnightBlue } smg { font-family: Verdana; font-size: 10px; color: &#808080; } hr.thinhr { margin-top: 0.15em; margin-bottom: 0.15em; } span.xscript { position: relative; } span.xscript sub { position: absolute; left: 0.1em; bottom: -1ex; } aosi_data 8 Variables   587 Observations Identifiers .hmisctable666856 { border: none; font-size: 85%; } .hmisctable666856 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable666856 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 58705871294196 30.3 59.6147.5294.0440.5528.4557.7 lowest : 1 2 3 4 5 , highest: 583 584 585 586 587 GROUP .hmisctable418072 { border: none; font-size: 85%; } .hmisctable418072 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable418072 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 58704 Value HR_ASD HR_neg LR_ASD LR_neg Frequency 95 318 3 171 Proportion 0.162 0.542 0.005 0.291 Study_Site .hmisctable482242 { border: none; font-size: 85%; } .hmisctable482242 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable482242 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 58704 Value PHI SEA STL UNC Frequency 149 152 145 141 Proportion 0.254 0.259 0.247 0.240 Gender .hmisctable967390 { border: none; font-size: 85%; } .hmisctable967390 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable967390 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinct 58702 Value Female Male Frequency 235 352 Proportion 0.4 0.6 V06.aosi.Candidate_Age .hmisctable712897 { border: none; font-size: 74%; } .hmisctable712897 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable712897 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 482105380.9966.5950.71795.8006.0006.1006.4006.9007.5008.095 lowest : 5.1 5.5 5.6 5.7 5.8 , highest: 8.7 8.9 9.0 9.3 9.4 V06.aosi.total_score_1_18 .hmisctable746616 { border: none; font-size: 85%; } .hmisctable746616 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable746616 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 482105240.9949.5624.622 4 5 7 9121517 lowest : 1 2 3 4 5 , highest: 20 21 22 24 28 V12.aosi.Candidate_Age .hmisctable253566 { border: none; font-size: 85%; } .hmisctable253566 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable253566 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 51275370.99412.590.703611.912.012.212.512.913.413.9 lowest : 0.0 11.4 11.6 11.7 11.8 , highest: 14.9 15.1 15.6 15.9 16.7 V12.aosi.total_score_1_18 .hmisctable283247 { border: none; font-size: 85%; } .hmisctable283247 td { text-align: center; padding: 0 1ex 0 1ex; } .hmisctable283247 th { color: MidnightBlue; text-align: center; padding: 0 1ex 0 1ex; font-weight: normal; } nmissingdistinctInfoMeanGmd.05.10.25.50.75.90.95 51275200.994.983.901 1 1 2 4 71012 Value 0 1 2 3 4 5 6 7 8 9 10 11 Frequency 23 51 66 68 72 51 34 40 27 18 21 10 Proportion 0.045 0.100 0.129 0.133 0.141 0.100 0.066 0.078 0.053 0.035 0.041 0.020 Value 12 13 14 15 16 17 20 22 Frequency 8 7 6 4 3 1 1 1 Proportion 0.016 0.014 0.012 0.008 0.006 0.002 0.002 0.002 # html(describe(aosi_data)) this does the saem thing as above with pipe 9.5.2 General tables In R Markdown, you can essentially create and format any table you can think of by simply creating a matrix object containing the table of interest and then applying a special function to format it into HTML form. Two of these “special” functions are kable() from the kableExtra package and htmlTable() from the htmlTable package. We cover kable() here though curious readers can also learn about htmlTable() online. First, we need to create a matrix which contains the results of interest in the row by column formt that matches the format of our desired table. For example, suppose we wanted to conduct an ANOVA analysis with some of the Mullen composite standard score and ASO total score with respect to diagnosis group (High Risk: ASD, High Risk: Negative, and Low Risk: Negative). library(lsmeans) data &lt;- read.csv(&quot;Data/Cross-sec_full.csv&quot;, stringsAsFactors=FALSE, na.strings = c(&quot;.&quot;, &quot;&quot;, &quot; &quot;)) data_v2 &lt;- data %&gt;% mutate(GROUP=fct_recode(GROUP, &quot;HR: Negative&quot;=&quot;HR_neg&quot;, &quot;LR: Negative&quot;=&quot;LR_neg&quot;, &quot;HR: ASD&quot;=&quot;HR_ASD&quot;)) ## Mullen # Get means and CIs composite_anova &lt;- lm(V12.mullen.composite_standard_score~GROUP, data=data_v2) composite_for_table &lt;- as.data.frame(lsmeans(composite_anova, specs=&quot;GROUP&quot;)) composite_for_table &lt;- composite_for_table %&gt;% mutate(Mean_SE=paste(round(lsmean,2), paste(&quot;(&quot;,round(SE,2),&quot;)&quot;,sep=&quot;&quot;),sep=&quot; &quot;)) %&gt;% select(GROUP, Mean_SE) composite_for_table &lt;- t(composite_for_table) colnames(composite_for_table) &lt;- composite_for_table[1,] composite_for_table &lt;- composite_for_table[-1,] composite_for_table &lt;- c(&quot;Score&quot;=&quot;Mullen Composite&quot;, composite_for_table) ## AOSI # Get means and CIs aosi_anova &lt;- lm(V12.aosi.total_score_1_18~GROUP, data=data_v2) aosi_for_table &lt;- as.data.frame(lsmeans(aosi_anova, specs=&quot;GROUP&quot;)) aosi_for_table &lt;- aosi_for_table %&gt;% mutate(Mean_SE=paste(round(lsmean,2), paste(&quot;(&quot;,round(SE,2),&quot;)&quot;,sep=&quot;&quot;),sep=&quot; &quot;)) %&gt;% select(GROUP, Mean_SE) aosi_for_table &lt;- t(aosi_for_table) colnames(aosi_for_table) &lt;- aosi_for_table[1,] aosi_for_table &lt;- aosi_for_table[-1,] aosi_for_table &lt;- c(&quot;Score&quot;=&quot;AOSI Total&quot;, aosi_for_table) ## Calculate overall F tests for each score composite_f_stat &lt;- round(anova(composite_anova)$`F value`[1],2) aosi_f_stat &lt;- round(anova(aosi_anova)$`F value`[1],2) composite_f_test &lt;- ifelse(anova(composite_anova)$`Pr(&gt;F)`[1]&gt;0.001, round(anova(composite_anova)$`Pr(&gt;F)`[1],3),&quot;&lt;0.001&quot;) aosi_f_test &lt;- ifelse(anova(aosi_anova)$`Pr(&gt;F)`[1]&gt;0.001, round(anova(aosi_anova)$`Pr(&gt;F)`[1],3),&quot;&lt;0.001&quot;) ## Combine into single table anova_table &lt;- rbind(composite_for_table, aosi_for_table) anova_table &lt;- cbind(anova_table, &quot;F Statistic&quot;=c(composite_f_stat, aosi_f_stat), &quot;P-value&quot;=c(composite_f_test, aosi_f_test)) # Remove rownames, can see they are not needed (composite_for_table, etc.) rownames(anova_table) &lt;- NULL # (NULL =&gt; empty, different from missing NA) # Print out table in crude format (default format from R output) anova_table ## Score HR: ASD HR: Negative LR_ASD ## [1,] &quot;Mullen Composite&quot; &quot;92.48 (1.44)&quot; &quot;101.21 (0.76)&quot; &quot;100.67 (7.47)&quot; ## [2,] &quot;AOSI Total&quot; &quot;7.34 (0.39)&quot; &quot;4.84 (0.21)&quot; &quot;3.67 (2)&quot; ## LR: Negative F Statistic P-value ## [1,] &quot;105.77 (1.05)&quot; &quot;18.57&quot; &quot;&lt;0.001&quot; ## [2,] &quot;3.99 (0.29)&quot; &quot;16.68&quot; &quot;&lt;0.001&quot; Let us go over the above code used to produce the table of interest. First, for each score, we Ran an ANOVA with the domain score as the outcome and diagnosis groups as the groups for comparison. We saved the ANOVA results as an object. Then, we calculated the least square means for each group from the ANOVA results. We saved these means as a data frame to make the output easier to manipulate. Using mutate(), we created a new column called Mean_SE which combined the mean and mean’s standard error (SE) together into the usual “mean (SE)” format. We did this use the function paste(). This function allows you to combine values x, y, and z into a single string (for example) using paste(x, y, z, sep=“w”) where sep=“w” denotes the string used to separate each input. For example, sep=“,” would result in “x,y,z”. We also rounded these means and SEs to 2 decimal places using the function round(). We then added a column called “Domain” which contains the name of the Mullen domain used in the ANOVA model. We then combined all of these estimated means and SEs across the domains using rbind(). We calculated the F test statistics and p-values for each of these ANOVA analyses. We added these as columns to the final table (notice the use of c() to combine all of the domain F statistics and p-values into two separate columns). While it took some code to do, we now have a table which is structured as we had desired for the ANOVA results. You can create similar tables for regression analyses by using the above code and making some changes to reflect the use of lm() or lme() instead of aov(). This pasting together of various results using cbind() and rbind(), as well as editing the results using paste(), round(), and the functions from dplyr is a general method you can use to create your tables for any analysis. Finally, we use kable() from the kableExtra package to print out this table in HTML format. The first argument is the table/matrix/data frame of interest. Some other useful arguments include row.names= which lets you specify that names of the rows of the output, col.names= which does the same for the column names, and caption= which allows you to specify the title for the output. Here, we omit the row names and just have the column names be the column names of the actual R object. To adjust more aspects, use the function kable_styling(); see the documentation for this function to learn more. This function also changes the overall look of the output and it is recommended to always apply it, even if you do not use any of its arguments (see below). library(kableExtra) kable(anova_table, col.names = colnames(anova_table), caption=&quot;Comparison of group means from regression of Mullen composite and AOSI total score on diagnosis group. Estimated group means and their corresponding standard errors (SE), rounded to two decimal spaces, are provided for each score These comparisons were done for Expressive Language, Fine Motor, and Gross Motor at 12 months. For each of these, the p-value and test statistic corresponding to the ANOVA F-test for group differences is reported.&quot;) %&gt;% kable_styling() (#tab:ANOVA_ex_kable)Comparison of group means from regression of Mullen composite and AOSI total score on diagnosis group. Estimated group means and their corresponding standard errors (SE), rounded to two decimal spaces, are provided for each score These comparisons were done for Expressive Language, Fine Motor, and Gross Motor at 12 months. For each of these, the p-value and test statistic corresponding to the ANOVA F-test for group differences is reported. Score HR: ASD HR: Negative LR_ASD LR: Negative F Statistic P-value Mullen Composite 92.48 (1.44) 101.21 (0.76) 100.67 (7.47) 105.77 (1.05) 18.57 &lt;0.001 AOSI Total 7.34 (0.39) 4.84 (0.21) 3.67 (2) 3.99 (0.29) 16.68 &lt;0.001 For more details on using kable() and kableExtra(), as well as additional examples, please see the following. 9.6 Practice with R Markdown For examples of R Markdown files and the corresponding documents that they create, please view the R Markdown files that created all of these HTML-type R tutorials. Reading through these R Markdown files and compiling them using Knit will be the best way of understand the capabilities of R Markdown, as well as how to use them. "],
["loops-and-functional-programming.html", "10 Loops and Functional Programming 10.1 Intro 10.2 Loops 10.3 Functional Programming 10.4 Simulation Studies", " 10 Loops and Functional Programming library(tidyverse) 10.1 Intro We have covered the foundamentals you need to complete standard statistical analyses and data processing in R, as well as to document your results. Now, we cover some more advanced techniques that can be used to make your code more efficient: loops (including the apply functions) and functional programming. Using these two tools, you can omit redundant code and make your code easier to read, quicker to write, and limit human errors when writing it. We also cover how to generate values from specified distributions, which is important when running simulation studies. 10.2 Loops Loops are a way of having R do repetitive tasks automatically in a sequential format. These are also called for loops since you specifiy a set of indicies/numbers, and R does the tasks for each value in this set, in order of the first number in the set to the last. Their format is the following: for(i in X){ … } where i keeps track of the index, X is the set of indicies to use, and … is where you place the code to be run for each. You can use any string (without spaces) of unquoted letters instead of i. Consider the simple example: for(i in 1:10){ print(i+1) } ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## [1] 11 You can see R places the current value of i, adds 1 to it, then prints out this value, and moves to the next value. The code 1:10, or seq(1,10), creates a sequence from 1 to 10, by 1. If you wanted a sequence by 1 to 10 by 2, use seq(1,10,2), etc. for(i in seq(1,10,2)){ print(i+1) } ## [1] 2 ## [1] 4 ## [1] 6 ## [1] 8 ## [1] 10 10.2.1 Example 1: Running many regression models Next we consider a less trival example. Suppose we have a number of regression models to fit, with all variables in the regression models from a single dataset. We run some logistic regression models where ASD status at 24 months (positive or negative) is the outcome and each model uses a different predictor (AOSI, Mullen Composite Score, and Mullen Cognitive T Score as examples) value at 12 months. To do this, we create a list of formulas which represents the models we wish to fit, and then run glm() on each element of the list using a loop. We save the output from each model fit in another list. This is a good general format for using loops to repeat analyses where only minor components are changed; create a list or vector or matrix to hold the different components and create a list to hold the output from each run of the loop. full_data &lt;- read.csv(&quot;Data/Cross-sec_full.csv&quot;, stringsAsFactors=FALSE, na.strings = c(&quot;.&quot;, &quot;&quot;, &quot;&quot;, &quot; &quot;)) full_data &lt;- full_data %&gt;% mutate(ASD_Status=ifelse(GROUP==&quot;HR_ASD&quot;, &quot;ASD_Pos&quot;, ifelse(is.na(GROUP)==1, NA, &quot;ASD_Neg&quot;)) %&gt;% factor() %&gt;% relevel(ref=&quot;ASD_Neg&quot;)) # Create list of formulas frmlas &lt;- list(ASD_Status~V12.aosi.total_score_1_18, ASD_Status~V12.mullen.composite_standard_score, ASD_Status~V12.mullen.cognitive_t_score_sum) output &lt;- list() # create empty list to hold output for(i in 1:length(frmlas)){ output[[i]] &lt;- glm(frmlas[[i]], data = full_data, family=binomial) names(output)[i] &lt;- capture.output(frmlas[[i]]) # name each entry in output list for each identification } # Let&#39;s check the results from the first model summary(output[[1]]) ## ## Call: ## glm(formula = frmlas[[i]], family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7800 -0.5936 -0.4563 -0.3813 2.3804 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.77241 0.24331 -11.394 &lt; 2e-16 *** ## V12.aosi.total_score_1_18 0.18760 0.03201 5.861 4.59e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 443.80 on 511 degrees of freedom ## Residual deviance: 407.84 on 510 degrees of freedom ## (75 observations deleted due to missingness) ## AIC: 411.84 ## ## Number of Fisher Scoring iterations: 4 summary(output[[2]]) ## ## Call: ## glm(formula = frmlas[[i]], family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.5793 -0.6063 -0.4554 -0.3391 2.4886 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.752058 0.905505 4.144 3.42e-05 ## V12.mullen.composite_standard_score -0.055758 0.009438 -5.908 3.47e-09 ## ## (Intercept) *** ## V12.mullen.composite_standard_score *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 451.91 on 525 degrees of freedom ## Residual deviance: 413.36 on 524 degrees of freedom ## (61 observations deleted due to missingness) ## AIC: 417.36 ## ## Number of Fisher Scoring iterations: 5 summary(output[[3]]) ## ## Call: ## glm(formula = frmlas[[i]], family = binomial, data = full_data) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.7160 -0.5943 -0.4621 -0.3431 2.4867 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.555768 0.870239 4.086 4.39e-05 *** ## V12.mullen.cognitive_t_score_sum -0.026943 0.004541 -5.934 2.96e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 451.91 on 525 degrees of freedom ## Residual deviance: 412.94 on 524 degrees of freedom ## (61 observations deleted due to missingness) ## AIC: 416.94 ## ## Number of Fisher Scoring iterations: 5 This avoids having to write three glm function calls (though you still have to write the three formulas), and easily collects the output into a list to improve organizaton of the results. You can use the above looping method with any analysis you wish, including data visualization with plots. We can do a bit more with this regression example. Currently, each model’s output is saved as a separate entry in the list entitled “output”. It is often useful to summarize all of these results into a single dataset, with column representing the model being fit, the regression parameetrs estimate for each covariate, corresponding confidence intervals and p-values, etc. We can modify our loop to accomplish this task. # Create list of formulas frmlas &lt;- list(ASD_Status~V12.aosi.total_score_1_18, ASD_Status~V12.mullen.composite_standard_score, ASD_Status~V12.mullen.cognitive_t_score_sum) output &lt;- list() # create empty list to hold output output_df_list &lt;- list() # create empty list to hold regression output as dataset for(i in 1:length(frmlas)){ output[[i]] &lt;- glm(frmlas[[i]], data = full_data, family=binomial) names(output)[i] &lt;- capture.output(frmlas[[i]]) # name each entry in output list for each identification # Now, create dataset for each regression model&#39;s output. Recall data frames are R objects which are the standard format for dataset. Using the data.frame() function, a data frame is created, which each argument being a column/variable in the dataset. Using &quot;..&quot; = to specify the name of the column/variable. When happens when you remove &quot;..&quot; = from each argument? output_df_list[[i]] &lt;- data.frame(&quot;Model&quot;=capture.output(frmlas[[i]]), &quot;Covariate&quot;=names(output[[i]]$coefficients), &quot;Beta_Estimate&quot;=output[[i]]$coefficients, &quot;CI_95_LL&quot;=confint(output[[i]])[,1], &quot;CI_95_UL&quot;=confint(output[[i]])[,2], &quot;P_val&quot;=summary(output[[i]])$coefficients[,4]) # Look at output_df_list[[i]]. Can see we have row labels which are not useful. This is because output[[i]]$coefficients has these row names, which were carried over when creating the data frame. We need to remove those, i.e. set them to NULL rownames(output_df_list[[i]]) &lt;- NULL } # Let&#39;s paste together all of these separate data frames into a single one. Note that we are pasting them together row-wise so we need to use rbind(). However, we also have to use the function do.call(). Why can we not simply use rnind(output_df_list)? What exactly does do.call() do? Hint: look at documentation for function using command ?do.call . output_df_full &lt;- do.call(&quot;rbind&quot;, output_df_list) output_df_full ## Model ## 1 ASD_Status ~ V12.aosi.total_score_1_18 ## 2 ASD_Status ~ V12.aosi.total_score_1_18 ## 3 ASD_Status ~ V12.mullen.composite_standard_score ## 4 ASD_Status ~ V12.mullen.composite_standard_score ## 5 ASD_Status ~ V12.mullen.cognitive_t_score_sum ## 6 ASD_Status ~ V12.mullen.cognitive_t_score_sum ## Covariate Beta_Estimate CI_95_LL ## 1 (Intercept) -2.77241345 -3.26943791 ## 2 V12.aosi.total_score_1_18 0.18759712 0.12585158 ## 3 (Intercept) 3.75205842 2.00272798 ## 4 V12.mullen.composite_standard_score -0.05575794 -0.07478838 ## 5 (Intercept) 3.55576763 1.87727181 ## 6 V12.mullen.cognitive_t_score_sum -0.02694335 -0.03611077 ## CI_95_UL P_val ## 1 -2.31342914 4.457601e-30 ## 2 0.25173696 4.594861e-09 ## 3 5.56455358 3.418860e-05 ## 4 -0.03767386 3.471997e-09 ## 5 5.30061841 4.389367e-05 ## 6 -0.01825424 2.959601e-09 10.2.2 Example 2: The Apply Functions R has series of functions which allow one to easily run loops over various dimensions of an object without having to write out the for loop explicitly. In this section, we discuss apply(), lapply(), and sapply() (there are others you may find useful as well which are not discussed here). Each of these works best with a specific type of object; apply() for vectors and matricies, lapply() for lists, and sapply() for lists or data frames. The apply() function applies a specified function to each row, column, or entry of a vector or matrix. For example, suppose we want to add up the values in each column of a matrix (that is, apply the function sum() to each column). We can do this easily using apply(X, MARGIN=2, FUN=sum) where X is the matrix object. If we wanted to apply the function to each row, we would specify MARGIN=1, and to apply the function to each entry use MARGIN=c(1,2). You can use a user-created function as well. x &lt;- matrix(c(rep(3,12)), nrow=4) # Create matrix apply(x, MARGIN=1, FUN=sum) # By row ## [1] 9 9 9 9 apply(x, MARGIN=2, FUN=sum) # By column ## [1] 12 12 12 times_2_sum &lt;- function(x){ 2*sum(x) } apply(x, MARGIN=1, FUN=times_2_sum) # By row ## [1] 18 18 18 18 apply(x, MARGIN=2, FUN=times_2_sum) # By column ## [1] 24 24 24 The lapply() and sapply() functions work similarly, but are designed for lists by applying a function to each component of the list. With lists, we do not have multiple dimensions, so the main arguments for lapply() and sapply() are X= and FUN= . The difference between theses two is that lapply() will print the output as a list, while sapply() will print the output in the same format as the input. Since a data frame is actually a list (with the columns/variables making up the components of the list), both will work on data frames. However, sapply() will keep the output as a data frame while lapply() will output a list. Recall at the end of Chapter 2, we used sapply() to print out the type for each each column in a data frame; you can also use it to print out various statistics for each column simply by specifying the corresponding function. 10.3 Functional Programming Functional programming is a concept that can be used frequently in standard statistical analyses and thus can be very helpful is making your code more efficient. We have already seen functions in R and how to use them throughout these tutorials; essentially, every computation that we have done has made use of a pre-built R function. However, it is very easy to create your own function. This can allow you to complete tasks which require lines of code and change a small aspect of these tasks with a single function call. To create a function, you use the following syntax: foo &lt;- function(x, y){ … } foo is some name for your function, x and y (and more if you need them) are the function’s arguments, and … is the code that executes when you call the function. You can essentially use any non-quoted string of characters (again, without spaces) as argument names instead of x, y, etc. (there are some restrictions). R will automatically take the output from the last line of code in the function and have it be the output when you call your function. Consider the following trivial example where we write a function which adds 3 numbers and the divides them by 2. sum_divide &lt;- function(x,y,z){ (x+y+z)/3 } sum_divide(1,2,3) ## [1] 2 We can see that the output from this sum and division is the last line in the function’s code and is also printed out by the function call. What happens if the last line of the function’s code does NOT output something? sum_divide &lt;- function(x,y,z){ z &lt;- (x+y+z)/3 } sum_divide(1,2,3) t &lt;- sum_divide(1,2,3) t ## [1] 2 We can see then nothing is output by the function. However, if we save the function call as an object (t in this case), then even though the function does not print out anything in it’s code, the value stored in the last line of the function is stored in object t when the function is called. In general, you can forget about this difference by simply having the last line of your function call be the printing of the output of interest. 10.3.1 Example: Regression Analysis as a Function Call Let us try a useful example of function calls. Suppose we wanted to run a lengthy linear regression analysis where we will Fit the regression model Create a scatterplot with the line of best fit Check the assumptions and we want to run a few such analyses for a set of regression models. We will do this by creating a single function to do all of these analyses and then change the model used with some arguments. We then run two linear regression analyses, each one computated using a single function call. As a result, we avoid repetitive code which can lead to typographic errors. lm_analysis &lt;-function(model){ model_fit &lt;- lm(model, data=full_data) scatterplot &lt;- ggplot(data=full_data, mapping=aes_string(y=as.character(model[2]), x=as.character(model[3])))+ geom_point()+ geom_smooth() # Create dataset of residuals and fitted values from model fit; needed for data= argument for later ggplot residual_fitted_data &lt;- data.frame(residual=model_fit$residuals, fitted_value=model_fit$fitted.values) residual_fitted_plot &lt;- ggplot(data=residual_fitted_data, mapping=aes(y=residual, x=fitted_value))+ geom_point() # Can create QQ plot using qqnorm() from previous tutorial or using ggplot # Use argument sample= to place residuals qqplot &lt;- ggplot(data=residual_fitted_data, mapping=aes(sample=residual))+ geom_qq()+ geom_qq_line() # Save everything as a list output &lt;- list(model=as.character(model), fit_obj=model_fit, residual_by_fitted_value_plot=residual_fitted_plot, qqplot=qqplot) output # Have function return output when called } # Now call function and store in object; run linear regression model with AOSI total score at 12 months as outcome and Mulen expressive language as predictor test_function &lt;- lm_analysis(model=V12.aosi.total_score_1_18 ~ V12.mullen.cognitive_t_score_sum) test_function$fit_obj # prints out the fit object from lm() ## ## Call: ## lm(formula = model, data = full_data) ## ## Coefficients: ## (Intercept) V12.mullen.cognitive_t_score_sum ## 12.83863 -0.03895 # Can easily run a different model; avoids repetitive code test_function &lt;- lm_analysis(model=V12.mullen.composite_standard_score ~ V12.aosi.total_score_1_18) test_function$fit_obj # prints out the fit object from lm() ## ## Call: ## lm(formula = model, data = full_data) ## ## Coefficients: ## (Intercept) V12.aosi.total_score_1_18 ## 106.653 -1.116 Note the use of aes_string() here instead of aes(). This illustrates an important concept when trying to do this form of functional programming; you will often have to use different versions of functions you usually use. The reason is the following. Suppose we want to pass in variables x and y from some dataset. You may think you can simply use xval=x and yval=y in your created function, where xval and yval are the names of this function’s arguments. However, this will generally result in an error since x and y are not separate objects, but are tied to the object data. Thus, you could use “x” and “y” instead and use a function that evaluates strings as R objects within your dataset. This is what aes_string() does. When using dplyr functions in your function, you will also have to do similar adjustments. Generally, this means using the functions select_(), filter_(), etc. though search online for more help and information on this concept. For the exceptionally curious reader about this concept, called evaluation, please see Hadley Wickham’s excellent book Advanced R. 10.4 Simulation Studies When developing or learning new statistical methods, it is often necessary to implement these methods on simulated datasets. Simulated datasets are composed of values which are generated by the user based on specified distributions. As a result, the user knows the properties of the data they are analyzing and can discern how well the method of interest estimates such properties. Suppose we wanted to generate a sequence of binary values (0 or 1). A variable that is either 0 or 1 has a Bernoulli distribution. In R, to generate \\(n\\) independent values from a Bernoulli distribution, you can use the rbinom(n, size, prob) function with size=1. The argument prob controls the probability of a 1 (“success”). When calling this function, R will output a vector of size \\(n\\) which contains the values. We generate \\(n=10\\) Bernoulli draws below with a 0.5 probability of a 1 for each draw. x=rbinom(10, 1, 0.5); sum(x) ## [1] 5 An extension of this distribution is the Binomial distribution, which models the number of “successes” (1s) among a set of independent attempts. One can think of this as modeling the number of positive diagnoses among subjects (“successes”) in a sample (“attempts”), assuming the subjects’ diagnoses are independent. Note that for a single attempt, the Bernoulli and Binomial distribution are the same. In R, to generate \\(n\\) independent values from a Binomial distribution, you can use the rbinom(n, size, prob) function, where size reflects the number of attempts and prob reflects the probability of a “success” for each of these attempts. We generate \\(n=10\\) independent Binomial values below with a 0.5 probability of a 1 for each attempt and 10 attempts per value (i.e., 10 trials with 10 subjects in each trial). x=rbinom(10, 10, 0.5); mean(x) ## [1] 5.5 There are many distributions you can chose from when generating continuous values. For example, suppose we wanted to generate n independent values from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). In R, this is done using **rnorm(n, mean=\\(\\mu\\), and sd=\\(\\sigma\\)). Below, we use \\(n=100\\), \\(\\mu=0\\), \\(\\sigma=1\\). x=rnorm(100, 0, 1); hist(x) For functions corresponding to distributions such as Uniform, Gamma, Poisson, etc., please consult the R documentation. Now, we consider two simple simulation examples; 1) simulated data with two groups, each of which having normally distributed data but with different means and/or standard deviations and 2) simulating data under a linear regression model. First, suppose we have subjects from two groups (~100 subjects in each group) denoted by 0 and 1 respectively. Suppose we have data from another variable for all subjects, where this variable is normally distributed with standard deviation of 1 but group 0 has a mean of 0 and group 1 has a mean of 5. We can simulate this data as follows: n &lt;- 200 # Set total sample size group = rbinom(n, 1, 0.5); sum(group) # simulate group labels ## [1] 95 Y &lt;- rep(NA, n) # create object to hold normally distributed variable Y[group==0] &lt;- rnorm(n-sum(group), 0, 1) # Replace Y indicies of those in group 0 with mean 0, normally distributed values Y[group==1] &lt;- rnorm(sum(group), 5, 1) # Replace Y indicies of those in group 1 with mean 5, normally distributed values hist(Y) # histogram for entire sample; notice that it is bimodal data &lt;- data.frame(Y, factor(group)) ggplot(data=data, mapping = aes(x=Y, fill=group))+ geom_histogram() As a second example, consider outcome variable \\(Y\\) and predictor \\(X\\) where \\(X\\) is normally distributed with mean 0 and standard deviation 1. We generated \\(Y\\) using the following model: \\(Y=1+2X+\\epsilon\\) where \\(\\epsilon\\) is normally distributed with mean 0 and standard deviation 2. This model for \\(Y\\) is an example of the usual linear regression model discussed in Chapter 8. We generate a sample of size 200 from these distributions below. n &lt;- 200 X &lt;- rnorm(n, 0, 1) # predictor values E &lt;- rnorm(n, 0, 2) # Error values Y &lt;- rep(1,n)+2*X+E # Outcome values data &lt;- data.frame(&quot;Y&quot;=Y, &quot;X&quot;=X) Now, let us fit a linear regression model to this data with \\(Y\\) as the outcome and \\(X\\) as the sole predictor. We will output the estimated regression parameters (the slope and intercept) and look at the distribution of the residuals using a QQ plot. For more detail on linear regression models and residuals, please see Chaper 8. lm_fit &lt;- lm(Y~X, data=data) summary(lm_fit) ## ## Call: ## lm(formula = Y ~ X, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9854 -1.3656 -0.0281 1.5406 5.3681 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0317 0.1443 7.152 1.62e-11 *** ## X 2.1575 0.1436 15.025 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.037 on 198 degrees of freedom ## Multiple R-squared: 0.5328, Adjusted R-squared: 0.5304 ## F-statistic: 225.8 on 1 and 198 DF, p-value: &lt; 2.2e-16 qqnorm(lm_fit$residuals) qqline(lm_fit$residuals) As expected, we see that when we fit a linear regression model to this data, the estimated intercept and slope are very close to the corresponding true values and the predicted residuals appear to be approximately normally distributed. 10.4.1 Setting the Seed: Reproducibility in Simulation Studies You will notice that each time you generate data from a specified distribution, R will output different values as you would expect. However, this makes reproducing the analysis from the simulated data impossible as the data changes everytime you run your script. In order to keep the dataset the same each time, you need to specify the seed that R uses when generating the data. For detail on seeds and on how R and other programs actually simulate the data from a specified distribution, please see https://en.wikipedia.org/wiki/Random_number_generation. In R, to set your seed manually, use the function set.seed(x) where x is the seed value. The seed value must be an integer. After the seed is held constant, you will see that the generated datasets remain the same, as shown below. You will see that the simulated data changes as you change the seed. As a result, do not try different seeds and then pick the one that gives the “best” results in your analysis. Pick one number and stick with it anytime you generate data to avoid biasing the analysis. rnorm(10, 0, 1) ## [1] -0.1267782 1.0592069 -1.1673960 -0.5576436 1.4881199 1.3586658 ## [7] 1.1632145 1.6615239 0.2040310 -0.5818837 rnorm(10, 0, 1) # different values despite using the same size and distribution ## [1] 0.55520406 1.05872313 2.41363327 -1.96498233 0.27323570 ## [6] 0.65479458 -0.05459866 -1.55782225 0.74150089 -0.77908574 set.seed(12) rnorm(10, 0, 1) ## [1] -1.4805676 1.5771695 -0.9567445 -0.9200052 -1.9976421 -0.2722960 ## [7] -0.3153487 -0.6282552 -0.1064639 0.4280148 set.seed(12) rnorm(10, 0, 1) # same value after setting the seed ## [1] -1.4805676 1.5771695 -0.9567445 -0.9200052 -1.9976421 -0.2722960 ## [7] -0.3153487 -0.6282552 -0.1064639 0.4280148 set.seed(4) rnorm(10, 0, 1) ## [1] 0.2167549 -0.5424926 0.8911446 0.5959806 1.6356180 0.6892754 ## [7] -1.2812466 -0.2131445 1.8965399 1.7768632 set.seed(4) rnorm(10, 0, 1) # same value after setting the seed but different values compared to previous seed value as expected ## [1] 0.2167549 -0.5424926 0.8911446 0.5959806 1.6356180 0.6892754 ## [7] -1.2812466 -0.2131445 1.8965399 1.7768632 rnorm(10, 0, 1) # different values; we must set the seed after EACH call to the random number generator else a random seed is chosen for the next call ## [1] 0.56660450 0.01571945 0.38305734 -0.04513712 0.03435191 ## [6] 0.16902677 1.16502684 -0.04420400 -0.10036844 -0.28344457 "]
]
